[
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html",
    "href": "posts/VSCode-setup/VSCode_setup.html",
    "title": "VS Code Setup",
    "section": "",
    "text": "There are various Integrated Development Environments (IDEs) suitable for Python development. PyCharm offers powerful features and Django integration but consumes significant system resources and is a paid tool. Jupyter Notebook is optimized for data science but has limitations in managing large codebases and traditional IDE features. Spyder is specialized for scientific computing but has limitations for general-purpose development. On the other hand, Visual Studio Code (VS Code) is a lightweight IDE with excellent extensibility, supporting various programming languages and consuming fewer system resources. It allows effective management of multiple projects, including Python development, by adding extension features as needed. The study group recommends VS Code for Python development.",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#recommended-python-integrated-development-environment",
    "href": "posts/VSCode-setup/VSCode_setup.html#recommended-python-integrated-development-environment",
    "title": "VS Code Setup",
    "section": "",
    "text": "There are various Integrated Development Environments (IDEs) suitable for Python development. PyCharm offers powerful features and Django integration but consumes significant system resources and is a paid tool. Jupyter Notebook is optimized for data science but has limitations in managing large codebases and traditional IDE features. Spyder is specialized for scientific computing but has limitations for general-purpose development. On the other hand, Visual Studio Code (VS Code) is a lightweight IDE with excellent extensibility, supporting various programming languages and consuming fewer system resources. It allows effective management of multiple projects, including Python development, by adding extension features as needed. The study group recommends VS Code for Python development.",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#installation-guidance",
    "href": "posts/VSCode-setup/VSCode_setup.html#installation-guidance",
    "title": "VS Code Setup",
    "section": "Installation Guidance",
    "text": "Installation Guidance\nIt is ideal to refer to the official documentation (https://code.visualstudio.com/docs/setup/setup-overview) for installation instructions tailored to your operating system. However, you can follow the simple guidance below to install VS Code.",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#installation-file",
    "href": "posts/VSCode-setup/VSCode_setup.html#installation-file",
    "title": "VS Code Setup",
    "section": "Installation File",
    "text": "Installation File\nDownload the latest installation file suitable for your operating system from the official website (https://code.visualstudio.com/Download). For Windows, the System installer (an installation file that makes VS Code available to all users) is recommended. As of September 29, 2024, the latest file for Windows is VSCodeSetup-1.93.1.exe.",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#installation-path",
    "href": "posts/VSCode-setup/VSCode_setup.html#installation-path",
    "title": "VS Code Setup",
    "section": "Installation Path",
    "text": "Installation Path\nFor Windows, the default installation path is C:\\Users\\{Username}\\AppData\\Local\\Programs\\Microsoft VS Code, which is recommended.",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#vs-code-layout",
    "href": "posts/VSCode-setup/VSCode_setup.html#vs-code-layout",
    "title": "VS Code Setup",
    "section": "VS Code Layout",
    "text": "VS Code Layout\nIn explaining the configuration, the terms used in the official VS Code documentation (https://code.visualstudio.com/docs) are adopted. The terms for layout components are referenced in Figure 1. However, terms such as “title & menu bar” are not explicitly defined in the official documentation but are used here for clarity.\n\n\n\n\n\n\nFigure 1: Basic layout of VS Code",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#installing-python-extensions",
    "href": "posts/VSCode-setup/VSCode_setup.html#installing-python-extensions",
    "title": "VS Code Setup",
    "section": "Installing Python Extensions",
    "text": "Installing Python Extensions\nTo write and execute Python code in VS Code, you need to install the Python extension. When you first launch VS Code, the Welcome page will appear. Install the Python extension from this page, or alternatively, use the Extensions menu in the Activity Bar (Ⓐ). Once installed, the extension can be verified under the INSTALLED section in the Extensions menu (Figure 2). This completes the basic setup for using Python in VS Code.\n\n\n\n\n\n\nFigure 2: Python Extension in VS Code",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#creating-project-folders",
    "href": "posts/VSCode-setup/VSCode_setup.html#creating-project-folders",
    "title": "VS Code Setup",
    "section": "Creating Project Folders",
    "text": "Creating Project Folders\nLike R, Python also implements independent management for each project at the project folder level. Since our study group uses both R and Python, we recommend creating a Projects folder under the C: root directory. We suggest naming each project folder with the version of the executable file first, followed by a concise name representing the project (Figure 3).\n\n\n\nC:\\Projects\\R-x'.y'.z'-Project_Name'\\            # R project directory and project name example\n         └─ Python-x.y.z-Project_Name\\  # Python project directory and project name example\n\n\nFigure 3: Project folder location and name example",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#creating-virtual-environments-for-each-project",
    "href": "posts/VSCode-setup/VSCode_setup.html#creating-virtual-environments-for-each-project",
    "title": "VS Code Setup",
    "section": "Creating Virtual Environments for Each Project",
    "text": "Creating Virtual Environments for Each Project\nPython virtual environments allow developers to manage various libraries and Python versions needed for different projects independently. This enables multiple projects with different dependency requirements to operate on the same system without conflicts, isolates the development environment to prevent issues in one project from affecting others, and supports easy replication or deployment of specific project settings to different environments. Virtual environments help maintain consistency between development, testing, and production environments and enable team members to work in the same settings. For these reasons, virtual environments are essential tools that significantly enhance project stability and development efficiency. The following explains how to implement Python virtual environments in VS Code.\n\nCreating Project Folders\nCreate project folders as described earlier. However, there is no option to create folders directly in VS Code. Therefore, it is recommended to use Windows Explorer to create project folders. (Alternatively, you can use the terminal tab in the title & menu bar of VS Code to create project folders, but this method may be more cumbersome than using Windows Explorer.) For example, name and create project folders as follows (e.g., C:-x.y.z-Project_Name).\n\n\nOpening Project Folders\nOpen project folders using the Explorer menu (Ⓐ Activity bar) or the File menu in the title & menu bar. (This is to ensure that the project folder opens when you open the terminal later.)\n\n\nCreating a New Terminal\nSelect the New Terminal option in the Terminal menu under the title & menu bar to open a new terminal in the Ⓓ panel. If the opened terminal is PowerShell, select the v(Select Default Profile) menu next to the +(New Terminal) icon on the right side of the panel title bar (Figure 4). Then, choose Command Prompt to create a new Command Prompt terminal.\n\n\n\n\n\n\nFigure 4: Command Prompt Terminal Creating\n\n\n\n\n\nCreating a Python Virtual Environment\nEnter and execute the following command in the newly created terminal.\n\n\n\nCommand Prompt Terminal\n\npython -m venv venv\n\n\n\n\n\n\n\n\nMeaning of python -m venv venv\n\n\n\n\n\nThe command uses the built-in Python module venv to create a new virtual environment.\n1. python This part calls the Python interpreter installed on the system. It uses the Python version set as default on the system to execute the module specified next.\n2. The -m flag instructs Python to run the module directly from the command line. It finds the Python module corresponding to the module name (venv, etc.) following this flag and executes that module like a script.\n3. The first venv is the name of the built-in Python module for creating virtual environments. This module is used to create, manage, and maintain virtual environments, providing an independent Python execution environment. This environment contains an independent Python interpreter, libraries, and scripts.\n4. The second venv is the directory name of the virtual environment to be created. In this example, a folder named ‘venv’ is created within the current directory, and the virtual environment is set up inside it. The directory name can be specified as any name the user desires, and the virtual environment is configured within the folder created with this name.\nTherefore, the command python -m venv venv creates a folder named ‘venv’ in the current working directory and sets up a new independent Python virtual environment inside it. This virtual environment provides a separate environment to install and manage Python packages independently from other projects. It prevents dependency conflicts and effectively manages libraries with different requirements for each project.\n\n\n\nThe successful execution of the above command will create a venv virtual environment folder under the project folder. If you see a folder named venv under the project folder in the Explorer, the operation was successful (Figure 5).\n\n\n\n\n\n\nFigure 5: Validation of venv Folder Creation\n\n\n\nThe following folders and files are created under the venv folder generated by the virtual environment (Figure 6).\n\n\n\n(venv)/\n   ├── Include/                  # Folder for C/C++ header files\n   ├── Lib/                      # Folder for standard libraries and third-party packages\n   │      └── site-packages/     # Folder where packages installed via pip are located\n   ├── Scripts/                  # Folder containing executable scripts, such as pip\n   │      ├── activate           # Script to activate the virtual environment\n   │      ├── deactivate         # Script to deactivate the virtual environment\n   │      ├── python.exe         # Python executable for the virtual environment\n   │      └── pip.exe            # pip executable for managing packages in the virtual environment\n   └── pyvenv.cfg                # Configuration file for the virtual environment\n\n\n\nFigure 6: (venv) Directory Structure\n\n\n\nWhen you install packages using pip in the virtual environment, they are installed in the site-packages folder within the venv virtual folder, rather than globally. This allows you to manage the packages needed for each project independently, preventing dependency conflicts between projects.\n\n\nLinking Project and Virtual Environment\nSelect View from the title menu and choose Command Palette. Then select Python: Select Interpreter from the list of available commands (Figure 7).\n\n\n\n\n\n\nFigure 7: Select Interpreter in Command Palette\n\n\n\nUpon selection, all available Python interpreters will be displayed. Choose the python.exe file located in the Scripts folder under the venv virtual environment folder created in the project folder (Figure 8).\n\n\n\n\n\n\nFigure 8: Interpreter Selection\n\n\n\n\n\nValidating the Virtual Environment\nEven after linking the Python interpreter of the virtual environment to a specific project, there will be no visible changes on the screen. To confirm that the connection was successful, open a new terminal. If you see the virtual environment folder (venv) enclosed in parentheses in the terminal prompt, as shown in Figure 9, it indicates a successful connection. This confirms that the virtual environment is currently active.\n\n\n\n\n\n\nFigure 9: Validation of Virtual Environment Activation",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/VSCode-setup/VSCode_setup.html#saving-project-settings",
    "href": "posts/VSCode-setup/VSCode_setup.html#saving-project-settings",
    "title": "VS Code Setup",
    "section": "Saving Project Settings",
    "text": "Saving Project Settings\nAfter completing the virtual environment setup in the project folder, you can save this state. To do this, select Save Workspace As from the File menu in the title & menu bar (Figure 10).\n\n\n\n\n\n\nFigure 10: Workspaces Save as function in File Menu\n\n\n\nWhen this action is taken, a projectfoldername.code-workspace file will be created in the project folder, saving the project settings (Figure 11). When you reopen the project, selecting this file will automatically load the project folder settings.\n\n\n\n\n\n\nFigure 11: code-workspace File Creation",
    "crumbs": [
      "Python",
      "VS Code Setup"
    ]
  },
  {
    "objectID": "posts/survival-analysis/survival_analysis.html",
    "href": "posts/survival-analysis/survival_analysis.html",
    "title": "Survival analysis",
    "section": "",
    "text": "인터넷검색 시 정리가 잘 된 부분을 발췌하였습니다.\n지루한 일상의 소중함 중에서 ……. https://every-day-life.tistory.com/30\n\n1. Kaplan-Meier curve\n상당수의 의학논문에서 생존분석을 이용해서 분석을 하는 경우, Kaplan-Meier curve를 제시하고 그 후 cox 분석을 주요분석법으로 사용한다. Kaplan curve를 보여주는 이유는 아무래도 단조로운 논문에 그래도 그림이 하나쯤은 필요하기 때문이기도 하지만 더 큰 이유는 cox 분석을 사용하기 위한 주요 가정 중 하나인 proportional hazard assumption을 만족하는지를 Kaplan curve가 rough하게 보여주기 때문이다.\n마지막으로 proportional hazard에 대한 언급을 해 보자. cox 분석은 여러 분석법 중 신경 쓸 일 많지 않은 쉬운 분석법 중에 하나이다. 하지만 비례위험 가정은 꼭!!! 어기면 안 된다.  비례위험 가정에 대해 예를 들어보면, 암환자의 경우 진단 후 초/중반에는 사망위험이 높지만 시간이 지날수록 사망 위험도는 점차 감소하다,  5년 이상 생존하는 경우 사망위험이 일반인보다 약간 높을 정도까지 낮아진다. 만약 새로운 항암제의 효과에 대해 연구하고 싶다면 신/구 항암제의 위험도 차이가 초반에나 후반에나 동일해야 한다는 것이다. \n\n왼쪽 그림과 같이 시간에 상관없이 위험도는 동일한 간격을 가져야 한다. 만약 오른쪽 그림과 같이 위험도가 일정한 간격을 지니지 않을 경우 시간에 대해 별도의 배려를 해야만 한다.  \n비례위험가정을 확인하는 법은 몇 가지 방법이 있지만 SPSS에서 가능한 방법은 누적위험함수 그래프를 확인하는 법과 Cox 분석 내에서의 time dependent covariate를 압력하는 방법 2가지 이다. 결과가 통계적으로 유의하더라도 Kaplan curve 및 위험함수 그래프에서 두 선이 접점을 가지게 되면 비례위험가정을 만족하지 못하는 것으로 생각하는 편이 맞다(정확히는 Scheonfeld residual을 구해서 확인해야 하는데 SPSS에서는 이 과정이 불가능하다.). 그리고 논문 첫 figure로 Kaplan curve를 넣는 이유 중 하나가 비례위험가정만족을 보여주는 그림이기 때문이다. 만약 Kaplan curve에서 두 선의 교차가 발생했는데도 불구하고 시간에 대한 별도의 배려 없이 논문이 진행되었다면 독자들은 그 이후 결과를 전체적으로 신뢰할 수 없게 될 것이다.  (상대적으로 Kaplan curve에서는 비례위험가정을 만족하더라도 대상자의 수가 적어지는 후반부에 교차(접점?)가 일어날 가능성이 있다. 다른 방법으로 비례위험가정을 확인했고 문제가 없다면 누적위험함수 그래프를 대신 제시하는 것도 하나의 방법이다.)\n\n\n2. Cox\nCox 분석의 정식 명칭은 proportional hazards model regression analysis이다. 정식 명칭을 보면 앞에서 왜 그토록  proportional hazard assumption을 강조했는지와, Cox 분석도 일반화선형회귀분석의 한 분류라는 것을 알 수 있다. 전체적인 분위기는 로지스틱회귀분석과 놀라울 정도로 닮아 있다. 잔차분석이 그리 중요하지 않다던가… 연속형 변수 사용에 유의해야 한다던가… \n시간의 분포를 보면 연구의 대략적인 결과를 파악할 수 있다. 생존분석에서 시간은 사망하거나(event), 연구에서 중도탈락(censoring)한 두 가지 경우 중 하나이다. 시간의 histogram을 보면 전체적으로 right shifted 된 느낌인 가운데 0과 200근처가 우뚝 서있는 것을 볼 수 있다. 0에 가까운 봉우리는 아마도 작심삼일 들일 것이며, 200에 가까운 봉우리는 연구 종료에 따른 censoring일 것이다. 위 그림을 보면 연구 종료를 제외하면 censoring이 없음을 알 수 있다. 이는 이 연구가 굉장히 잘 관리된 연구 결과임을 보여준다.\n \n대부분의 연구에서 무시되고 있는 내용인데, 생존분석에서 중도탈락은 무작위(random)하게 이루어지는 것으로 가정된다. 하지만 실제 연구에서는 중도탈락이 의미를 지니는 경우가 많다. 치료약의 부작용이 심하거나, 환자가 스스로 판단하기에 호전이 없을 경우, 무작위 배정 결과가 중간에 환자에게 노출되어 위약군 환자가 연구에서 빠져나가는 경우… 어찌 보면 중도탈락은 사망결과만큼이나 중요한 결과이지만 일반적인 cox분석 결과에서는 중도탈락을 결과물로 제시하지 않는다. 하지만 연구 결과를 판단할 때 신약의 치료효과가 아무리 좋더라도 이해할 수 없는 이유로 치료군 혹은 placebo군의 중도탈락이 반대편에 비해 의미가 있을 정도로 많다면 그 결과는 신뢰하지 않는 것이 바람직하다.\n로지스틱회귀분석에서와 같이 생존분석에서도 연속변수를 분석하는 데에는 주의가 따른다. 따라서 적절한 값을 기준으로 잘라서 범주형 변수로 분석하는 것이 합리적이다. 하지만 대상자의 수가 많지 않아서 범주형 변수로 잘랐을 때 통계적으로 유의해지지 않는다던가, 범주형 변수로 변환하더라도 어느 값을 기준으로 몇 개의 범주로 나누는 것이 합리적인지 판단할 때에는 연속형 변수를 이용한 분석이 도움이 될 때도 있다. 이때 사용되는 것이 Martingale 잔차이지만 SPSS에서는 이를 지원하지 않기 때문에, SPSS를 이용해서 생존분석을 하는 경우 연속형 변수를 분석에 투입하면 안 된다.\n변수를 범주화시킬 때 주의사항은 임상적인 의미를 가지는 절단점이 있는 경우 그 점을 우선시하되, 나눈 후 각 범주에 속한 대상자들이 어느 정도 균등해지는 것도 고려해야 한다. 만약 systolic BP를 기준으로 한다고 했을 때 가장 최적의 절단점은 140일 것이다. 하지만 어떤 이유로 140mmHg 이상 혹은 이하의 대상자가 아주 많거나 적으면 그 변수를 분석에 투입한다고 해도 의미를 잃을 것이다. 그렇다고 systoloc BP를 quantile 값으로 나눠서 넣으면 분석은 잘되겠지만, 논문의 심사자 혹은 독자들은 왜 이걸 이리 잘랐지?라는 의문을 가질 수밖에 없다. 따라서 케이스 바이 케이스의 적절한 판단이 요구될 수 밖에 없다.\nproportional hazard 가정을 확인하는 방법으로는\n1) Kaplan curve 이용한 육안적 확인\n2) Schenofeld residual 그래프 이용한 육안적 확인\n3) Scheonfeld method 이용한 통계적 확인\n4) time dependent covariate 투입 후 통계적 유의성 확인 \n4가지 방법이 있다. SPSS에서는 2와 3번 방법은 불가능하다. 하지만 생존분석에서 비례위험가정의 중요성을 감안하면 다른 방법이 존재하며 이 부분은 SPSS의 한계가 있음을 알아야 한다.\n\n\n폐암환자의 성별에 따른 Kaplan curve와 위험함수 그래프이다. 앞에서 언급했듯이 논문에 Kaplan curve를 넣는 이유는 비례위험가정을 만족하는지 확인하기 위함이라고 하였다. Cox 분석에서 통계적으로 유의성을 가지는 변수가 (유의성 없는 변수는 접점이 몇 개 있던 몇 번 교차하던 상관없다.) Kaplan curve에서 접점을 지니거나 교차하면 비례위험 가정에서 벗어나 있을 가능성을 고려해야 한다. \n다만 실제 논문에는 위의 예제와 같이 성별에 따른 Kaplan curve보다는 논문의 주 목표인 치료법에 의한 Kaplan curve를 주로 넣게 될 것 같다. 문제는 시간이 어느 정도 흘러서 양쪽 그룹에 대상자가 줄어들게 되면 Kaplna curve에서는 비례위험 가정을 만족하는 상태에서도 접점 비슷한 모양이 보일 수 있다는 점이다. 당장 위의 예제만 봐도 시간 800근처에서 접점이 생길 뻔했다. \n비례위험가정의 확인은 어디까지나 위험함수를 가지고 하는 것이다. 편의 상 Kaplan curve를 논문에 넣기는 하지만 위의 예와 같이 누적위험함수에서는 별 문제없어 보이지만 Kaplan curve에서는 접점 비슷한 것이 보이는 경우, Kaplan curve를 그림으로 넣은 상황에서는 논문 내에 비례위험 가정이 어쩌고저쩌고 말로 변명해봐야 소용없는 경우가 있을 수 있다. 따라서 이런 경우 약간 덜 일반적일지라도 차라리 위험함수곡선을 제시하거나 아니면 SPSS말고 다른 통계 패키지를 이용해서 Schenfeld 잔차를 구해 이를 제시하고 그림은 Cox curve를 제시하는 것도 하나의 방법이다. \n하여간 Kaplan curve건 위험 함수 그래프이건 접점을 가지는 것은 비례위험가정을 만족하지 않는다는 것을 보여주는 상황이며 이에 대해서는 적절한 대응이 필요하다.\n\n\n분석팁\nEMR로부터 다운로드하거나 연구가가 작성한 자료는 주로 엑셀파일형태이다. 이를 R로 읽어오면 엑셀에서 각 컬럼별로 자료형태를 지정해 두었더라도 달라질 수 있어 적절한 숫자형(연속형변수)이나 문자형(범주형변수)으로 수정해야 한다.\n연속형변수의 생존분석을 위해서는 2분화가 일반적인데 cut-off를 결정하기 위해 ROC 분석보다는 maxstat 패키지의 maxstat.test()를 이용한다. (https://rpubs.com/cardiomoon/84975)\n비례위험가정과 log-rank test에서의 유의성 확인을 위해 Kaplan-Meier plot를 그린다. ’survminer’패키지의 ’ggsurvplot’함수\n이때 재발여부 등의 값은 단순히 numeric으로 0과 1로만 지정해야 할 수 있다. factor로 지정하면 Right censoered data only 오류가 발생할 수 있다.\nCox 단변량 다변량분석을 한다.\n웹에서 하는 R통계(web-r.org)에서는 이와 같은 방법으로 생존분석을 자동화하여 연속형변수는 cutpoint를 구해 자동으로 그래프를 그려준다. 또한 여러개의 변수를 영향변수(독립변수)로 넣어주면 각각의 변수에 대한 Cox비례위험 모형 및 각각의 survival curve를 그려주고 다변량 분석 및 stepwise backward elemination 까지 한번 입력으로 얻을 수 있으며 그 결과를 html또는 pdf로 다운받을 수 있고 그래프도 고해상도로 다운 받을 수 있다.\n다변량분석에서 결측치의 처리는 아래의 기준을 따른다.\n5% 이하의 결측치 비율: (이 내용는 chatGPT의 추천이므로 추후 검증이 필요함.)\n참고문헌: Little, R.J.A. & Rubin, D.B. (2002). Statistical Analysis with Missing Data (2nd Edition). Wiley-Interscience. ISBN: 978-0471183860. 근거: Little과 Rubin의 연구에서는 결측치가 5% 이하일 때, 무작위 결측(MCAR, Missing Completely at Random)으로 간주될 수 있으며, 이는 분석 결과에 큰 영향을 미치지 않는다는 점을 강조합니다. 따라서 결측치가 5% 이하인 경우, 데이터를 분석에 포함시키고 단순히 대체하거나 무시할 수 있습니다.\n5% ~ 20%의 결측치 비율: (이 내용는 chatGPT의 추천이므로 추후 검증이 필요함.)\n참고문헌: Schafer, J.L. (1997). Analysis of Incomplete Multivariate Data. CRC Press. ISBN: 978-0412040610. 근거: Schafer의 연구에서는 결측치가 5%에서 20% 사이일 경우, 결측치의 패턴과 원인을 분석하고 적절한 대체 방법을 사용하는 것이 중요하다고 언급합니다. 이 범위 내의 결측치는 분석 방법에 따라 처리 방법을 다르게 적용할 수 있으며, 결측치 대체 방법의 선택이 분석 결과에 중요한 영향을 미칠 수 있습니다.\n결측치를 대체는 일반적으로 간편하게는 평균값으로 대체하는 것과 다소 복잡하게는 회귀모델을 이용하는 방법이 있다. 자세한 방법은 R 코딩 예시를 참고한다.\n암환자 생존분석자료의 검증 분석대상자들의 나이에 대한 검증 해당암의 우리나라 연령대별 발생율과 비교하는 것을 고려해 보았다. 5세구간으로 남녀가 비교가 가능하다. 대장암자료로 생존분석한 예시 코드 참고를 바란다.",
    "crumbs": [
      "Statistics",
      "Survival analysis"
    ]
  },
  {
    "objectID": "posts/shiny/shiny_example.html",
    "href": "posts/shiny/shiny_example.html",
    "title": "Shiny Application Example",
    "section": "",
    "text": "Shiny는 R로 웹 어플리케이션을 만들 수 있게 해주는 패키지입니다. 이 문서에서는 API로 구한 데이터를 이용하여 Shiny Application 으로 시각화를 구현하는 예제를 만들어보겠습니다.\n1단계: RStudio에서 Shiny Application 내장된 예제 프로젝트 로딩하기\n\n\n\n\n\n\n1단계 예시\n\n\n\n\n\n\nRStudio File 메뉴에서 New Project &gt; New Directory &gt; Shiny Application을 차례로 선택하고 프로젝트 디렉토리를 C:/Projects 하부에 아래의 예시와 같이 만듭니다.\n\n\n\n\nCreat Shiny Application message box\n\nR-4.4.1-Shiny_Application_Example\n\n\n\ngit repository와 renv도 선택하여 진행하시는 것을 추천 드립니다.\n프로젝트 폴더에 app.R 파일이 생성되어 있으며 이를 열어보면 아래의 예제코드가 보입니다.\n\n\n\n\napp.R\n\n# This is a Shiny web application. You can run the application by clicking\n# the 'Run App' button above.\n#\n# Find out more about building applications with Shiny here:\n#\n#    https://shiny.posit.co/\n#\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n이 코드는 기본적인 Shiny Application 예제 코드입니다. Figure 1 에서 보이는 Run App 버튼을 틀릭하여 코드를 실행시켜보면 Output/Viewer pane에서 자신의 로컬컴퓨터 상에서 웹 어플리케이션이 실행되는 것을 확인할 수 있습니다.\n\n\n\n\n\n\n\nFigure 1: The Run App button can be found at the top-right of the source pane.\n\n\n\n\n실행된 application의 왼쪽 사이드에 있는 Number of bins을 욺직이면 우측 히스토그램에서 x축 간격이 달라짐을 보실 수 있습니다.\n\n\n\n\n2단계: Shiny Application의 핵심 구조 이해하기\n\n\n\n\n\n\n2단계 예시\n\n\n\n\n\n\n1단계에서 로딩한 내장된 예제코드를 아래와 같이 핵심구조만 남기고 삭제하여 매우 단순화 시키고 일부는 대체를 하여 shiny Application의 핵심구조에 대해 설명하겠습니다.\n\n\n\n\napp.R\n\nlibrary(shiny)\n\n# Define UI for application\nui &lt;- fluidPage(\n      textInput(\"user_input\", \"문자열을 입력하세요:\", \"\"),  # User input field\n      textOutput(\"output_text\")  # Dynamic text output\n)\n\n# Define server logic \nserver &lt;- function(input, output) {\n  \n  # Render the concatenated text\n  output$output_text &lt;- renderText({\n    paste(\"사용자가 아래의 문자열을 입력하셨습니다:\", input$user_input)  # Concatenate using paste()\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n단순화된 app.R은 3가지 주요 부분으로 구성되어 있습니다. 사용자 인터페이스 (UI) 정의:\n\n첫 번째 부분은 사용자 인터페이스(UI)로, 화면에서 어떤 입력을 받고, 어떤 출력을 할지 정의합니다. 여기서는 ui &lt;- fluidPage()로 구현되어 있습니다.\n\nfluidPage()는 반응형 레이아웃을 지원하는 화면 구성 함수로, 다양한 디바이스(데스크탑, 태블릿, 모바일)에서 화면 크기에 맞춰 UI가 자동으로 조정됩니다. 이 함수 내부에는 입력과 출력을 처리하는 함수들이 포함됩니다.\n입력 함수: textInput(“user_input”, “문자열을 입력하세요:”, ““)는 텍스트 입력을 받는 함수입니다.\n첫 번째 인자인 “user_input”은 입력된 값을 저장할 변수명입니다.\n두 번째 인자인 “문자열을 입력하세요:”는 입력 필드의 라벨로, 사용자가 텍스트를 입력할 때 보이는 안내 문구입니다.\n세 번째 인자인 ““는 기본값으로, 입력 필드가 처음에 비어 있음을 의미합니다.\n출력 함수: textOutput(“output_text”)는 출력 결과를 화면에 표시하는 함수입니다.\n첫 번째 인자인 “output_text”는 출력할 값을 저장된 변수명입니다.\n\n두 번째 부분은 서버(server) 함수로, 사용자 인터페이스에서 입력된 값을 어떻게 처리하고 출력할지를 정의합니다.\n\n서버 함수는 server &lt;- function(input, output) {}로 구성됩니다. 여기서:\ninput은 사용자가 입력한 값들이 저장된 참조형 반응형 리스트 객체입니다.\noutput은 변환된 출력값이 저장될 참조형 반응형 리스트 객체입니다.\n서버 함수 내부에서는 renderText() 함수를 사용해 텍스트를 처리하고, 그 내부에서 paste() 함수를 사용하여 입력된 텍스트를 출력할 텍스트와 결합합니다.\npaste() 함수는 input$user_input을 참조하여, UI에서 입력된 값을 가져옵니다. 이 값은 반응형 객체인 input을 통해 실시간으로 서버에 전달됩니다.\noutput$output_text에 직접 paste() 함수의 결과를 넣는 대신, renderText() 함수를 사용한 후 결과를 참조하게 됩니다. 이는 renderText()가 반응형으로 동작하여, 사용자가 입력할 때마다 출력값을 실시간으로 갱신해주기 때문입니다.\n\n마지막 부분은 Shiny 애플리케이션을 실행하는 shinyApp(ui = ui, server = server)입니다. 이 함수는 정의된 UI와 서버 로직을 결합하여 애플리케이션을 실행시킵니다.\n\n이제 Figure 1 에서 보이는 Run App 버튼을 틀릭하여 코드를 실행시켜보면 Output/Viewer pane에서 자신의 로컬컴퓨터 상에서 웹 어플리케이션이 실행되는 것을 확인할 수 있습니다.\n\n입력을 하는 textInput 필드에 “Hello, Shiny!”를 입력하면 아래의 출력창에 “사용자가 아래의 문자열을 입력하셨습니다: Hello, Shiny!”가 출력됩니다. 그리고 새로운 문자를 다시 입력해 보시면 새로운 문자가 보임을 알 수 있습니다.\n이로써 핵심기능에 대한 설명을 마칩니다.\n\n\n\n\n3단계: API 다운로드 데이터 Shiny Application을 통한 시각화 예제\n\n\n\n\n\n\n3단계 예시\n\n\n\n\n\n\n3단계에서 API 데이터를 shiny Application으로 그래프를 그려주는 예제입니다. 아래의 코드를 app.R 파일에 복사하여 붙여넣기 하시면 됩니다.\n\n\n\n\napp.R\n\n################################################################################\n## API data download\n################################################################################\nlibrary(rjson)\nlibrary(httr)\n\n# API 호출 정보 설정\nbase_url &lt;- \"http://apis.data.go.kr/B551172/getDiagnosisRemoteCancerous\"\ncall_url &lt;- \"AllCancerRemoteOccurrenceTrend\"\nmethod &lt;- \"GET\"\n\nMy_API_Key &lt;- \"wqdX2OnQY29zYQ7BXsGafDqVNaIbIYUoqAqS1bOeK6/yyqdukiVcRcj25wue+U8tqSaSXThVPwfaWDNpUc6cwQ==\"\n# 요청 파라미터 설정\nparams &lt;- list(\n  serviceKey = My_API_Key,  # 실제 API 키로 변경\n  pageNo = 1,\n  numOfRows = 10,\n  resultType = \"json\"\n)\n\n# API 호출\nurl &lt;- paste0(base_url, \"/\", call_url)\nresponse &lt;- GET(url, query = params)\n\n# 응답 상태 확인\n# if (http_status(response) == 200)\nif (status_code(response) == 200)  {\n  # JSON 데이터 파싱\n  print(response)\n  str(response)\n  \n} else {\n  print(paste(\"API 호출 실패:\", status_code(response)))\n}\n\njson_text &lt;- content(response, as = \"text\")\nprint(json_text)\nprint(\"------------------\")\n\ndata &lt;- fromJSON(json_text)\nprint(data)\n\n# 예시: 리스트 내부에 있는 항목을 추출하여 데이터프레임으로 변환\ndata_list &lt;- data$items  # 적절한 필드로 접근\n\n# 데이터프레임으로 변환\ndf &lt;- as.data.frame(do.call(rbind, lapply(data_list, as.data.frame)))\n\n################################################################################\n## ShinyApp\n################################################################################\n\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Define UI for the application\nui &lt;- fluidPage(\n\n  # Application title\n  titlePanel(\"API Data Vizualization with ShinyApp\"),\n  \n  # Sidebar layout\n  sidebarLayout(\n    sidebarPanel(\n      # Dropdown to select Y axis variable\n      selectInput(\"y_var\", \n                  \"Choose Y-axis Variable:\", \n                  choices = c(\"TOTAL\", \"VALUE\"),\n                  selected = \"TOTAL\")  # Default to TOTAL\n    ),\n    \n    # Main panel to display the plot\n    mainPanel(\n      plotOutput(\"yearPlot\")\n    )\n  )\n)\n\n# Define server logic to create the plot based on user selection\nserver &lt;- function(input, output) {\n  \n  # Render the plot\n  output$yearPlot &lt;- renderPlot({\n\n    # Plot using the selected Y variable\n    ggplot(data = df, aes(x = YEAR, y = .data[[input$y_var]])) +\n      geom_point() +\n      labs(x = \"Year\", y = input$y_var)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n위 코드는 크게 API 데이터 다운로드 부분과 ShinyApp 부분으로 나누어져 있으며 각 부분은 주석으로 구역을 구분해 두었습니다.\nAPI를 통해 데이터를 다운로드 하는 부분은 설명을 생략하겠습니다.\nui에서 달라진 점은 fluidPage 함수 내부에 titlePanel 함수와 sidebarLayout 함수가 있으며, sidebarPanel과 mainPanel로 구성되어 있습니다.\ntitlePanel 함수는 애플리케이션의 제목을 정의합니다. Figure 2\n\n\n\n\n\n\n\nFigure 2: Structure of a basic app with sidebar\n\n\n\n\nsidebarLayout 함수는 사이드바와 메인 패널을 정의합니다. Figure 2\n여기서는 textInput 함수 대신에 selectInput 함수를 사용하여 드롭다운 메뉴를 만들었습니다.\n출력함수로 textOutput 대신 plotOutput 함수를 사용하여 그래프를 출력합니다.\n서버로직 내에서는 renderText 대신 renderPlot 함수를 사용하여 그래프를 그립니다.\ninput 객체의 y_var 변수를 참조하여 y 축 값을 선택하고 y 축 제목을 지정하고 있습니다.\n그래프에서 사용하는 data는 앞단의 API downloaded data를 data.frame으로 만든 df를 그대로 사용하시면 됩니다.\n위 코드를 실행할 때에는 아래의 패키지들을 설치한 후 진행하셔야 합니다.\n실행하기 전에 아래의 패키지를 설치해야 합니다.\n\n\n\n\nR Console pane\n\nrenv::install(\"rjson\")\n\n\n\n\n\nR Console pane\n\nrenv::install(\"httr\")\n\n\n\n\n\nR Console pane\n\nrenv::install(\"ggplot2\")\n\n\n\n실행을 하면 아래와 같은 화면이 나타납니다.\n\n\n\n\n\n\n\nFigure 3: API data visualization with ShinyApp\n\n\n\n\nR Shiny applicaion을 이용한 다양한 시각화 예제는 Posit사의 사이트 (https://shiny.posit.co/r/gallery/)를 참고하시길 바랍니다.",
    "crumbs": [
      "R",
      "Shiny Application Example"
    ]
  },
  {
    "objectID": "posts/shiny/shiny_example.html#introduction",
    "href": "posts/shiny/shiny_example.html#introduction",
    "title": "Shiny Application Example",
    "section": "",
    "text": "Shiny는 R로 웹 어플리케이션을 만들 수 있게 해주는 패키지입니다. 이 문서에서는 API로 구한 데이터를 이용하여 Shiny Application 으로 시각화를 구현하는 예제를 만들어보겠습니다.\n1단계: RStudio에서 Shiny Application 내장된 예제 프로젝트 로딩하기\n\n\n\n\n\n\n1단계 예시\n\n\n\n\n\n\nRStudio File 메뉴에서 New Project &gt; New Directory &gt; Shiny Application을 차례로 선택하고 프로젝트 디렉토리를 C:/Projects 하부에 아래의 예시와 같이 만듭니다.\n\n\n\n\nCreat Shiny Application message box\n\nR-4.4.1-Shiny_Application_Example\n\n\n\ngit repository와 renv도 선택하여 진행하시는 것을 추천 드립니다.\n프로젝트 폴더에 app.R 파일이 생성되어 있으며 이를 열어보면 아래의 예제코드가 보입니다.\n\n\n\n\napp.R\n\n# This is a Shiny web application. You can run the application by clicking\n# the 'Run App' button above.\n#\n# Find out more about building applications with Shiny here:\n#\n#    https://shiny.posit.co/\n#\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n이 코드는 기본적인 Shiny Application 예제 코드입니다. Figure 1 에서 보이는 Run App 버튼을 틀릭하여 코드를 실행시켜보면 Output/Viewer pane에서 자신의 로컬컴퓨터 상에서 웹 어플리케이션이 실행되는 것을 확인할 수 있습니다.\n\n\n\n\n\n\n\nFigure 1: The Run App button can be found at the top-right of the source pane.\n\n\n\n\n실행된 application의 왼쪽 사이드에 있는 Number of bins을 욺직이면 우측 히스토그램에서 x축 간격이 달라짐을 보실 수 있습니다.\n\n\n\n\n2단계: Shiny Application의 핵심 구조 이해하기\n\n\n\n\n\n\n2단계 예시\n\n\n\n\n\n\n1단계에서 로딩한 내장된 예제코드를 아래와 같이 핵심구조만 남기고 삭제하여 매우 단순화 시키고 일부는 대체를 하여 shiny Application의 핵심구조에 대해 설명하겠습니다.\n\n\n\n\napp.R\n\nlibrary(shiny)\n\n# Define UI for application\nui &lt;- fluidPage(\n      textInput(\"user_input\", \"문자열을 입력하세요:\", \"\"),  # User input field\n      textOutput(\"output_text\")  # Dynamic text output\n)\n\n# Define server logic \nserver &lt;- function(input, output) {\n  \n  # Render the concatenated text\n  output$output_text &lt;- renderText({\n    paste(\"사용자가 아래의 문자열을 입력하셨습니다:\", input$user_input)  # Concatenate using paste()\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n단순화된 app.R은 3가지 주요 부분으로 구성되어 있습니다. 사용자 인터페이스 (UI) 정의:\n\n첫 번째 부분은 사용자 인터페이스(UI)로, 화면에서 어떤 입력을 받고, 어떤 출력을 할지 정의합니다. 여기서는 ui &lt;- fluidPage()로 구현되어 있습니다.\n\nfluidPage()는 반응형 레이아웃을 지원하는 화면 구성 함수로, 다양한 디바이스(데스크탑, 태블릿, 모바일)에서 화면 크기에 맞춰 UI가 자동으로 조정됩니다. 이 함수 내부에는 입력과 출력을 처리하는 함수들이 포함됩니다.\n입력 함수: textInput(“user_input”, “문자열을 입력하세요:”, ““)는 텍스트 입력을 받는 함수입니다.\n첫 번째 인자인 “user_input”은 입력된 값을 저장할 변수명입니다.\n두 번째 인자인 “문자열을 입력하세요:”는 입력 필드의 라벨로, 사용자가 텍스트를 입력할 때 보이는 안내 문구입니다.\n세 번째 인자인 ““는 기본값으로, 입력 필드가 처음에 비어 있음을 의미합니다.\n출력 함수: textOutput(“output_text”)는 출력 결과를 화면에 표시하는 함수입니다.\n첫 번째 인자인 “output_text”는 출력할 값을 저장된 변수명입니다.\n\n두 번째 부분은 서버(server) 함수로, 사용자 인터페이스에서 입력된 값을 어떻게 처리하고 출력할지를 정의합니다.\n\n서버 함수는 server &lt;- function(input, output) {}로 구성됩니다. 여기서:\ninput은 사용자가 입력한 값들이 저장된 참조형 반응형 리스트 객체입니다.\noutput은 변환된 출력값이 저장될 참조형 반응형 리스트 객체입니다.\n서버 함수 내부에서는 renderText() 함수를 사용해 텍스트를 처리하고, 그 내부에서 paste() 함수를 사용하여 입력된 텍스트를 출력할 텍스트와 결합합니다.\npaste() 함수는 input$user_input을 참조하여, UI에서 입력된 값을 가져옵니다. 이 값은 반응형 객체인 input을 통해 실시간으로 서버에 전달됩니다.\noutput$output_text에 직접 paste() 함수의 결과를 넣는 대신, renderText() 함수를 사용한 후 결과를 참조하게 됩니다. 이는 renderText()가 반응형으로 동작하여, 사용자가 입력할 때마다 출력값을 실시간으로 갱신해주기 때문입니다.\n\n마지막 부분은 Shiny 애플리케이션을 실행하는 shinyApp(ui = ui, server = server)입니다. 이 함수는 정의된 UI와 서버 로직을 결합하여 애플리케이션을 실행시킵니다.\n\n이제 Figure 1 에서 보이는 Run App 버튼을 틀릭하여 코드를 실행시켜보면 Output/Viewer pane에서 자신의 로컬컴퓨터 상에서 웹 어플리케이션이 실행되는 것을 확인할 수 있습니다.\n\n입력을 하는 textInput 필드에 “Hello, Shiny!”를 입력하면 아래의 출력창에 “사용자가 아래의 문자열을 입력하셨습니다: Hello, Shiny!”가 출력됩니다. 그리고 새로운 문자를 다시 입력해 보시면 새로운 문자가 보임을 알 수 있습니다.\n이로써 핵심기능에 대한 설명을 마칩니다.\n\n\n\n\n3단계: API 다운로드 데이터 Shiny Application을 통한 시각화 예제\n\n\n\n\n\n\n3단계 예시\n\n\n\n\n\n\n3단계에서 API 데이터를 shiny Application으로 그래프를 그려주는 예제입니다. 아래의 코드를 app.R 파일에 복사하여 붙여넣기 하시면 됩니다.\n\n\n\n\napp.R\n\n################################################################################\n## API data download\n################################################################################\nlibrary(rjson)\nlibrary(httr)\n\n# API 호출 정보 설정\nbase_url &lt;- \"http://apis.data.go.kr/B551172/getDiagnosisRemoteCancerous\"\ncall_url &lt;- \"AllCancerRemoteOccurrenceTrend\"\nmethod &lt;- \"GET\"\n\nMy_API_Key &lt;- \"wqdX2OnQY29zYQ7BXsGafDqVNaIbIYUoqAqS1bOeK6/yyqdukiVcRcj25wue+U8tqSaSXThVPwfaWDNpUc6cwQ==\"\n# 요청 파라미터 설정\nparams &lt;- list(\n  serviceKey = My_API_Key,  # 실제 API 키로 변경\n  pageNo = 1,\n  numOfRows = 10,\n  resultType = \"json\"\n)\n\n# API 호출\nurl &lt;- paste0(base_url, \"/\", call_url)\nresponse &lt;- GET(url, query = params)\n\n# 응답 상태 확인\n# if (http_status(response) == 200)\nif (status_code(response) == 200)  {\n  # JSON 데이터 파싱\n  print(response)\n  str(response)\n  \n} else {\n  print(paste(\"API 호출 실패:\", status_code(response)))\n}\n\njson_text &lt;- content(response, as = \"text\")\nprint(json_text)\nprint(\"------------------\")\n\ndata &lt;- fromJSON(json_text)\nprint(data)\n\n# 예시: 리스트 내부에 있는 항목을 추출하여 데이터프레임으로 변환\ndata_list &lt;- data$items  # 적절한 필드로 접근\n\n# 데이터프레임으로 변환\ndf &lt;- as.data.frame(do.call(rbind, lapply(data_list, as.data.frame)))\n\n################################################################################\n## ShinyApp\n################################################################################\n\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Define UI for the application\nui &lt;- fluidPage(\n\n  # Application title\n  titlePanel(\"API Data Vizualization with ShinyApp\"),\n  \n  # Sidebar layout\n  sidebarLayout(\n    sidebarPanel(\n      # Dropdown to select Y axis variable\n      selectInput(\"y_var\", \n                  \"Choose Y-axis Variable:\", \n                  choices = c(\"TOTAL\", \"VALUE\"),\n                  selected = \"TOTAL\")  # Default to TOTAL\n    ),\n    \n    # Main panel to display the plot\n    mainPanel(\n      plotOutput(\"yearPlot\")\n    )\n  )\n)\n\n# Define server logic to create the plot based on user selection\nserver &lt;- function(input, output) {\n  \n  # Render the plot\n  output$yearPlot &lt;- renderPlot({\n\n    # Plot using the selected Y variable\n    ggplot(data = df, aes(x = YEAR, y = .data[[input$y_var]])) +\n      geom_point() +\n      labs(x = \"Year\", y = input$y_var)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n위 코드는 크게 API 데이터 다운로드 부분과 ShinyApp 부분으로 나누어져 있으며 각 부분은 주석으로 구역을 구분해 두었습니다.\nAPI를 통해 데이터를 다운로드 하는 부분은 설명을 생략하겠습니다.\nui에서 달라진 점은 fluidPage 함수 내부에 titlePanel 함수와 sidebarLayout 함수가 있으며, sidebarPanel과 mainPanel로 구성되어 있습니다.\ntitlePanel 함수는 애플리케이션의 제목을 정의합니다. Figure 2\n\n\n\n\n\n\n\nFigure 2: Structure of a basic app with sidebar\n\n\n\n\nsidebarLayout 함수는 사이드바와 메인 패널을 정의합니다. Figure 2\n여기서는 textInput 함수 대신에 selectInput 함수를 사용하여 드롭다운 메뉴를 만들었습니다.\n출력함수로 textOutput 대신 plotOutput 함수를 사용하여 그래프를 출력합니다.\n서버로직 내에서는 renderText 대신 renderPlot 함수를 사용하여 그래프를 그립니다.\ninput 객체의 y_var 변수를 참조하여 y 축 값을 선택하고 y 축 제목을 지정하고 있습니다.\n그래프에서 사용하는 data는 앞단의 API downloaded data를 data.frame으로 만든 df를 그대로 사용하시면 됩니다.\n위 코드를 실행할 때에는 아래의 패키지들을 설치한 후 진행하셔야 합니다.\n실행하기 전에 아래의 패키지를 설치해야 합니다.\n\n\n\n\nR Console pane\n\nrenv::install(\"rjson\")\n\n\n\n\n\nR Console pane\n\nrenv::install(\"httr\")\n\n\n\n\n\nR Console pane\n\nrenv::install(\"ggplot2\")\n\n\n\n실행을 하면 아래와 같은 화면이 나타납니다.\n\n\n\n\n\n\n\nFigure 3: API data visualization with ShinyApp\n\n\n\n\nR Shiny applicaion을 이용한 다양한 시각화 예제는 Posit사의 사이트 (https://shiny.posit.co/r/gallery/)를 참고하시길 바랍니다.",
    "crumbs": [
      "R",
      "Shiny Application Example"
    ]
  },
  {
    "objectID": "posts/Resources/introduction_to_resources.html",
    "href": "posts/Resources/introduction_to_resources.html",
    "title": "Introduction to Resources",
    "section": "",
    "text": "Introducing medical (big) data resources that are helpful for research.",
    "crumbs": [
      "Resources",
      "Introduction to Resources"
    ]
  },
  {
    "objectID": "posts/R-setup/R_setup.html",
    "href": "posts/R-setup/R_setup.html",
    "title": "R setup",
    "section": "",
    "text": "While the official R documentation on CRAN (The Comprehensive R Archive Network, https://cran.r-project.org/index.html) under Manuals and R Installation and Administration provides comprehensive guidance, the following summary can also be helpful.",
    "crumbs": [
      "R",
      "R setup"
    ]
  },
  {
    "objectID": "posts/R-setup/R_setup.html#r-installation-guide-document",
    "href": "posts/R-setup/R_setup.html#r-installation-guide-document",
    "title": "R setup",
    "section": "",
    "text": "While the official R documentation on CRAN (The Comprehensive R Archive Network, https://cran.r-project.org/index.html) under Manuals and R Installation and Administration provides comprehensive guidance, the following summary can also be helpful.",
    "crumbs": [
      "R",
      "R setup"
    ]
  },
  {
    "objectID": "posts/R-setup/R_setup.html#r-installation-file",
    "href": "posts/R-setup/R_setup.html#r-installation-file",
    "title": "R setup",
    "section": "R Installation File",
    "text": "R Installation File\nDownload the latest version of R for your operating system from CRAN (https://cran.r-project.org/index.html). As of September 27, 2024, the latest 64-bit Windows installation file is R-4.4.1-win.exe.\n\nInstallation Path\nBy default, R is installed in C:\\\\Program Files\\\\R\\\\R-4.4.1 on Windows. However, the following path is recommended by the study group.\n\n\n\nR Install Option Window\n\nC:\\R\\R-4.4.1\n\n\n\n\n\n\n\n\nPros and Cons of C:\\Program Files\\ and C:\\ Paths\n\n\n\n\n\n\nMost Windows applications are installed in the Program Files folder, making it a standard location for managing system software and applications, which facilitates system organization. The Program Files folder requires special system permissions, making it difficult for regular users to easily modify files within the folder, thus protecting against changes by malicious software. However, it can be inconvenient for users who want to use R freely as administrative privileges are required for installing or updating R packages, and the Program Files path contains spaces, which may cause issues with older scripts or tools that do not handle spaces in paths correctly.\nDirect installation under C:\\ (e.g., C:\\R): Installing or updating packages does not require administrative privileges, and since there are no spaces in the path, this path can be used in all scripts or programs without compatibility issues. However, programs installed with user permissions may be relatively less secure, more susceptible to changes by malicious software, and if non-standard installation locations are used, system software and applications may be distributed, making management difficult.\nIn conclusion, when choosing an installation location, factors such as security, user convenience, and ease of system management should be considered. A custom path like C:\\R may be more convenient for individual users or development environments, while installing under C:\\Program Files may be more suitable for corporate environments or situations where security is a priority.\n\n\n\n\n\n\nEnvironment Variable Configuration\nAfter installation, you need to specify the R executable file path in the system environment variables.\n\nWhen calling R from users or programs like RStudio, or when installing packages, the system needs to know the installation path of the R executable file to proceed smoothly.\nA quick way to do this is to open the Run dialog box by pressing Win+R, type sysdm.cpl, select Environment Variables under the Advanced tab of System Properties, choose Path in the System variables list, and add or edit the path of the R executable file.\n\n\n\n\nRun Dialog Box\n\nsysdm.cpl\n\n\n\n\nInstallation Verification\n\nTo verify a successful installation, run R and check the version. Since the path to the R executable file is registered in the environment variables, you can run R regardless of the folder where the run command is executed. To do this, open the Run dialog box by pressing Win+R, type cmd to open the Command Prompt, and type “R” to run R. Check if the R version displayed in the Command Prompt matches the installed version.\n\n\n\n\nRun Dialog Box\n\ncmd\n\n\n\n\n\nCommand Prompt\n\nR",
    "crumbs": [
      "R",
      "R setup"
    ]
  },
  {
    "objectID": "posts/R-setup/R_setup.html#rtools-installation",
    "href": "posts/R-setup/R_setup.html#rtools-installation",
    "title": "R setup",
    "section": "Rtools Installation",
    "text": "Rtools Installation\n\nRtools Features and Roles\nRtools is a collection of development tools related to the R programming language, primarily used on Windows operating systems. The main roles and features of Rtools include:\n\nPackage Compilation and Installation: When installing or updating R packages, especially some packages provided by CRAN, are provided in source code form. In such cases, Rtools helps compile the source code for installation.\nProviding GNU Build Tools: Rtools includes GNU build tools such as GCC (GNU Compiler Collection), make, tar, git, etc., providing a build environment similar to Linux in Windows.\nSupport for R Package Development: It provides various tools and libraries needed in the process of developing R packages. It is particularly useful when developing R packages that interact with C/C++ code.\nCommand Line Tools: It provides tools to perform R and related tasks from the command line, enabling automation of more sophisticated and complex tasks.\n\n\n\nInstallation File\nYou can download the Rtools installation file from CRAN (https://cran.r-project.org/index.html). As of June 14, 2024, the latest Windows installation file is rtools44-6104-6039.exe.\n\n\nInstallation Path\nThe default installation path, the root of the C: drive, is recommended (e.g., C:\\rtools44). The path to the Rtools executable file is automatically set in the system environment variables during the installation process. (If not, set it manually.)\n\n\nInstallation Verification\nTo verify the path setting in the system environment variables, run Sys.which(\"make\") in the R console. If it returns the installation path correctly, the installation is successful.",
    "crumbs": [
      "R",
      "R setup"
    ]
  },
  {
    "objectID": "posts/R-setup/R_setup.html#r-매뉴얼",
    "href": "posts/R-setup/R_setup.html#r-매뉴얼",
    "title": "R setup",
    "section": "R 매뉴얼",
    "text": "R 매뉴얼\nR을 심도있게 공부할려면 R 개발자들이 만든 매뉴얼 중 The R language definition과 R Internals이 적합할 것 같습니다 (https://cran.r-project.org/index.html). 하지만 처음부터 어려운 매뉴얼을 공부하는 것보다 자신의 자료를 분석하는데 필요한 명령과 함수들을 Help 기능 등으로 이해하는 방법으로 진행하는 것도 필요할 것 같습니다.",
    "crumbs": [
      "R",
      "R setup"
    ]
  },
  {
    "objectID": "posts/R-setup/R_setup.html#프로젝트-관리",
    "href": "posts/R-setup/R_setup.html#프로젝트-관리",
    "title": "R setup",
    "section": "프로젝트 관리",
    "text": "프로젝트 관리\n\n프로젝트 폴더 구조\n\n실행파일 설치 폴더\nR 새 버전으로 업데이트하면 드물겠지만 기존 코드나 사용 중인 패키지가 예상대로 작동하지 않을 수 있으며, 새 버전에서는 패키지의 지원이 변경될 수 있어 시스템의 안정성을 유지하기 위해 이전 버전의 실행파일을 유지하는 것도 필요합니다. 따라서 아래의 예시와 같은 폴더구조가 추천됩니다.\nC:\\R\\\n         ├─ R-4.4.0\\\n         └─ R-4.4.1\\",
    "crumbs": [
      "R",
      "R setup"
    ]
  },
  {
    "objectID": "posts/R-setup/R_setup.html#패키지-종속성-관리를-위한-renv",
    "href": "posts/R-setup/R_setup.html#패키지-종속성-관리를-위한-renv",
    "title": "R setup",
    "section": "패키지 종속성 관리를 위한 renv",
    "text": "패키지 종속성 관리를 위한 renv\n원래 R에서 패키지는 해당버전 R의 설치폴더 하부의 library 폴더에 설치됩니다. renv는 R 프로젝트에서 패키지 의존성을 관리하기 위해 설계된 도구로써, renv를 설치하고 활성화하면 해당 프로젝트 폴더 하부에 renv 폴더가 만들어지고, 그 하부에 패키지를 설치하게 됩니다. 또한 설치된 패키지들의 정보를 renv.lock 파일에 관리하게 됩니다. 이러한 방법으로 R에서는 프로젝트별로 패키지를 관리할 수 있으므로 필자를 이를 추천합니다.\nRStudio에서 새로운 프로젝트를 생성할 때 renv 사용여부를 check하면 자동으로 설정됩니다.\n사용법에 대해서는 https://rstudio.github.io/renv/articles/renv.html 참고하시길 바랍니다.",
    "crumbs": [
      "R",
      "R setup"
    ]
  },
  {
    "objectID": "posts/Quarto/introduction_to_Quarto.html",
    "href": "posts/Quarto/introduction_to_Quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Quarto is an open-source integrated publishing system developed by Posit (formerly RStudio) and officially released on April 18, 2022. Quarto provides differentiated features from the existing R Markdown, allowing the inclusion of execution results from various programming languages such as R and Python in integrated documents. This enables the output of documents in various formats such as MS Word, PDF, LaTeX, as well as powerful features as a digital publishing platform like responsive dynamic web pages. Quarto is a modern tool for reproducible research and data-centric reporting, supporting dynamic documentation in multi-language environments and integrating programming workflows.\nThe importance of reproducible research\nMedical research inherently requires reproducible research. Reproducibility refers to the ability to provide analysis processes and data transparently so that other researchers can derive the same results based on the same data. Quarto includes code written in languages such as R and Python in documents, and automatically executes the code when rendering the document to derive analysis results. This ensures that research results and analysis processes are consistently maintained and reproducible.\nIntegrated documentation and reporting\nQuarto is a versatile tool that allows easy creation of medical papers, reports, slide presentations, blogs, websites, and more. This provides a significant advantage for doctors to distribute and share analysis results in various formats. In particular, Quarto supports various output formats such as LaTeX, JATS, and Word, making it easy to create documents tailored to medical journal submission formats.\nData visualization and presentation\nVisualizing medical data is an important process that helps to easily understand complex statistical results. Quarto provides high-quality visual data through compatibility with advanced visualization tools, especially integrating easily with libraries such as R’s ggplot2 package or Python’s Matplotlib. This allows researchers to easily include sophisticated graphs and charts in documents and maximize visual effects in clinical presentations.",
    "crumbs": [
      "Quarto",
      "Introduction to Quarto"
    ]
  },
  {
    "objectID": "posts/Python/introduction_to_Python.html",
    "href": "posts/Python/introduction_to_Python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python was developed by Guido van Rossum, a Dutch programmer. He created Python while working at CWI (Centrum Wiskunde & Informatica) to overcome the limitations of the educational programming language ABC (A Basic Compiler) [1].\nThe first release of Python, version 0.9.0, was announced in 1991 and included core features of modern programming languages such as classes, exception handling, and functionality. Python was designed with clear syntax and concise code writing in mind, allowing easy integration of various extension modules and libraries due to its extensibility. Thanks to these characteristics, Python has been widely adopted by the global developer community and has become a preferred language in various technical fields such as data science, web development, automation scripts, and machine learning [2].\nHere are some examples of Python’s applications in the medical field:\n\nMedical Image Analysis: Python is used for MRI, CT, and X-ray image analysis. Libraries such as PyDicom, OpenCV, TensorFlow, and Keras are used for medical image analysis [[3]][4][5].\nGenomic Data Analysis: In bioinformatics, Python is used for gene sequence analysis and gene expression data processing. Biopython is an example of this application [6].\nClinical Research Data Management: Python supports the management and analysis of large-scale clinical research data. Pandas and NumPy provide statistical analysis capabilities [[7]][8].\nPredictive Modeling and Machine Learning: Python is used for developing disease onset prediction models using patient data. Scikit-learn and TensorFlow are used to build these models [[3]][9].\n\n\n\n\n\nReferences\n\n1. Rossum G van. Foreword for \"programming python\" [Internet]. 1996. Available from: https://www.python.org/doc/essays/foreword/\n\n\n2. Wikipedia. History of python [Internet]. 2024. Available from: https://en.wikipedia.org/wiki/History_of_Python\n\n\n3. Mason D. Pydicom: An open source DICOM library. Medical Physics [Internet]. 2011;38:3493–3. Available from: https://aapm.onlinelibrary.wiley.com/doi/abs/10.1118/1.3611983\n\n\n4. Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: Large-scale machine learning on heterogeneous systems. Mountain View, CA: Tensorflow; 2015.\n\n\n5. KERAS. Deep learning for humans [Internet]. 2024. Available from: https://keras.io/\n\n\n6. Cock PJ, Antao T, Chang JT, Chapman BA, Cox CJ, Dalke A, et al. Biopython: Freely available python tools for computational molecular biology and bioinformatics. Bioinformatics. 2009;25:1422.\n\n\n7. McKinney W, Team P. Pandas-powerful python data analysis toolkit. Pandas—Powerful Python Data Analysis Toolkit. 2015;1625.\n\n\n8. Numpy_team. NumPy: The fundamental package for scientific computing with python [Internet]. 2024. Available from: https://numpy.org/\n\n\n9. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research. 2011;12:2825–30.",
    "crumbs": [
      "Python",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html",
    "href": "posts/manuscript-example/manuscript_example.html",
    "title": "Manuscripting with Quarto Example",
    "section": "",
    "text": "RStudio를 실행하고 파일메뉴에서 New Project… &gt; New Directory &gt; New Project를 단계적으로 선택하고\nC:\\R\\Projects (=Default working directory) 하위에 Directory name 입력란에는 아래의 예시와 같이 R의 버전과 프로젝트를 상징하는 이름을 조합하여 입력할 것을 추천합니다.\n\n\n\n예시\n\nR-4.4.1-RPythonStudy_HANJM\n\n\ngit repository(=저장소) 만들기와 renv 사용하기 check box는 예제 R프로젝트를 다운로드 하기 위해서는 선택해야 합니다.\n\n이제 탐색기로 프로젝트 폴더(C:.1-RPythonStudy_HANJM)를 살펴보면 .git 폴더, .Rproj.user 폴더, renv 폴더, .gitignore 파일, .Rprofile 파일, R-4.4.1-RPythonStudy-HANJM.Rproj 파일, renv.lock 파일들이 생성되었음을 확인하실 수 있습니다.\n\ngit를 사용하기 때문에 설정이 보관되는 .git 폴더와 버전관리 예외(규칙)이 기록되어 있는 .gitignore 파일이 생성되어 있습니다. renv를 사용하기 때문에 프로젝트에 패키지가 설치되는 renv 폴더가 생성되었고 설치되는 패키지의 대한 기록이 되는 renv.lock 파일 생성되어 있습니다. RStudio에서 프로젝트로 이 폴더를 사용하기 때문에 .Rproj.user 폴더, .Rprofile 파일, R-4.4.1-RPythonStudy_HANJM.Rproj 파일이 생성되어 있습니다.\n\n\n\n이 과정을 위해서는 당연히 git가 미리 설치되어 있어야 합니다. 설치 되어 있는지 확인은 Console 창의 Terminal tab에서 “git –version”을 실행하여 버전이 출력되면 적절히 설치가 되어 있는 것입니다 (Window 기준).\n\n\n\nTerminal\n\ngit --version\n\n\n아래는 github 원격저장소에 있는 생존분석데이터를 이용한 샘플용 R프로젝트를 나의 R 프로젝트 폴더에 다운로드 받는 방법입니다.\n먼저 현재의git 프로젝트에 github 원격저장소의 URL을 등록해야 합니다. Git 명령을 입력하기 위해 아래의 그림의 console pane의 Terminal 탭으로 이동하여 git remote add 명령을 실행합니다.\n\n\n\n\nTerminal\n\ngit remote add origin https://github.com/RPythonStudy/HANJM.git\n\n\n그리고 원격저장소가 적절히 등록되었는지는 아래의 명령으로 확인하시면 됩니다.\n\n\n\nTerminal\n\ngit remote -v\n\n\n만약 이미 등록되어 있는데 다시 등록한다면 “error: remote origin already exists.”라는 메세지가 나올 것입니다.\n😅 그리고 다운로드 준비를 해야 하는데 git pull 명령으로 원격저장소의 모든 파일을 다운로드 할 예정인데 로컬 폴더에 중복되는 파일이 하나라도 있으면 오류가 발생하므로, 중복이 되는 파일인 .gitignore, .Rprofile과 renv.lock 을 로컬 프로젝트폴더에서 삭제해 줍니다.\n이후 아래의 git 명령으로 원격저장소의 모든 파일을 다운로드 받아 주면 됩니다.\n\n\n\nTerminal\n\ngit pull origin master\n\n\n(중복이 있더라도 원격저장소으 파일로 강제로 덮어쓰는 명령이 있지만 제가 시도할 때는 오류 메세지가 같이 나오기 때문에 일단 여기에는 소개하지 않습니다.)\n\n필자주: 이전에 다운로드 받은 적이 있고, 원격저장소의 파일이 갱신되어 있어 로컬저장소를 갱신하고자할 때로 위의 pull 명령을 사용하시면 됩니다. 이전 다운로드 이후에 로컬에서 수정한 적이 없으면 잘 다운로드 됩니다. 이후에는 renv::store() 명령으로 원격저장소의 renv.lock파일에 기록된 패키지들이 로컬과 차이가 있는지 점검하는 것이 좋습니다. 만약 로컬에서 파일을 수정하였기 때문에 로컬과 원격의 충돌이 발생한다면 아래의 명령을 시도해 봅니다. 이 경우 로컬의 파일들이 지워지고 자동으로 백업이 되지는 않기 때문에 필요시에는 백업을 미리 해 두셔야 합니다. 이렇게 해도 문제가 있다면 프로젝트 폴더를 삭제하고 처음부터 과정을 다시 진행하시길 바랍니다.\n\n\n\n\nTerminal\n\ngit fetch origin\ngit reset --hard origin/master\n\n\n\n\n\n원격저장소에는 예제 R프로젝트 파일은 있지만 이를 위해 필요한 R 패키지까지 같이 있지는 않습니다. 하지만 renv.lock 파일에는 설치된 패키지 버전과 목록이 기록되어 있으므로 이를 이용해서 설치하시면 됩니다.\nConsole pane의 Consle 탭으로 이동해서 renv::restore() 명령을 실행하시면 됩니다.\n\n\n\nConsole\n\nrenv::restore ()\n\n\n만약 이전에 renv::restore()를 수행하여 패키지가 이미 update된 상태에서 다시 위 명령을 수행한다면 “- The library is already synchronized with the lockfile.”이라는 메세지가 출력할 것이며 이미 update된 것 상태이므로 다음 단계를 진행하시면 됩니다.\n\n\n\n예제 R로 논문쓰기 프로젝트를 실행할 때 raw data로써 “deidentified_han20230213.xlsx”이 필요합니다. 이 파일은 이미 개인정보보호조치(개인정보익명화, 날짜정보를 날짜간의 차이정보로 변환)가 되어 있지만 연구회의 방침상 업로드는 되어 있지는 않습니다. 내부연구자 분들이 실습을 위해 위 파일이 필요한 경우에는 연구회에 연락 바랍니다. 이 파일을 프로젝트 폴더 하부의 raw_data 폴더를 만들고 위 xlsx 파일을 복사해 주시면 됩니다.\n\n\n\n예제 R로 논문쓰기 프로젝트가 로컬에서 준비가 완료되었습니다. 원하는 기능들이 R청크에서 어떻게 구현되었는지 확인하시고 코드줄, 블록 또는 청크단위로 실행하시면서 구현결과를 확인하시면 됩니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#빈-프로젝트-만들기",
    "href": "posts/manuscript-example/manuscript_example.html#빈-프로젝트-만들기",
    "title": "Manuscripting with Quarto Example",
    "section": "",
    "text": "RStudio를 실행하고 파일메뉴에서 New Project… &gt; New Directory &gt; New Project를 단계적으로 선택하고\nC:\\R\\Projects (=Default working directory) 하위에 Directory name 입력란에는 아래의 예시와 같이 R의 버전과 프로젝트를 상징하는 이름을 조합하여 입력할 것을 추천합니다.\n\n\n\n예시\n\nR-4.4.1-RPythonStudy_HANJM\n\n\ngit repository(=저장소) 만들기와 renv 사용하기 check box는 예제 R프로젝트를 다운로드 하기 위해서는 선택해야 합니다.\n\n이제 탐색기로 프로젝트 폴더(C:.1-RPythonStudy_HANJM)를 살펴보면 .git 폴더, .Rproj.user 폴더, renv 폴더, .gitignore 파일, .Rprofile 파일, R-4.4.1-RPythonStudy-HANJM.Rproj 파일, renv.lock 파일들이 생성되었음을 확인하실 수 있습니다.\n\ngit를 사용하기 때문에 설정이 보관되는 .git 폴더와 버전관리 예외(규칙)이 기록되어 있는 .gitignore 파일이 생성되어 있습니다. renv를 사용하기 때문에 프로젝트에 패키지가 설치되는 renv 폴더가 생성되었고 설치되는 패키지의 대한 기록이 되는 renv.lock 파일 생성되어 있습니다. RStudio에서 프로젝트로 이 폴더를 사용하기 때문에 .Rproj.user 폴더, .Rprofile 파일, R-4.4.1-RPythonStudy_HANJM.Rproj 파일이 생성되어 있습니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#github로부터-다운로드",
    "href": "posts/manuscript-example/manuscript_example.html#github로부터-다운로드",
    "title": "Manuscripting with Quarto Example",
    "section": "",
    "text": "이 과정을 위해서는 당연히 git가 미리 설치되어 있어야 합니다. 설치 되어 있는지 확인은 Console 창의 Terminal tab에서 “git –version”을 실행하여 버전이 출력되면 적절히 설치가 되어 있는 것입니다 (Window 기준).\n\n\n\nTerminal\n\ngit --version\n\n\n아래는 github 원격저장소에 있는 생존분석데이터를 이용한 샘플용 R프로젝트를 나의 R 프로젝트 폴더에 다운로드 받는 방법입니다.\n먼저 현재의git 프로젝트에 github 원격저장소의 URL을 등록해야 합니다. Git 명령을 입력하기 위해 아래의 그림의 console pane의 Terminal 탭으로 이동하여 git remote add 명령을 실행합니다.\n\n\n\n\nTerminal\n\ngit remote add origin https://github.com/RPythonStudy/HANJM.git\n\n\n그리고 원격저장소가 적절히 등록되었는지는 아래의 명령으로 확인하시면 됩니다.\n\n\n\nTerminal\n\ngit remote -v\n\n\n만약 이미 등록되어 있는데 다시 등록한다면 “error: remote origin already exists.”라는 메세지가 나올 것입니다.\n😅 그리고 다운로드 준비를 해야 하는데 git pull 명령으로 원격저장소의 모든 파일을 다운로드 할 예정인데 로컬 폴더에 중복되는 파일이 하나라도 있으면 오류가 발생하므로, 중복이 되는 파일인 .gitignore, .Rprofile과 renv.lock 을 로컬 프로젝트폴더에서 삭제해 줍니다.\n이후 아래의 git 명령으로 원격저장소의 모든 파일을 다운로드 받아 주면 됩니다.\n\n\n\nTerminal\n\ngit pull origin master\n\n\n(중복이 있더라도 원격저장소으 파일로 강제로 덮어쓰는 명령이 있지만 제가 시도할 때는 오류 메세지가 같이 나오기 때문에 일단 여기에는 소개하지 않습니다.)\n\n필자주: 이전에 다운로드 받은 적이 있고, 원격저장소의 파일이 갱신되어 있어 로컬저장소를 갱신하고자할 때로 위의 pull 명령을 사용하시면 됩니다. 이전 다운로드 이후에 로컬에서 수정한 적이 없으면 잘 다운로드 됩니다. 이후에는 renv::store() 명령으로 원격저장소의 renv.lock파일에 기록된 패키지들이 로컬과 차이가 있는지 점검하는 것이 좋습니다. 만약 로컬에서 파일을 수정하였기 때문에 로컬과 원격의 충돌이 발생한다면 아래의 명령을 시도해 봅니다. 이 경우 로컬의 파일들이 지워지고 자동으로 백업이 되지는 않기 때문에 필요시에는 백업을 미리 해 두셔야 합니다. 이렇게 해도 문제가 있다면 프로젝트 폴더를 삭제하고 처음부터 과정을 다시 진행하시길 바랍니다.\n\n\n\n\nTerminal\n\ngit fetch origin\ngit reset --hard origin/master",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#r-package-설치하기",
    "href": "posts/manuscript-example/manuscript_example.html#r-package-설치하기",
    "title": "Manuscripting with Quarto Example",
    "section": "",
    "text": "원격저장소에는 예제 R프로젝트 파일은 있지만 이를 위해 필요한 R 패키지까지 같이 있지는 않습니다. 하지만 renv.lock 파일에는 설치된 패키지 버전과 목록이 기록되어 있으므로 이를 이용해서 설치하시면 됩니다.\nConsole pane의 Consle 탭으로 이동해서 renv::restore() 명령을 실행하시면 됩니다.\n\n\n\nConsole\n\nrenv::restore ()\n\n\n만약 이전에 renv::restore()를 수행하여 패키지가 이미 update된 상태에서 다시 위 명령을 수행한다면 “- The library is already synchronized with the lockfile.”이라는 메세지가 출력할 것이며 이미 update된 것 상태이므로 다음 단계를 진행하시면 됩니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#raw_data-붙여넣기",
    "href": "posts/manuscript-example/manuscript_example.html#raw_data-붙여넣기",
    "title": "Manuscripting with Quarto Example",
    "section": "",
    "text": "예제 R로 논문쓰기 프로젝트를 실행할 때 raw data로써 “deidentified_han20230213.xlsx”이 필요합니다. 이 파일은 이미 개인정보보호조치(개인정보익명화, 날짜정보를 날짜간의 차이정보로 변환)가 되어 있지만 연구회의 방침상 업로드는 되어 있지는 않습니다. 내부연구자 분들이 실습을 위해 위 파일이 필요한 경우에는 연구회에 연락 바랍니다. 이 파일을 프로젝트 폴더 하부의 raw_data 폴더를 만들고 위 xlsx 파일을 복사해 주시면 됩니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#완료",
    "href": "posts/manuscript-example/manuscript_example.html#완료",
    "title": "Manuscripting with Quarto Example",
    "section": "",
    "text": "예제 R로 논문쓰기 프로젝트가 로컬에서 준비가 완료되었습니다. 원하는 기능들이 R청크에서 어떻게 구현되었는지 확인하시고 코드줄, 블록 또는 청크단위로 실행하시면서 구현결과를 확인하시면 됩니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#yaml-문법",
    "href": "posts/manuscript-example/manuscript_example.html#yaml-문법",
    "title": "Manuscripting with Quarto Example",
    "section": "YAML 문법",
    "text": "YAML 문법\nYAML 헤더는 quarto 문법 중 Front Matter (https://quarto.org/docs/authoring/front-matter.html)를 참고하면 YAML 문법에 맞게 작성할 수 있습니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#r-청크-옵션",
    "href": "posts/manuscript-example/manuscript_example.html#r-청크-옵션",
    "title": "Manuscripting with Quarto Example",
    "section": "R 청크 옵션",
    "text": "R 청크 옵션\nR코드청크(=code cell) 는 그 자체는 일반적인 R 스크립트와 같이 작성하면 되지만 청크의 실행과 출력결과를 조절하기 위한 옵션들(https://quarto.org/docs/reference/cells/cells-knitr.html)이 있습니다. R코드청크 내부에 #| 이후에 옵션을 설정하게 되며 아래에 예제 R프로젝트에 사용된 예시들이 있습니다.\n```{{r}}\n#| label: Load-raw-data #제목을 설정, \"_\"는 오류가 가능하다 하니 \"-\"로 단어 연결 추천\n#| label: fig-myplot #그래프의 라벨을 설정 \"fig-\" 시작해야 만 함\n#| label: tbl-mytable #테이블의 라벨을 설정 \"tbl-\" 시작해야 만 함\n#| output: false # 텍스트형태의 결과물이 문서에 포함되지 않음\n#| fig.show: 'hide'# 그래프형태의 결과물이 문서에 포함되지 않음\n#| fig.height: 6 \n#| fig.width: 6\n#| eval: false # 코드가 실행되지 않음\n```",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#quarto-문법",
    "href": "posts/manuscript-example/manuscript_example.html#quarto-문법",
    "title": "Manuscripting with Quarto Example",
    "section": "Quarto 문법",
    "text": "Quarto 문법\nQuarto markdown 문법은 R Markdown과 비슷하지만 더 많은 기능을 제공합니다. Quarto markdown 문법은 https://quarto.org/docs/authoring/markdown-basics.html를 참고하면 됩니다. Quarto는 Visual Editor 모드를 지원하여, WYSIWYG(What You See Is What You Get) 방식으로 문서를 작성할 수 있습니다. Visual Mode를 사용하면, 코드 블록 삽입, 이미지 추가, 테이블 생성 등을 GUI를 통해 쉽게 할 수 있습니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#manuscript-프로젝트-폴더",
    "href": "posts/manuscript-example/manuscript_example.html#manuscript-프로젝트-폴더",
    "title": "Manuscripting with Quarto Example",
    "section": "manuscript 프로젝트 폴더",
    "text": "manuscript 프로젝트 폴더\nQuarto로 새로운 프로젝트를 만들 때 R로 논문쓰기에 적합한 manuscripit 프로젝트가 있으며 예제 R 프로젝트도 manuscript 프로젝트를 이용해 만들어졌습니다. 일반적인 폴더와 파일구성은 다음과 같은 구조를 가질 수 있습니다.\nProject/\n         ├─  _freeze/\n         ├─  _manuscript/\n         ├─  _quarto.yml\n         ├─  .gitignore\n         ├─  .Rhistory\n         ├─  .Rprofile\n         ├─  index.qmd\n         ├─  R-4.4.1-RPythonStudy_HANJM.Rproj\n         ├─  references.bib\n         ├─  renv/\n         └─  renv.lock\n\n\n_freeze 폴더\nQuarto는 청크 단위의 실행 결과를 캐싱할 수 있습니다. YAML 헤더의 execute.freeze 옵션을 auto 또는 true로 설정하면 캐싱을 위해 _freeze 폴더가 생성됩니다.\n\n\n_manuscript 폴더\nQuarto manuscript 프로젝트에서는 기본적으로 렌더링 결과를 _manuscript 폴더에 만듭니다. 출력 결과를 다른 폴더로 변경하고자 할 때에는 YAML 헤더에서 output-dir: 원하는 폴더명으로 수정하시면 됩니다.\n\n\n_quarto.yml 파일\n_quarto.yml 파일은 Quarto 프로젝트의 설정을 관리하는 구성 파일입니다. 이 파일을 통해 프로젝트의 전반적인 설정, 문서 렌더링 옵션, 그리고 특정 문서의 형식과 출력을 제어할 수 있습니다.\n아래는 예제 R 프로젝트에 사용된 _quarto.yml 의 부분입니다.\n\n\n\nexample of _quarto.yml\n\nproject:\n  type: manuscript\n\nmanuscript:\n  article: index.qmd\n\nformat:\n  html:\n    comments: \n      hypothesis: true \n#  docx: default \n#  jats: default\n\nexecute:\n  freeze: true\n\neditor: visual\n\n\nQuarto 프로젝트 설정은 문서 또는 기사 작성에 주로 사용되며, ‘manuscript’으로 유형이 지정되어 있습니다. 이 설정은 프로젝트의 메인 문서 파일로 ’index.qmd’를 지정하고 있습니다. ’index.qmd’ 파일은 Quarto Markdown 형식으로, 프로젝트의 핵심 내용을 포함하고 있습니다.\n문서의 출력 형식으로 HTML이 지정되어 있어, 생성된 문서가 웹 페이지 형태로 렌더링되어 표시될 것입니다. 이 HTML 출력 설정에는 댓글 기능이 포함되어 있으며, Hypothesis라는 오픈 소스 주석 도구가 활성화되어 있습니다. Hypothesis 도구를 통해 사용자는 웹 페이지에 직접 주석을 추가할 수 있습니다, 이는 인터랙티브한 문서 환경을 조성합니다.\n‘freeze: false’ 설정은 문서 빌드 시 실행 코드의 결과를 캐싱하지 않겠다는 것을 의미합니다. 즉, 이 설정은 문서가 빌드될 때마다 관련 코드가 매번 실행되어 결과가 갱신되도록 합니다.\n추가적으로, ‘docx’와 ’jats’ 형식은 현재 비활성화된 상태로 주석 처리되어 있습니다. 이는 렌더링 시간을 단축하기 위한 조치로, 비활성화된 상태에서는 이 포맷들이 문서 출력 형식으로 사용되지 않습니다.\n마지막으로, ‘editor: visual’ 설정은 Quarto 문서 작성 시 사용되는 에디터 유형을 명시합니다. Visual editor는 사용자가 마크다운 문법을 몰라도 문서를 작성할 수 있도록 지원하는 사용자 친화적인 인터페이스를 제공합니다. 이는 문서 작성 과정을 보다 직관적이고 접근하기 쉽게 만듭니다.\n이러한 설정들은 효과적인 문서 작성과 관리를 가능하게 하며, 사용자에게 다양한 도구와 옵션을 제공하여 유연한 작업 환경을 조성합니다.\n\n\n.gitignore 파일\n.gitignore 파일은 Git 버전 관리 시스템에서 특정 파일이나 디렉토리를 추적하지 않도록 설정하는 데 사용됩니다. 이 파일은 프로젝트 디렉토리 내에 위치하며, Git이 파일을 무시하도록 지시하는 패턴 목록을 포함하고 있습니다.\n\n\n.Rhistory 파일\n.Rhistory 파일은 R 프로그래밍 환경에서 사용자가 실행한 모든 명령어의 이력을 저장하는 파일입니다. 이 파일은 R 콘솔에서 직접 입력하거나 스크립트를 실행하는 동안의 사용자의 명령어들을 자동으로 기록합니다. 이 기능은 사용자가 이전 세션에서 수행한 작업을 추적하고, 필요한 경우 명령어를 재사용하거나 참조할 수 있게 해 줍니다.\n\n\n.Rprofile 파일\n.Rprofile 파일은 R 세션의 시작 시 자동으로 실행되는 스크립트를 포함하는 설정 파일입니다. 이 파일은 사용자 또는 특정 프로젝트에 대한 환경 설정을 사용자 정의하고, R 세션을 시작할 때마다 일관된 작업 환경을 구성하는 데 사용됩니다. .Rprofile은 R의 시작 시 자동으로 로드되므로, R을 사용하는 데 필요한 라이브러리 로딩, 옵션 설정, 환경 설정 등을 자동화할 수 있습니다. 예제 R프로젝트에서는 renv 환경이 활성화되도록 설정되어 있습니다.\n\n\nIndex.qmd 파일\nindex.qmd 파일은 Quarto 문서 프로젝트에서 중요한 역할을 하는 메인 문서 파일입니다. .qmd 확장자는 Quarto Markdown을 의미하며, 이 파일 형식은 Markdown의 확장 버전으로, Quarto에서 제공하는 다양한 기능을 지원합니다.\n\n\n*.Rproj 파일\n.Rproj 파일은 RStudio의 프로젝트 파일로, 특정 R 프로젝트와 관련된 설정과 환경을 저장합니다. 이 파일은 RStudio 환경에서 프로젝트를 더 효율적으로 관리할 수 있게 해 주며, 프로젝트별로 별도의 작업 공간과 설정을 유지할 수 있도록 도와줍니다.\n\n\nreferences.bib 파일\nreferences.bib 파일은 레퍼런스 관리와 문헌 인용을 위한 BibTeX 파일입니다. 이 파일은 주로 LaTeX 문서나 다양한 텍스트 편집 소프트웨어와 통합하여 사용되며, 학술적 작업, 연구 보고서, 논문 등에서 출처를 인용하고 관리하는 데 필요한 정보를 포함합니다.\n\n\nrenv 폴더\nrenv 폴더는 R 프로젝트에서 프로젝트별 종속성 관리를 위해 사용되는 renv 패키지에 의해 생성되는 디렉토리입니다. 이 폴더는 프로젝트의 라이브러리를 격리시켜 다른 프로젝트나 시스템의 R 패키지 설치와 독립적으로 유지할 수 있도록 도와줍니다. renv는 R의 프로젝트에 대한 복원 가능하고 재현 가능한 환경을 제공하는 데 중점을 두고 있습니다.\n\n\nrenv.lock 파일\nrenv.lock 파일은 R 프로젝트에서 renv 패키지를 사용할 때 생성되는 중요한 파일로, 프로젝트의 종속성(의존하는 R 패키지들)을 정확하고 재현 가능하게 기록합니다. 이 파일은 프로젝트에 필요한 모든 R 패키지의 버전과 소스 정보를 포함하여 프로젝트 환경을 재현할 수 있도록 돕습니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#quarto-렌더링",
    "href": "posts/manuscript-example/manuscript_example.html#quarto-렌더링",
    "title": "Manuscripting with Quarto Example",
    "section": "Quarto 렌더링",
    "text": "Quarto 렌더링\nQuarto 문서 렌더링은 .qmd 파일(Quarto Markdown 파일)을 다양한 출력 형식으로 변환하는 복잡한 과정을 포함합니다. 이 과정은 데이터 분석, 코드 실행, 결과의 시각화를 포함한 종합적인 문서 생성을 목표로 하며, 주로 Quarto의 명령줄 인터페이스(Command Line Interface, CLI)를 통해 실행됩니다. 예를 들어, quarto render mydocument.qmd 명령은 mydocument.qmd 파일을 지정된 형식으로 변환합니다. 이 CLI 명령은 RStudio의 콘솔 창 아래에 있는 터미널에서 실행할 수 있습니다. 추가적으로, RStudio에서는 ‘Render’ 버튼을 사용하여 문서의 미리보기가 가능합니다. 이 버튼은 RStudio 환경 내에서 Quarto 문서를 쉽게 렌더링하고 결과를 실시간으로 확인할 수 있는 사용자 친화적인 방법을 제공합니다. 이를 통해 사용자는 문서의 최종 형태를 확인하고 필요에 따라 수정할 수 있습니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#그림-삽입",
    "href": "posts/manuscript-example/manuscript_example.html#그림-삽입",
    "title": "Manuscripting with Quarto Example",
    "section": "그림 삽입",
    "text": "그림 삽입\nQuarto manual 중 해당부분(https://quarto.org/docs/manuscripts/authoring/rstudio.html)을 참고하시길 바라며, 간단히 요약하면 아래와 같습니다.\n그림(=그래프)의 생성은 manusctipt 앞부분에 위치한 R chunk의 흐름에 따라 생성되도록 하며, 삽입을 원하는 본문의 위치에 별도의 그래프용 chuck를 만드는 것을 추천합니다.\n아래는 예시입니다.\nPropensity score 를 구하기 위해 활용된 로지스틱 회귀분석에 적용된 변수는\n다음과 같습니다. 환자 factor 로서 성별, 연령, ASA score 등\nPrimary tumor factor 로서 tumor location, lymph node ratio 등\nMetastatic factor 로서 간엽 침범 정도, 전이 병변의 개수 등을 설정하였습니다. \n이에 따라 Propensity score 에 따른 allocation probability 의 AUC 는\n0.811, p-value 는 0.4906 로 나타났습니다 \n(@fig-AUC-allocation-probability-PSM).\n\n```{r} \n#| label: fig-AUC-allocation-probability-PSM \n#| fig.cap: Propensity score 에 따른 allocation probability 의 AUC \n#| fig.height: 6 \n#| fig.width: 6 \n\nROC(form=Adju_TA~pr.score, data=HAN_IPTW, plot = \"ROC\")}\n```\n렌더링하면 manuscrip에는 다음과 같이 보이게 됩니다. (화면을 캡쳐한 것이며 실제로는 HTML 문서입니다.)\n\n이때 chuck label은 fig-로 시작해야 cross-reference에서 인식됩니다.\n또한 chuck option을 통해서 caption, 크기 등을 설정할 수 있습니다.\n다음의 예시는 그래프 두 개를 나란히 하는 경우입니다.\n@fig-survival 은 연구에 포함된 환자 에 대한 Kaplan Meier 생존곡선입니다. 왼쪽은 metastases 에 대한 progression survival 을 나타내고 오른쪽은 overall survival 을 나타냅니다. f/u 기간의 중앙값은 45.5개월 이었으며, 관찰기간 중의 PFS event 는 85건, OS event 는 67건 이었습니다.\n\n```{r}\n#| label: fig-survival\n#| fig.cap: Kaplan Meier survival curves for patients with targeted therapy after surgical resection of colorectal liver metastasis\nlibrary(gridExtra)\n\ngrid.arrange(PFS$plot, OS_TA$plot, ncol=2)\n```\n결과는 아래와 같습니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#표-삽입",
    "href": "posts/manuscript-example/manuscript_example.html#표-삽입",
    "title": "Manuscripting with Quarto Example",
    "section": "표 삽입",
    "text": "표 삽입\n같은 방식으로 표를 삽입할 수 있습니다. 예시는 생략합니다. 다만 chuck label은 tbl-로 시작해야 cross-reference에서 인식됩니다. 그리고 table처럼 보이지만 HTML 테이블형식이 아닌 경우는 인식이 되지 않으므로 주의해야 합니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/manuscript-example/manuscript_example.html#참고문헌작성",
    "href": "posts/manuscript-example/manuscript_example.html#참고문헌작성",
    "title": "Manuscripting with Quarto Example",
    "section": "참고문헌작성",
    "text": "참고문헌작성\nQuarto에서 참고문헌을 작성하고 관리하는 방법은 매우 유연하며, 주로 .bib 파일과 함께 사용됩니다. Quarto는 LaTeX의 biblatex 스타일과 호환되며, 다양한 인용 스타일(Citation Style Language, CSL) 파일을 지원하여 특정 저널 형식에 맞춘 참고문헌을 생성할 수 있습니다.\n\n참고문헌 파일 준비 (.bib 파일)\n참고문헌을 관리하기 위해 먼저 .bib 파일을 준비합니다. 이 파일은 BibTeX 형식으로 작성되며, 모든 참고문헌 항목이 이 파일에 저장됩니다.\n\n\nQuarto 문서에 참고문헌 파일 포함\nQuarto 문서의 YAML 헤더에 참고문헌 파일을 지정합니다. 이를 통해 Quarto는 문서에서 인용된 항목을 자동으로 참조하고, 문서 끝에 참고문헌 목록을 생성합니다.\n\n\n본문에서 인용하기\n본문에서 참고문헌 항목을 인용할 때는 [@key] 형식을 사용합니다. 여기서 key는 .bib 파일에서 정의된 항목의 고유 키입니다.\n\n\n참고문헌 목록 생성\n문서의 끝에서 # References 섹션을 추가하면 Quarto는 인용된 항목을 자동으로 참고문헌 목록에 포함시킵니다.\n\n\nCitation Style (CSL) 사용\n특정 저널의 인용 스타일을 사용하려면 CSL 파일을 사용할 수 있습니다. 예를 들어, Nature 스타일을 사용하려면 nature.csl 파일을 다운로드하고, YAML 헤더에 추가합니다.\n예시는 annals of surgery의 citation style langue 을 https://www.zotero.org/styles로부터 다운로드 받아서 프로젝트 폴더에 위치시킨 후 설정한 것입니다.\n---\ntitle: Role of Targeted Therapy after Surgical Resection of Colorectal Liver Metastases\nauthor:\n  - name: Ui Sep Shin\n    orcid: 0000-0002-1714-7469\n    corresponding: true\n    roles: \n      - Formal Analysis\n      - Supervision\n    affiliations:\n      - name: KIRAMS\n        department: Department of General Surgery\n    email: uisupshin@kirams.re.kr\nbibliography: references.bib\ncsl: annals-of-surgery.csl\nnumber-sections: true\n---\n렌더링 후 결과는 아래와 같습니다.",
    "crumbs": [
      "Quarto",
      "Manuscripting with Quarto Example"
    ]
  },
  {
    "objectID": "posts/group-comparison/group_comparison.html",
    "href": "posts/group-comparison/group_comparison.html",
    "title": "Group comparison in continuous variables",
    "section": "",
    "text": "추론통계\n의학 연구에서 전체 환자를 대상으로 데이터를 수집하는 것은 현실적으로 불가능한 경우가 많습니다. 추론 통계는 표본 데이터를 통해 모집단의 특성을 추정합니다.\n\n표본 통계량 (Sample Statistics): 표본에서 계산된 평균, 표준편차, 비율 등의 통계적 수치. 이 값들은 모집단의 특성을 추정하는 데 사용됩니다.\n신뢰구간 (Confidence Interval): 표본 통계량을 바탕으로, 모집단의 모수가 특정 범위 내에 있을 확률을 나타냅니다. 예를 들어, 95% 신뢰구간은 해당 구간 내에 모집단의 평균이 존재할 확률이 95%라는 것을 의미합니다.\n\n\n\n\n\n\n\nNote\n\n\n\n모수(母數, parametric)적 방법은 모집단이 특정한 분포를 따르고, 그 분포를 특정하는 평균(平均, mean), 분산(分散, variance) 등을 추정하는 기법임에 반해, 비모수(非母數, non-parametric) 방법은 모집단의 특정 분포를 가정하지 않고 순위(順位, ranks) 등을 사용하여 추정하는 것입니다.\n\n\n\n\n\n정규분포\n정규분포(Normal Distribution)는 다양한 데이터가 평균을 중심으로 대칭적으로 분포하는 확률분포이며 가우스분포라고도 합니다 (Equation 1 , Figure 1) .\n\n\n\n\\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)  \\tag{1}\\]\n\n\\(f(x)\\) : 확률 밀도 함수\n\\(\\mu\\) : 평균(mean)\n\\(\\sigma\\) : 표준편차(standard deviation)\n\\(\\sigma^2\\) : 분산(variance)\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of Normal Distributions with Different Variances\n\n\n\n\n\n\n\n정규분포는 Figure 1 에서처럼 평균(\\(\\mu\\))과 표준편차(\\(\\sigma\\))에 따라 그 형태가 달라집니다. 이로 인해 서로 다른 정규분포를 비교하거나, 특정 데이터가 정규분포 내에서 어떤 위치에 있는지를 평가하는 것이 어렵습니다.\n\n\nZ-분포\nZ-분포는 정규분포에서 데이터를 표준화(standardization)하여 얻어진 분포로, 평균이 0이고 표준편차가 1인 표준정규분포입니다 (Equation 2, Figure 2).\n\n\n  \\[\nf(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)\n\\tag{2}\\]\n\n\\(z\\): 표준화된 확률변수(Z-값)\n\\(f(z)\\): 해당 \\(z\\) 값에서의 확률 밀도\n\\(\\frac{1}{\\sqrt{2\\pi}}\\): 정규화 상수, 전체 확률 밀도가 1이 되도록 조정\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Standard Normal Distributions with Z-value\n\n\n\n\n\n\n\nZ-분포에서 Z-값은 통계검정량(test statistic)으로 사용되며, 특정 데이터 포인트가 평균으로부터 얼마나 떨어져 있는지를 표준편차 단위로 나타냅니다. Z-값은 다음과 같이 계산됩니다: Equation 3\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\tag{3}\\]\n\n\\(Z\\): 표준화된 값 (Z-값)\n\\(X\\): 관측된 데이터 값\n\\(\\mu\\): 모집단 평균\n\\(\\sigma\\): 모집단 표준편차\n\nZ-분포의 활용은 다음과 같습니다.\n\n통계적 가설 검정: Z-값을 사용하여 특정 데이터가 평균에서 얼마나 떨어져 있는지를 판단하고, 이를 바탕으로 가설 검정을 수행합니다.\n신뢰구간 계산: Z-분포는 신뢰구간을 계산할 때 사용됩니다. 특정 신뢰수준에서의 Z-값을 이용해 구간을 설정합니다.\n확률 계산: Z-값을 통해 특정 데이터가 주어진 정규분포에서 얼마나 자주 발생할지를 확률적으로 계산할 수 있습니다.\n\n\n\n중심극한정리\n어떤 모집단의 분포가 무엇이든 간에 표본 크기가 충분히 크면(일반적으로 \\(n≥30\\)) 표본 평균의 분포는 정규분포에 근사하게 된다는 것이며, 표본 평균의 분포는 모집단의 평균 \\(\\mu\\)를 중심으로 하고, 표준오차(SE, Standard Error)인 \\(\\frac{\\sigma}{\\sqrt{n}}\\)를 표준편차로 가지는 정규분포에 근사하게 됩니다 Figure 3.\n\n\n\n\n\n\nFigure 3: Illustration of Central Theorem (https://velog.io/ted_log/중심극한정리와와-신뢰구간)\n\n\n\n이를 기반으로 모평균의 95% 신뢰구간을 추정하면 다음과 같습니다: Equation 4\n\\[\n\\overline{X} \\pm Z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\n\\tag{4}\\]\n\n\\(\\overline{X}\\) : 표본 평균\n\\(Z_{\\alpha/2}\\) : 95% 신뢰수준에서의 Z-값(약 1.96)\n\\(\\frac{\\sigma}{\\sqrt{n}}\\) : 표본 평균의 표준오차\n\n위 신뢰구간 추정식은 모집단의 표준편차 \\(\\sigma\\)를 알고 있다는 가정하에 유도되었습니다. 그러나 실제로는 모집단의 표준편차 \\(\\sigma\\)를 알 수 없는 경우가 대부분입니다. 이 경우 \\(\\sigma\\)를 사용할 수 없으므로 표본 표준편차 \\(s\\)를 사용해야 합니다. 따라서 신뢰구간 추정식은 다음과 같이 수정됩니다: Equation 5\n\\[\n\\overline{X} \\pm Z_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\n\\tag{5}\\]\n여기서 \\(s\\)는 표본 표준편차입니다.\n위의 전제들은 표본의 크기가 크다는 전제가 있기 때문에 표본의 크기가 작다면 모평균을 추정이 제한적이게 됩니다.\n\n\nt-분포\nt-분포는 1908년, 영국의 통계학자 윌리엄 고셋에 의해 처음으로 개발되었습니다. 당시 고셋은 맥주 양조 회사인 기네스(Guinness)에서 일하고 있었고, 소규모 실험에서 발생하는 불확실성을 다루기 위해 새로운 통계적 방법을 개발할 필요가 있었습니다.\n필명 ‘Student’: 고셋은 회사의 기밀 유지 정책 때문에 자신의 이름을 사용하지 못했고, 대신 ‘Student’라는 필명으로 논문을 발표했습니다. 그래서 t-분포는 종종 Student’s t-distribution이라고 불립니다.\n고셋은 정규분포와 카이제곱분포의 관계를 사용하여 t-분포를 도출했습니다. 카이제곱분포는 정규분포에서 도출된 분포로, 모집단의 분산을 모를 때 표본 분산을 통해 도출됩니다.\n표본 평균이 정규분포를 따르며, 표본 분산이 독립적인 카이제곱분포를 따른다는 사실에 기초해 t-분포를 수학적으로 유도했습니다 Equation 6.\n\n\n\n\n\n\nNote\n\n\n\n이는 t-검정에서는 모집단이 정규분포를 따른다는 가정을 전제하고 있음을 시사합니다.\n\n\n\\[\nf(t) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu \\pi} \\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu + 1}{2}}\n\\tag{6}\\]\n여기서:\n\n\\(t\\): t-분포의 확률변수\n\\(\\nu\\): 자유도(degrees of freedom)\n\\(\\Gamma\\): 감마 함수 (Gamma function),\n\\(\\Gamma(n)\\)은 \\((n-1)!\\)과 동일한 의미를 가짐\n\n이 수식에서 t-분포의 모양은 자유도 \\(\\nu\\)에 따라 달라지며, 자유도가 클수록 t-분포는 표준정규분포에 가까워집니다.\n\n\\(\\nu\\)가 작을수록, t-분포의 꼬리 부분이 더 두터워져서 극단적인 값이 나올 확률이 더 높습니다.\n\\(\\nu\\)가 무한대로 커지면, t-분포는 표준정규분포와 동일하게 됩니다.\n\nt-통계량은 다음과 같이 정의됩니다: Equation 7\n\\[\nt = \\frac{\\overline{X} - \\mu}{\\frac{s}{\\sqrt{n}}}\n\\tag{7}\\]\n\n\\(\\overline{X}\\) : 표본 평균,\n\\(\\mu\\) : 모집단 평균,\n\\(s\\) : 표본 표준편차,\n\\(n\\) : 표본 크기입니다.\n\nt-통계량의 의미는 다음과 같습니다.\n표본평균과 모평균의 비교:\nt-통계량은 표본평균과 가설에서 설정한 모집단 평균 \\(\\mu\\) 간의 차이가 표본 표준오차(SE) 대비 얼마나 큰지를 나타냅니다. 즉, t-값은 표본평균이 모집단 평균과 얼마나 떨어져 있는지를 표준오차의 단위로 표현한 값입니다.\n가설 검정에서의 사용:\nt-분포를 이용한 가설 검정에서는 계산된 t-값과 t-분포의 임계값을 비교하여 귀무가설을 기각할지 결정합니다. 예를 들어, t-값이 t-분포의 임계값을 벗어나면, 두 평균 간에 유의미한 차이가 있다고 판단하고 귀무가설을 기각합니다.\n자유도와 t-분포:\nt-통계량은 t-분포를 따르며, 이 t-분포의 모양은 자유도(degrees of freedom, df)에 따라 달라집니다. 자유도는 일반적으로 \\(n - 1\\)로 계산되며, 표본 크기가 커질수록 t-분포는 표준정규분포에 가까워집니다. 표본 크기가 작을 때는 t-분포의 꼬리가 더 두터워지며, 이는 극단적인 t-값이 나올 가능성을 더 잘 반영합니다.\nt-값의 해석:\nt-값이 크면 클수록 표본평균과 모집단 평균 간의 차이가 크다고 해석됩니다. t-값이 0에 가까울수록 표본평균이 모집단 평균과 유사하다는 의미입니다.\n\n\n\n\n\n\nFigure 4: 표준정규분포와 t분포의 차이\n\n\n\n\n\nt-분포를 이용한 모평균 신뢰구간\nt-분포를 이용하여 모평균의 신뢰구간을 구하는 식은 다음과 같습니다: Equation 8\n\\[\n\\overline{X} \\pm t_{\\alpha/2, \\, df} \\cdot \\frac{s}{\\sqrt{n}}\n\\tag{8}\\]\n여기서 각 요소는 다음을 의미합니다:\n\n\\(\\overline{X}\\) : 표본 평균\n\\(t_{\\alpha/2, \\, df}\\) : 신뢰수준 \\(\\alpha\\)에 해당하는 t-값, 자유도 \\(df\\) (일반적으로 \\(df = n - 1\\))에 따라 결정\n\\(s\\): 표본 표준편차\n\\(n\\): 표본 크기\n\\(\\frac{s}{\\sqrt{n}}\\): 표준오차(SE), 표본 평균이 모집단 평균을 얼마나 잘 추정하는지를 나타냄\n\n예를 들어, 표본 크기가 \\(n = 25\\), 표본 평균이 \\(\\overline{X} = 100\\), 표본 표준편차가 \\(s = 10\\)이며, 95% 신뢰구간을 계산하려고 한다면, \\(t_{\\alpha/2, \\, 24}\\) 값을 찾고 이 식을 사용하여 신뢰구간을 계산할 수 있습니다.\n\n\n두 평균의 비교\n모평균 추정에서 t-분포 통계량인 t를 산출하는 것은 모평균과 표본평균의 차이를 표준오차로 보정하여 구하지만, 두 평균의 비교(t-검정)에서는 두 평균의 차이를 보정할 때 두 표본의 분산이 모두 사용되므로 결합분산(pooled variance)을 이용하여 보정하게 됩니다 Equation 9.\n\n\n\n\n\n\nNote\n\n\n\n이때 결합분산은 두 모집단의 분산이 같다는 가정하에 계산됩니다.\n\n\n\\[\nt = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\sqrt{s_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\tag{9}\\]\n여기서: - \\(\\overline{X}_1\\), \\(\\overline{X}_2\\) 는 각각 두 표본의 평균입니다. - \\(n_1\\), \\(n_2\\) 는 각각 두 표본의 크기입니다. - \\(s_p^2\\)는 두 표본의 결합분산(pooled variance)으로, 다음과 같이 계산됩니다: Equation 10\n\\[\ns_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\tag{10}\\]\n여기서 \\(s_1^2\\)와 \\(s_2^2\\)는 각각 두 표본의 분산입니다.\n\n\nR에서의 t-검정\n기본적으로 t.test() 함수를 사용하여 수행할 수 있습니다. t-검정에는 단일 표본 t-검정, 독립 표본 t-검정, 대응 표본 t-검정이 있습니다. R에서 t.test() 함수의 주요 인자는 벡터입니다. t.test() 함수는 기본적으로 두 가지 벡터를 입력받아 두 그룹 간의 평균 차이를 비교합니다. 다음은 t.test() 함수의 주요 인자들과 그 설명입니다:\n\n1. x (필수 인자)\n\n첫 번째 그룹의 데이터를 나타내는 벡터입니다.\n예시: t.test(x = c(5, 6, 7, 8, 9))\n\n\n\n2. y (선택적 인자)\n\n두 번째 그룹의 데이터를 나타내는 벡터입니다.\n만약 y 인자를 제공하지 않으면, t.test()는 단일 표본 t-검정을 수행합니다.\n예시: t.test(x = c(5, 6, 7, 8, 9), y = c(10, 11, 12, 13, 14))\n\n\n\n3. alternative (선택적 인자)\n\n대립가설의 형태를 지정합니다. \"two.sided\", \"greater\", \"less\" 중 하나를 사용할 수 있습니다.\n기본값은 \"two.sided\"입니다.\n\n\n\n4. mu (선택적 인자)\n\n단일 표본 t-검정에서 가설 검정의 기준이 되는 모집단 평균을 지정합니다. 기본값은 mu = 0입니다.\n\n\n\n5. paired (선택적 인자)\n\n두 벡터가 짝지어진 데이터인지 여부를 지정합니다.\nTRUE로 설정하면 대응 표본 t-검정을 수행합니다. 기본값은 FALSE입니다.\n\n\n\n6. var.equal (선택적 인자)\n\n두 벡터의 분산이 같다고 가정할지를 지정합니다.\nTRUE로 설정하면 분산이 동일하다고 가정하며 Student의 t-검정을 수행합니다. 기본값은 FALSE이며, 이 경우 Welch의 t-검정이 수행됩니다.\n\n\n\n7. conf.level (선택적 인자)\n\n신뢰구간의 신뢰수준을 지정합니다. 기본값은 0.95(95% 신뢰수준)입니다.\n\n\n\n단일 표본 t-검정 (One-Sample t-test)\n단일 표본 t-검정은 한 표본의 평균이 특정 값과 다른지를 검정하는 데 사용됩니다.\n예시 코드와 실행결과 (여기서 mu는 비교하고자 하는 모집단 평균입니다.)\n\n# 예시 데이터\ndata &lt;- c(5.2, 4.9, 6.3, 5.8, 5.4, 5.7, 5.1)\n\n# 단일 표본 t-검정\nt.test(data, mu = 5.5)\n\n\n    One Sample t-test\n\ndata:  data\nt = -0.078567, df = 6, p-value = 0.9399\nalternative hypothesis: true mean is not equal to 5.5\n95 percent confidence interval:\n 5.040799 5.930630\nsample estimates:\nmean of x \n 5.485714 \n\n\n\n\n독립 표본 t-검정 (Independent Two-Sample t-test)\n독립 표본 t-검정은 두 독립된 그룹 간의 평균 차이를 검정하는 데 사용됩니다. 그룹 간의 분산이 같다고 가정하는 경우와 그렇지 않은 경우에 따라 달라집니다.\n예시 코드와 실행결과\n\n# 예시 데이터\ngroup1 &lt;- c(6.1, 5.9, 6.3, 6.5, 6.0)\ngroup2 &lt;- c(5.8, 5.7, 5.9, 6.0, 5.6)\n\n# 독립 표본 t-검정\nt.test(group1, group2)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 2.7941, df = 6.908, p-value = 0.02711\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.054515 0.665485\nsample estimates:\nmean of x mean of y \n     6.16      5.80 \n\n\n\n기본적으로, t.test() 함수는 두 그룹 간의 분산이 같다고 가정하지 않습니다. 만약 분산이 같다고 가정하려면 var.equal = TRUE 인자를 추가합니다.\n\n\n# 분산이 같다고 가정한 t-검정\nt.test(group1, group2, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  group1 and group2\nt = 2.7941, df = 8, p-value = 0.02341\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.06289215 0.65710785\nsample estimates:\nmean of x mean of y \n     6.16      5.80 \n\n\n\n\n대응 표본 t-검정 (Paired t-test)\n대응 표본 t-검정은 같은 대상에서 두 번의 측정을 수행한 결과(예: 치료 전후)를 비교하는 데 사용됩니다.\n예시 코드와 실행결과\n\n# 예시 데이터\nbefore &lt;- c(5.5, 5.7, 5.8, 5.9, 5.6)\nafter &lt;- c(6.0, 5.8, 6.2, 6.1, 6.0)\n\n# 대응 표본 t-검정\nt.test(before, after, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  before and after\nt = -4.3546, df = 4, p-value = 0.01211\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.5240262 -0.1159738\nsample estimates:\nmean difference \n          -0.32 \n\n\n\n여기서 paired = TRUE 인자는 두 표본이 짝을 이루고 있음을 지정합니다.\n\n\n\nt-검정 결과 해석\nt.test() 함수의 결과는 다음과 같은 요소들을 포함하는 리스트로 반환됩니다:\n\nt-value: t-통계량 값\ndf: 자유도 (degrees of freedom)\np-value: p-값, 귀무가설을 기각할지 여부를 결정하는 데 사용됩니다.\nconf.int: 지정된 신뢰수준에 대한 신뢰구간\nmean of x/y: 각 그룹의 평균 예시 결과:\n\n\n# 결과 예시\nt.test(group1, group2)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 2.7941, df = 6.908, p-value = 0.02711\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.054515 0.665485\nsample estimates:\nmean of x mean of y \n     6.16      5.80 \n\n\n\n이 결과를 해석할 때, p-값이 유의수준(예: 0.05)보다 작으면 귀무가설을 기각하고, 두 그룹 간의 평균에 유의미한 차이가 있다고 결론지을 수 있습니다.\n\n\n\n\nWelch t-검정\n\n이질적 분산: 두 그룹의 분산이 서로 다를 수 있다는 가정을 전제로 합니다. 이는 데이터가 다양한 조건에서 수집된 경우에 특히 유용합니다.\n자유도의 조정: Welch t-검정은 자유도를 조정하여 두 그룹의 분산이 다를 때 표본 크기와 분산의 차이를 반영합니다. 자유도는 두 그룹의 분산과 표본 크기에 따라 달라지며, 다음과 같은 식으로 계산됩니다:\n\n\\[\ndf = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{s_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{s_2^2}{n_2}\\right)^2}{n_2 - 1}}\n\\]\n여기서:\n\n\\(s_1^2\\), \\(s_2^2\\)는 각각 두 그룹의 표본 분산,\n\\(n_1\\), \\(n_2\\)는 각각 두 그룹의 표본 크기입니다.\n\n\n비대칭적 신뢰구간: Welch t-검정은 t-분포를 따르지 않으며, 신뢰구간이 비대칭적일 수 있습니다. 이는 데이터의 분포가 비대칭적일 때 더 정확한 신뢰구간을 제공합니다.\n\n\nWelch t-검정의 t-통계량은 다음과 같이 계산됩니다: Equation 11\n\\[\nt = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\tag{11}\\]\n여기서:\n\n\\(\\overline{X}_1\\)와 \\(\\overline{X}_2\\)는 각각 두 그룹의 표본 평균,\n\\(s_1^2\\)와 \\(s_2^2\\)는 각각 두 그룹의 표본 분산,\n\\(n_1\\)와 \\(n_2\\)는 각각 두 그룹의 표본 크기입니다.\n\nR에서의 Welch t-검정 수행 방법 R에서 Welch t-검정을 수행하기 위해 t.test() 함수를 사용합니다. 기본적으로 t.test() 함수는 두 집단의 분산이 같지 않다고 가정하여 Welch t-검정을 수행합니다.\n\n# 두 그룹의 데이터 생성\ngroup1 &lt;- c(6.1, 5.9, 6.3, 6.5, 6.0)\ngroup2 &lt;- c(5.8, 5.7, 5.9, 6.0, 5.6)\n\n# Welch t-검정 수행\nresult &lt;- t.test(group1, group2)\n\n# 결과 출력\nprint(result)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 2.7941, df = 6.908, p-value = 0.02711\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.054515 0.665485\nsample estimates:\nmean of x mean of y \n     6.16      5.80 \n\n\n\n\nMann-Whitney U-검정\n\nWilcoxon의 기초 작업\n1945년: 프랭크 윌콕슨(Frank Wilcoxon)이 비모수적 방법을 기반으로 두 그룹 간의 차이를 비교하는 Wilcoxon 순위합 검정을 처음으로 제안했습니다. Wilcoxon의 접근법은 두 집단 간의 차이를 순위로 변환하여 순위 합을 비교하는 것이었으며, 이는 정규분포를 가정하지 않고도 두 독립된 표본 간의 차이를 평가할 수 있는 중요한 방법으로 자리 잡았습니다.\n\n\nMann과 Whitney의 확장\n1947년: 헨리 B. 만(Henry B. Mann)과 도로시 로빈스 와이트니(Dorothy Robbins Whitney)는 Wilcoxon의 순위합 검정을 확장하고 일반화했습니다. Mann과 Whitney는 두 독립된 표본의 순위를 비교하는 새로운 방법을 제안했으며, 이 방법은 Mann-Whitney U 검정으로 알려지게 되었습니다.\n이 검정법은 두 그룹 간의 차이를 평가하기 위해 U 통계량을 도입했습니다. U 통계량은 두 그룹의 순위를 비교하여 계산되며, 두 그룹이 동일한 분포를 따를 경우 U 통계량의 기대값이 계산됩니다. 귀무가설이 참일 때 이 U 통계량의 분포는 알려진 분포를 따르며, 이를 통해 p-값을 계산할 수 있습니다.\n\n\n비모수적 방법의 중요성\nMann-Whitney U 검정의 개발은 비모수적 방법의 중요성을 강조했습니다. 정규분포를 가정하지 않고도 데이터를 분석할 수 있는 방법을 제공했으며, 이는 특히 작은 표본이나 비정규 분포의 데이터를 다루는 데 매우 유용했습니다.\nMann-Whitney U 검정은 데이터의 실제 값이 아닌 순위를 사용하여 통계적 검정을 수행하므로, 이상값(outliers)이나 비대칭 분포의 영향을 덜 받습니다. 이 검정은 두 그룹 간의 차이를 비교할 때, 중앙값의 차이에 대한 강력한 비모수적 대안으로 널리 사용됩니다.\n\n\nMann-Whitney U 검정의 계산 과정:\n첫 번째 그룹에 대한 U 통계량:\n\\[\nU_1 = R_1 - \\frac{n_1 (n_1 + 1)}{2}\n\\]\n두 번째 그룹에 대한 U 통계량:\n\\[\nU_2 = R_2 - \\frac{n_2 (n_2 + 1)}{2}\n\\]\n여기서:\n\n\\(R_1\\)과 \\(R_2\\)는 각각 첫 번째와 두 번째 그룹의 순위 합,\n\\(n_1\\)과 \\(n_2\\)는 각각 첫 번째와 두 번째 그룹의 표본 크기입니다.\n\n최종적으로 두 값 중 작은 U 통계량이 선택되며, 이 값에 따라 두 그룹 간의 차이가 유의미한지를 판단합니다.\n\n# 필요한 패키지 로드\nlibrary(ggplot2)\n\n# 정규분포를 따르지 않는 두 그룹 데이터 생성\nset.seed(123)\ngroup1 &lt;- rexp(30, rate = 0.2)  # 지수분포\ngroup2 &lt;- rexp(30, rate = 0.3) + 2  # 다른 지수분포 + 상수로 이동\n\n# t-검정 수행\nt_test_result &lt;- t.test(group1, group2)\nt_test_p_value &lt;- t_test_result$p.value\n\n# Mann-Whitney U 검정 수행\nu_test_result &lt;- wilcox.test(group1, group2)\nu_test_p_value &lt;- u_test_result$p.value\n\n# 결과 출력\ncat(\"t-검정 p-값:\", t_test_p_value, \"\\n\")\n\nt-검정 p-값: 0.1227817 \n\ncat(\"Mann-Whitney U 검정 p-값:\", u_test_p_value, \"\\n\")\n\nMann-Whitney U 검정 p-값: 0.03576713 \n\n# 두 그룹의 분포를 히스토그램으로 시각화\ndf &lt;- data.frame(\n  value = c(group1, group2),\n  group = factor(c(rep(\"Group 1\", 30), rep(\"Group 2\", 30)))\n)\n\np &lt;- ggplot(df, aes(x = value, fill = group)) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 15) +\n  theme_minimal() +\n  labs(title = \"Comparison of Two Non-Normal Distributions\",\n       x = \"Value\", y = \"Frequency\") +\n  theme(legend.position = \"top\")\n\n# 그래프 출력\nprint(p)",
    "crumbs": [
      "Statistics",
      "Group comparison in continuous variables"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html",
    "href": "posts/Git-setup/Git_setup.html",
    "title": "Git and Github setup",
    "section": "",
    "text": "Git is an open-source distributed version control system (DVCS) developed by Linus Torvalds in 2005. It records snapshots of source code, allowing you to manage change history, revert to a specific point in time, and develop code in a distributed environment with branching, where multiple developers can work simultaneously. Git has become the standard version control tool used by the majority of development teams worldwide, thanks to its distributed structure and performance optimization.",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#whats-git",
    "href": "posts/Git-setup/Git_setup.html#whats-git",
    "title": "Git and Github setup",
    "section": "",
    "text": "Git is an open-source distributed version control system (DVCS) developed by Linus Torvalds in 2005. It records snapshots of source code, allowing you to manage change history, revert to a specific point in time, and develop code in a distributed environment with branching, where multiple developers can work simultaneously. Git has become the standard version control tool used by the majority of development teams worldwide, thanks to its distributed structure and performance optimization.",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#usefulness-of-git",
    "href": "posts/Git-setup/Git_setup.html#usefulness-of-git",
    "title": "Git and Github setup",
    "section": "Usefulness of Git",
    "text": "Usefulness of Git\nAlthough the Git usage may be somewhat difficult for coding beginners, it is useful for them to revert to a point in time when there was no problem using the revision history management feature when a problem occurs during code modification. This feature itself serves as a backup function.",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#installation-guide",
    "href": "posts/Git-setup/Git_setup.html#installation-guide",
    "title": "Git and Github setup",
    "section": "Installation Guide",
    "text": "Installation Guide\nIt is ideal to refer to the official Git installation document (https://git-scm.com/book/en/v2/Getting-Started-Installing-Git), but you can also refer to the summary below for installation.",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#installation-file",
    "href": "posts/Git-setup/Git_setup.html#installation-file",
    "title": "Git and Github setup",
    "section": "Installation File",
    "text": "Installation File\nDownload the latest installation file suitable for your operating system from the official Git download site (https://git-scm.com/downloads) and install it. As of September 23, 2024, the latest installation file for Windows 64-bit is Git-2.46.2-64-bit.exe.",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#installation-options",
    "href": "posts/Git-setup/Git_setup.html#installation-options",
    "title": "Git and Github setup",
    "section": "Installation Options",
    "text": "Installation Options\n\nInstallation Path\nThe default is C:\\Program Files\\Git, and the default is recommended.\n\n\nSelecting Components\nThe default is as shown in Figure 1, and the default is recommended.\n\n\n\n\n\n\nFigure 1: Options to select Components\n\n\n\n\n\nShortcut Icon\nThe default is to create a shortcut icon in the Git Start Menu folder as shown in Figure 2. However, if you use Git in RStudio, you can use Git in RStudio, so it is recommended not to create a shortcut icon.\n\n\n\n\n\n\nFigure 2: Options to select Start Menu Folder for programs’ shortcut\n\n\n\n\n\nText Editor\nThe default is Vim, but the research group recommends VS Code as a Python IDE, which is relatively easier to use than Vim, as shown in Figure 3.\n\n\n\n\n\n\nFigure 3: Options to select Text Editor\n\n\n\n\n\nDefault Branch Name\nThe default is master, but since October 1, 2020, GitHub has changed the default branch name from master to main. Therefore, it is recommended to set the default branch name to main when installing Git, as shown in Figure 4.\n\n\n\n\n\n\nFigure 4: Options to select Default Branch Name\n\n\n\n\n\nEnvironment Variable Setting\nThe default is to add the Git executable file path to the environment variable, as shown in Figure 5, and the default is recommended.\n\n\n\n\n\n\nFigure 5: Options to select Path Environment\n\n\n\n\n\nSSH Selection\nGit often uses SSH for secure communication with remote repositories. Platforms such as GitHub, GitLab, and Bitbucket provide SSH key-based authentication, allowing you to securely connect to remote repositories using SSH keys instead of passwords. OpenSSH is open-source software that can be optionally installed during Git installation and is mainly used when setting up SSH-based authentication. OpenSSH is selected by default during the installation process, as shown in Figure 6, and it is recommended to use the default value unless there is a specific reason.\n\n\n\n\n\n\nFigure 6: Options to select OpenSSH\n\n\n\n\n\nSSL/TLS Library Selection\nSSL/TLS is an encryption library used by Git when communicating with remote repositories over HTTPS. OpenSSL is recommended when broader encryption capabilities, support for the latest standards, and compatibility with Linux are important. The Windows-specific SSL/TLS library (Schannel) is recommended when you want integration with Windows systems and easy configuration. The research group recommends the default OpenSSL, as shown in Figure 7.\n\n\n\n\n\n\nFigure 7: Options to select SSL/TLS library\n\n\n\n\n\nLine Ending Conversion Setting\nIn Windows, the default line ending setting is CRLF (Carriage Return Line Feed). In contrast, Unix-based systems (Linux, macOS) use LF (Line Feed). When developing across operating systems, Git supports line ending conversion to resolve these line ending differences. You can set whether to automatically convert line endings during checkout and commit, with the default being Checkout Windows-style, commit Unix-style line endings, which is suitable for development on Windows. If you are developing on Linux or macOS, it is recommended to select Checkout as-is, commit Unix-style line endings, as shown in Figure 8.\n\n\n\n\n\n\nFigure 8: Options to select Line Ending Conversion\n\n\n\n\n\nTerminal Emulator\nThis option determines the terminal environment to use in Git Bash. Each option defines the terminal to be used when Git Bash is run, and there are differences in user experience and terminal functionality. MinTTY provides a more advanced terminal experience and a working environment closer to Unix style. It is particularly suitable for users familiar with Linux/Unix or those who mainly use Git Bash. The Windows default console window is a terminal interface provided by Windows by default, suitable for users who prefer a familiar environment like CMD. It is especially suitable for Windows users or those who mainly use CMD, as shown in Figure 9.\n\n\n\n\n\n\nFigure 9: Options to select Terminal Emulator\n\n\n\n\n\ngit pull Setting\nThis setting determines how to merge changes between the local branch and the remote branch when the git pull command is executed, as shown in Figure 10.\n1. Fast-forward or merge (default setting) This option is set as the default and Git selects fast-forward merging or merge commit depending on the situation.\nFast-forward: If there are no additional changes in the local branch and you can simply append the commits from the remote branch, Git performs a fast-forward merge. In this case, no merge commit is created, and a new commit is added to the existing commit line.\nAdvantages: You can maintain a clean history, keeping a concise log without unnecessary merge commits.\nDisadvantages: There may be limitations in managing complex merge records during collaboration.\nMerge: If there are changes in the local branch or you cannot simply append the changes from the remote branch, a merge commit is created. This commit merges the histories of the two branches, clearly indicating the merge point.\nAdvantages: You can clearly distinguish the point where the merge occurred, making it easy to track who did what work.\nDisadvantages: The log can become complex as the number of merge commits increases.\n2. Rebase The Rebase option applies changes from the remote branch to the local branch without merge commits. Rebase rewrites the history by rearranging commits to make it look like you worked on the remote branch.\nRebase operation: Temporarily remove the commits made in the local branch, apply the changes from the remote branch first, and then reapply the local commits. This way, the history is straightened out, and the commit log becomes very clean.\nAdvantages: The history is concise and straightened out, making it easy to read the log. Since no merge commits are created, you can maintain a clean history even during collaboration.\nDisadvantages: Since Rebase rewrites the history, if you rebase a commit that has already been shared, conflicts may occur during collaboration or affect other developers’ commits. Rebase should be used carefully during collaboration, especially in branches where multiple developers are working simultaneously.\n3. Only ever fast-forward The Only ever fast-forward option sets a strong constraint to allow only fast-forward merges. If you select this option, Git performs a merge only when fast-forward is possible without creating a merge commit.\nOperation: If the local branch is ahead of the remote branch or a conflict occurs, it does not allow merging and raises an error. In other words, a fast-forward merge is possible only when the branch is behind the remote branch.\nAdvantages: You can keep the history absolutely clean. Since no merge commits are created, the history remains very concise. It is easy to manage the history, and it is suitable for those who prefer a clean, linear commit log.\nDisadvantages: Even if changes from other developers and local changes need to be merged during collaboration, only fast-forward is allowed, so Git may refuse to merge. In such cases, the merge may fail, and you may need to perform the merge manually. It can be restrictive in complex collaboration situations. Since merge commits may be necessary during collaboration, this option is somewhat limited.\n\n\n\n\n\n\nFigure 10: Options to select git pull\n\n\n\n\n\nGit Credential Manager\nGit Credential Manager is a tool that securely stores and automatically manages authentication information for remote repositories in Git. Through this, users can securely use Git without having to repeatedly enter credentials, and it is supported on various operating systems such as Windows, macOS, and Linux. It also supports modern authentication methods such as OAuth and token-based authentication, providing both security and convenience. Git Credential Manager can be optionally installed during Git installation, and it is recommended to use the default value.\n\n\n\n\n\n\nFigure 11: Options to select Git Credential Manager\n\n\n\n\n\nExtra Options\nFile System Caching is a feature that allows Git to more quickly query metadata (file status, permissions, timestamps, etc.) for the file system using a cache. By default, Git checks the status of files (e.g., modification time, size) to detect file changes in the file system. This process can cause performance degradation in large projects containing many files. Symbolic Links are a special type of file that points to a reference to a file or directory. Symbolic links store the path of the original file or directory and work by referencing it. When you enable the Symbolic Links option in Git, Git can store and manage the symbolic link itself instead of storing it as a file. Symbolic links are widely used and supported in Linux and macOS, so it is common to enable this option. Symbolic links are limited by default in Windows and require administrator privileges or specific settings to enable. Therefore, enabling this option in Windows may not work properly or may cause unexpected behavior. The default is to enable File System Caching, as shown in Figure 12.\n\n\n\n\n\n\nFigure 12: Options to select Extra Options",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#installation-verification",
    "href": "posts/Git-setup/Git_setup.html#installation-verification",
    "title": "Git and Github setup",
    "section": "Installation Verification",
    "text": "Installation Verification\nRun the following command in the command prompt to check the version. (The version should be displayed regardless of the folder location where the command prompt is opened because the path to the executable file is automatically registered in the environment variable when Git is installed.)\n\n\n\nCommand Prompt\n\ngit --version",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#git-concepts-and-terminology",
    "href": "posts/Git-setup/Git_setup.html#git-concepts-and-terminology",
    "title": "Git and Github setup",
    "section": "Git Concepts and Terminology",
    "text": "Git Concepts and Terminology\nIt is recommended to learn from the Korean manual on the Git official website (https://git-scm.com/book/en/v2/Getting-Started-Git-Basics). After learning, you should be able to explain concepts such as add/commit/push/pull/reset and distinguish the status of files as untracked/staged/committed, as shown in Figure 13.\n\n\n\n\n\n\nFigure 13: Concept of Git and terminology",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#git-configuration",
    "href": "posts/Git-setup/Git_setup.html#git-configuration",
    "title": "Git and Github setup",
    "section": "Git Configuration",
    "text": "Git Configuration\n\nSetting Username and Email Address\nGit records the username and email address of the person who made the commit each time a commit is made. To do this, set the username and email address as shown below example during installation.\n\n\n\nCommand Prompt\n\ngit config --global user.name \"BenKorea\"\n\n\n\n\n\nCommand Prompt\n\ngit config --global user.email \"kimbi.kirams@gmail.com\"\n\n\n\n\nConfirming User Information Settings\nYou can check the username and email address with the following commands.\n\n\n\nCommand Prompt\n\ngit config user.name\n\n\n\n\n\nCommand Prompt\n\ngit config user.email\n\n\n\n\nSetting the Default Branch\nIf you want to set the default branch name to main instead of master, you can set init.defaultBranch to main as shown below.\n\n\n\nCommand Prompt\n\ngit config --global init.defaultBranch main",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#applying-git-to-a-project",
    "href": "posts/Git-setup/Git_setup.html#applying-git-to-a-project",
    "title": "Git and Github setup",
    "section": "Applying Git to a Project",
    "text": "Applying Git to a Project\nTo start version management, run the git init command in the desired folder (= project folder). As a result of the execution, a hidden folder named .git is created, and this folder contains the files necessary for version management.\n\n\n\nCommand Prompt\n\ngit init",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Git-setup/Git_setup.html#what-is-github",
    "href": "posts/Git-setup/Git_setup.html#what-is-github",
    "title": "Git and Github setup",
    "section": "What is Github?",
    "text": "What is Github?\nGitHub is a web-based hosting service based on Git, providing a platform for source code management and collaboration. GitHub uses Git to centrally manage the codebase of projects, providing developers with various tools and interfaces to efficiently conduct code reviews, issue management, project management, and collaboration. Through Pull Requests (PRs), you can review and merge code changes, and it supports automation features such as Continuous Integration/Continuous Deployment (CI/CD) to increase the productivity and quality of software development.",
    "crumbs": [
      "Tools",
      "Git and Github setup"
    ]
  },
  {
    "objectID": "posts/Chi-square-test-in-R/Chi_square_test_in_R.html",
    "href": "posts/Chi-square-test-in-R/Chi_square_test_in_R.html",
    "title": "Chi-square test in R",
    "section": "",
    "text": "Package stats version 4.4.1\nIn R, chisq.test is implemented to perform both chi-squared contingency table tests and goodness-of-fit tests. It is recommended to learn how to use this function in R through the manual (= ?chisq.test).\nHowever, the following is recorded for future reference.\n\n\n\n\n\nchisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE, B = 2000)\n\n\n\n\n\nx: a numeric vector or matrix. Both x and y can be specified as factors.\ny: a numeric vector; ignored if x is a matrix. If x is a factor, y must be a factor of the same length.\ncorrect: a logical indicating whether to apply continuity correction when calculating the test statistic for a 2x2 table. Continuity correction subtracts 1/2 from all |O - E| differences. If simulate.p.value = TRUE, no correction is applied.\n\n\n\n\n\n\n\nCriteria for applying continuity correction\n\n\n\n\n\n\nIn the past, Yates’ continuity correction was applied when the expected value of a cell in a 2x2 contingency table was less than 5.\nHowever, recently, Fisher’s exact test is mainly applied for small cell frequencies.\n\n\n\n\n\np: a probability vector of the same length as x. If any element of p is negative, an error occurs.\n\n\n\n\n\n\n\nThe p argument is used for goodness-of-fit tests and …\n\n\n\n\n\n\nThe p argument is used to specify the expected probabilities when performing a goodness-of-fit test.\nBy default, if p is not given, it assumes that each category has the same probability.\nThat is, the default value is set to rep(1/length(x), length(x)), which assigns equal probabilities to all categories.\nIf you assume that each category has different theoretical probabilities, you must specify these probabilities as a vector.\nFor example, if you assume that categories A, B, and C have probabilities of 40%, 30%, and 30%, respectively, you can set p = c(0.4, 0.3, 0.3).\n\n\n\n\n\nrescale.p: a logical scalar; if TRUE, p is rescaled so that it can sum to 1. If FALSE and the sum of p is not 1, an error occurs.\n\n\n\n\n\n\n\nEnsuring that the sum of p is 1 …\n\n\n\n\n\n\nThe p probability factor should ideally sum to 1.\nIf the sum of the user-input p values is not 1 and rescale.p is set to TRUE, the p values are readjusted to sum to 1.\n\n\n\n\n\nsimulate.p.value: a logical indicating whether to calculate the p-value by Monte Carlo simulation.\nB: an integer specifying the number of replicates to use in the Monte Carlo test.\n\n\n\n\n\n\n\nThe reason for Monte Carlo simulation …\n\n\n\n\n\n\nThe Monte Carlo simulation option allows you to calculate the p-value.\nThis option is set to FALSE by default, but if set to TRUE, it estimates the p-value through random sampling instead of directly calculating it.\nMonte Carlo simulation randomly samples data and calculates the chi-squared test statistic for each sample to estimate the distribution and derive the p-value.\nIt is an option to estimate the p-value. The integer specifies the number of replicates to use in the Monte Carlo test. The default value is 2000, and you can increase the number of replicates for more accuracy.\nFor example, setting B = 10000 will give you a more precise p-value.\nMonte Carlo simulation is computationally expensive, so using it on very large datasets may take a long time.",
    "crumbs": [
      "R",
      "Chi-square test in R"
    ]
  },
  {
    "objectID": "posts/Chi-square-test-in-R/Chi_square_test_in_R.html#chisq.test-1",
    "href": "posts/Chi-square-test-in-R/Chi_square_test_in_R.html#chisq.test-1",
    "title": "Chi-square test in R",
    "section": "",
    "text": "Package stats version 4.4.1\nIn R, chisq.test is implemented to perform both chi-squared contingency table tests and goodness-of-fit tests. It is recommended to learn how to use this function in R through the manual (= ?chisq.test).\nHowever, the following is recorded for future reference.\n\n\n\n\n\nchisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE, B = 2000)\n\n\n\n\n\nx: a numeric vector or matrix. Both x and y can be specified as factors.\ny: a numeric vector; ignored if x is a matrix. If x is a factor, y must be a factor of the same length.\ncorrect: a logical indicating whether to apply continuity correction when calculating the test statistic for a 2x2 table. Continuity correction subtracts 1/2 from all |O - E| differences. If simulate.p.value = TRUE, no correction is applied.\n\n\n\n\n\n\n\nCriteria for applying continuity correction\n\n\n\n\n\n\nIn the past, Yates’ continuity correction was applied when the expected value of a cell in a 2x2 contingency table was less than 5.\nHowever, recently, Fisher’s exact test is mainly applied for small cell frequencies.\n\n\n\n\n\np: a probability vector of the same length as x. If any element of p is negative, an error occurs.\n\n\n\n\n\n\n\nThe p argument is used for goodness-of-fit tests and …\n\n\n\n\n\n\nThe p argument is used to specify the expected probabilities when performing a goodness-of-fit test.\nBy default, if p is not given, it assumes that each category has the same probability.\nThat is, the default value is set to rep(1/length(x), length(x)), which assigns equal probabilities to all categories.\nIf you assume that each category has different theoretical probabilities, you must specify these probabilities as a vector.\nFor example, if you assume that categories A, B, and C have probabilities of 40%, 30%, and 30%, respectively, you can set p = c(0.4, 0.3, 0.3).\n\n\n\n\n\nrescale.p: a logical scalar; if TRUE, p is rescaled so that it can sum to 1. If FALSE and the sum of p is not 1, an error occurs.\n\n\n\n\n\n\n\nEnsuring that the sum of p is 1 …\n\n\n\n\n\n\nThe p probability factor should ideally sum to 1.\nIf the sum of the user-input p values is not 1 and rescale.p is set to TRUE, the p values are readjusted to sum to 1.\n\n\n\n\n\nsimulate.p.value: a logical indicating whether to calculate the p-value by Monte Carlo simulation.\nB: an integer specifying the number of replicates to use in the Monte Carlo test.\n\n\n\n\n\n\n\nThe reason for Monte Carlo simulation …\n\n\n\n\n\n\nThe Monte Carlo simulation option allows you to calculate the p-value.\nThis option is set to FALSE by default, but if set to TRUE, it estimates the p-value through random sampling instead of directly calculating it.\nMonte Carlo simulation randomly samples data and calculates the chi-squared test statistic for each sample to estimate the distribution and derive the p-value.\nIt is an option to estimate the p-value. The integer specifies the number of replicates to use in the Monte Carlo test. The default value is 2000, and you can increase the number of replicates for more accuracy.\nFor example, setting B = 10000 will give you a more precise p-value.\nMonte Carlo simulation is computationally expensive, so using it on very large datasets may take a long time.",
    "crumbs": [
      "R",
      "Chi-square test in R"
    ]
  },
  {
    "objectID": "posts/built-in-dataset/built_in_dataset.html",
    "href": "posts/built-in-dataset/built_in_dataset.html",
    "title": "Built-in Data sets in R",
    "section": "",
    "text": "data(\"volcano\") ## built-in dataset 중에서 volcano 사용 \nlibrary(survival) \ndata(package=\"survival\") ## survival package에 어떤 데이터 세트들이 있는지 확인 \ndata(cancer) ## data(cancer, package=\"survival\") 와 같이 사용해도 된다. \nstr(lung) ## cancer dataset에는 다양한 암종류의 생존분석용 데이터가 들어가 있다.\n\n'data.frame':   228 obs. of  10 variables:\n $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n $ time     : num  306 455 1010 210 883 ...\n $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n $ meal.cal : num  1175 1225 NA 1150 NA ...\n $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...",
    "crumbs": [
      "R",
      "Built-in Data sets in R"
    ]
  },
  {
    "objectID": "posts/built-in-dataset/built_in_dataset.html#volcano",
    "href": "posts/built-in-dataset/built_in_dataset.html#volcano",
    "title": "Built-in Data sets in R",
    "section": "",
    "text": "data(\"volcano\") ## built-in dataset 중에서 volcano 사용 \nlibrary(survival) \ndata(package=\"survival\") ## survival package에 어떤 데이터 세트들이 있는지 확인 \ndata(cancer) ## data(cancer, package=\"survival\") 와 같이 사용해도 된다. \nstr(lung) ## cancer dataset에는 다양한 암종류의 생존분석용 데이터가 들어가 있다.\n\n'data.frame':   228 obs. of  10 variables:\n $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n $ time     : num  306 455 1010 210 883 ...\n $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n $ meal.cal : num  1175 1225 NA 1150 NA ...\n $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...",
    "crumbs": [
      "R",
      "Built-in Data sets in R"
    ]
  },
  {
    "objectID": "posts/built-in-dataset/built_in_dataset.html#rotterdam-breast-cancer-dataset-in-survival-package",
    "href": "posts/built-in-dataset/built_in_dataset.html#rotterdam-breast-cancer-dataset-in-survival-package",
    "title": "Built-in Data sets in R",
    "section": "rotterdam : breast cancer dataset in survival package",
    "text": "rotterdam : breast cancer dataset in survival package\n\nlibrary(moonBook) \nmytable(grade~. , data=rotterdam)\n\n\n       Descriptive Statistics by 'grade'       \n———————————————————————————————————————————————— \n                  2               3          p  \n               (N=794)        (N=2188)    \n———————————————————————————————————————————————— \n pid       1328.4 ± 865.1  1569.0 ± 860.9  0.000\n year       1987.9 ±  3.1   1988.3 ±  3.0  0.004\n age         54.4 ± 12.7     55.3 ± 13.1   0.086\n meno                                      0.000\n   - 0       392 (49.4%)     920 (42.0%)        \n   - 1       402 (50.6%)    1268 (58.0%)        \n size                                      0.000\n   - &lt;=20    462 (58.2%)     925 (42.3%)        \n   - 20-50   290 (36.5%)    1001 (45.7%)        \n   - &gt;50     42 ( 5.3%)      262 (12.0%)        \n nodes        2.0 ±  3.7      3.0 ±  4.6   0.000\n pgr        236.2 ± 385.8   134.9 ± 242.8  0.000\n er         179.8 ± 291.9   161.8 ± 265.0  0.127\n hormon                                    0.000\n   - 0       735 (92.6%)    1908 (87.2%)        \n   - 1       59 ( 7.4%)      280 (12.8%)        \n chemo                                     1.000\n   - 0       640 (80.6%)    1762 (80.5%)        \n   - 1       154 (19.4%)     426 (19.5%)        \n rtime     2458.3 ± 1408.6 1967.1 ± 1370.8 0.000\n recur                                     0.000\n   - 0       480 (60.5%)     984 (45.0%)        \n   - 1       314 (39.5%)    1204 (55.0%)        \n dtime     2908.9 ± 1278.6 2495.2 ± 1287.8 0.000\n death                                     0.000\n   - 0       532 (67.0%)    1178 (53.8%)        \n   - 1       262 (33.0%)    1010 (46.2%)        \n———————————————————————————————————————————————— \n\n\n\n# suppressMessages(library(dplyr)) \n# mytable(grade~. , data=rotterdam) %&gt;% mylatex() %&gt;% cat}\n\nLaTeX을 이용하여 깔끔한 논문형식의 테이블을 만들 수 있는 방법은 위 코드를 실행시킬 수 있게 되면 다음에 소개하겠다.",
    "crumbs": [
      "R",
      "Built-in Data sets in R"
    ]
  },
  {
    "objectID": "posts/about/about.html",
    "href": "posts/about/about.html",
    "title": "for Medical Big Data Analysis",
    "section": "",
    "text": "The Necessity of R and Python in Medical Data Research\nStatistical programs like SPSS or MedCalc are widely used in medical research. However, they are typically only available within hospital settings and cannot automate analysis processes.\nIn contrast, the open-source statistical language R allows automation of analysis processes through simple programming, excels in visualization, and seamlessly integrates with medical paper writing and web publication.\nPython, a general-purpose open-source programming language, is strong in machine learning and deep learning, and integrates well with R. Both languages also have the advantage of being capable of big data analysis.\nSome journals even accept submissions in R Markdown format to ensure reproducibility. Therefore, these open-source tools are advantageous for researchers, including those involved in clinical research.\n\n\nR Python Study Group\nThis site was created to assist in-house researchers who are interested in analyzing medical data using R and Python by RPythonStudy Group.\n\n\n\ne-mail\n\nr.python.study@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nShiny Application Example\n\n\n\n\n\n\nR\n\n\nAPI\n\n\nShiny\n\n\nexample\n\n\n\nLearn how to set up and use Shiny\n\n\n\n\n\nOct 21, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nChi-square test in R\n\n\n\n\n\n\nR\n\n\nChi-square\n\n\n\nBrief introduction on basic statistics functions in R\n\n\n\n\n\nJul 17, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Goverment Data\n\n\n\n\n\n\nResource\n\n\nmedical data\n\n\n\nHow to use OGD and Examples\n\n\n\n\n\nOct 2, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nGit Setup in RStudio\n\n\n\n\n\n\nGit\n\n\nsetup\n\n\n\nLearn how to set up and use Git in RStudio\n\n\n\n\n\nOct 5, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nChi-Square Test\n\n\n\n\n\n\nstatistics\n\n\nChi-Square\n\n\nCategoical Data\n\n\n\nStatistics concept of Chi-Square test\n\n\n\n\n\nOct 4, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nVS Code Setup\n\n\n\n\n\n\nPython\n\n\nVS Code\n\n\nsetup\n\n\n\nGuidance on Installing and Configuring VS Code\n\n\n\n\n\nAug 31, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nGroup comparison in continuous variables\n\n\n\n\n\n\nstatistics\n\n\n\nStatistics concept in group comparison in continuous variables\n\n\n\n\n\nSep 27, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to IT\n\n\n\n\n\n\nintroduction\n\n\nIT\n\n\n\nIntroducing IT knowledges that are helpful for research groups\n\n\n\n\n\nSep 27, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Quarto\n\n\n\n\n\n\nintroduction\n\n\n\nQuarto is a modern tool for reproducible research and data-centric reporting, supporting dynamic documentation in multi-language environments and integrating programming workflows.\n\n\n\n\n\nSep 1, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Resources\n\n\n\n\n\n\nintroduction\n\n\n\nIntroducing medical (big) data resources that are helpful for research\n\n\n\n\n\nAug 31, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to statistics\n\n\n\n\n\n\nintroduction\n\n\n\nStatistics in R\n\n\n\n\n\nSep 27, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival analysis\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\nsurvival analysis\n\n\n\njust copy from other well-organized webpages\n\n\n\n\n\nAug 31, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Utilities\n\n\n\n\n\n\nintroduction\n\n\n\nIntroducing open source programs that are helpful for research groups\n\n\n\n\n\nSep 27, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nManuscripting with Quarto Example\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\n\nQuarto example how to wwite journal manuscript with unidentified real data\n\n\n\n\n\nAug 31, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nGit and Github setup\n\n\n\n\n\n\nGit\n\n\nsetup\n\n\n\nGuidance on Installing and Configuring Git & Github\n\n\n\n\n\nMay 6, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nPython setup\n\n\n\n\n\n\nPython\n\n\nsetup\n\n\n\nGuidance on Installing and Configuring Python\n\n\n\n\n\nSep 8, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python\n\n\n\n\n\n\nPython\n\n\nintroduction\n\n\n\nBrief history of Python development and application examples in medical research\n\n\n\n\n\nSep 7, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nR syntax\n\n\n\n\n\n\nR\n\n\nsyntax\n\n\n\nUnderstanding data type and manupulation\n\n\n\n\n\nAug 31, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nBasic statistics functions\n\n\n\n\n\n\nR\n\n\n\nBrief introduction on basic statistics functions in R\n\n\n\n\n\nJul 17, 2024\n\n\nHaewon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nBuilt-in Data sets in R\n\n\n\n\n\n\nR\n\n\n\nR includes built-in datasets. These datasets are useful for testing, teaching, and practicing.\n\n\n\n\n\nJul 17, 2024\n\n\nHaewon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nOutliers detection in continuous variables\n\n\n\n\n\n\nR\n\n\npreprocessing\n\n\noutlier\n\n\n\nHow to detect outliers in continuous variables using normal distribution and Box-Cox transformation.\n\n\n\n\n\nJul 8, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nR setup\n\n\n\n\n\n\nR\n\n\nsetup\n\n\n\nGuidance on Installing and Configuring R and Rtools\n\n\n\n\n\nMay 6, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio setup\n\n\n\n\n\n\nR\n\n\nRStudio\n\n\nsetup\n\n\n\nGuidance on Installing and Configuring RStudio\n\n\n\n\n\nMay 6, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduciton to R\n\n\n\n\n\n\nR\n\n\nintroduction\n\n\n\nBrief history of R developement and application examples in medical research\n\n\n\n\n\nMay 4, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\n\n\n\n\n\n\nfor Medical Big Data Analysis\n\n\n\n\n\n\nintroduction\n\n\n\nThis site was created to assist in-house researchers who are interested in analyzing medical data using R and Python.\n\n\n\n\n\nApr 12, 2024\n\n\nRPythonStudyGroup feat. ChatGPT\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/basic-statistics/basic_statistics.html",
    "href": "posts/basic-statistics/basic_statistics.html",
    "title": "Basic statistics functions",
    "section": "",
    "text": "t-test\nR function : t.test -\noption arguments : alternative = c(“two.sided”, “less”, “greater”), formula (종속변수~ 독립변수)\nhelp files : ?t.test 를 치면 함수의 argument, values(results), detail에 대해서 설명이 나옴\n\nlibrary(survival)\ndata(\"rotterdam\")\n\nWarning in data(\"rotterdam\"): data set 'rotterdam' not found\n\ngroup1 &lt;- rotterdam[ rotterdam$grade == 2, \"age\"]\ngroup2 &lt;- rotterdam[ rotterdam$grade != 2, \"age\"]\nt.test(group1, group2) ## unmatched 임의의 두개의 vector로 비교\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = -1.7437, df = 1444.4, p-value = 0.08143\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.959901  0.115264\nsample estimates:\nmean of x mean of y \n 54.38161  55.30393 \n\n\n\nt.test(age~meno,data=rotterdam) ## matched 한개의 데이터프레임에서 paired t-test\n\n\n    Welch Two Sample t-test\n\ndata:  age by meno\nt = -76.545, df = 2972.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -21.53192 -20.45636\nsample estimates:\nmean in group 0 mean in group 1 \n       43.30107        64.29521 \n\n\n\n\n𝜒2 (chi-square) test\nR function : chisq.test\n\ntable(rotterdam[,c(\"hormon\",\"size\")])\n\n      size\nhormon &lt;=20 20-50  &gt;50\n     0 1283  1119  241\n     1  104   172   63\n\n\n\nchisq.test(table(rotterdam[,c(\"hormon\",\"size\")]), correct = TRUE)\n\n\n    Pearson's Chi-squared test\n\ndata:  table(rotterdam[, c(\"hormon\", \"size\")])\nX-squared = 51.92, df = 2, p-value = 5.317e-12\n\n\n\nchisq.test(rotterdam$hormon, rotterdam$chemo)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  rotterdam$hormon and rotterdam$chemo\nX-squared = 29.771, df = 1, p-value = 4.862e-08\n\n\n\nx &lt;- matrix(c(12, 5, 7, 7), ncol = 2) ## matrix를 만들어서 검정하는 방법\nx\n\n     [,1] [,2]\n[1,]   12    7\n[2,]    5    7\n\n\n\nchisq.test(x)$p.value ## chisq test의 결과물은 list이다 여기서 p.value 부분만 출력\n\n[1] 0.4233054\n\n\n\nchisq.test(x, simulate.p.value = TRUE, B = 10000)$p.value\n\n[1] 0.2942706\n\n\n\n\nGeneralized linear regression and Loess smoothing (LOcal regrESSion)\nR function : glm (generalized linear models) 다중 선형회귀\n\nsuppressMessages(library(ggplot2))\ndata(economics, package=\"ggplot2\")\neconomics$index &lt;- 1:nrow(economics) # create index variable\nglm_model1 &lt;- glm(psavert~pop, data = economics)\nsummary(glm_model1)\n\n\nCall:\nglm(formula = psavert ~ pop, data = economics)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.595e+01  4.812e-01   53.92   &lt;2e-16 ***\npop         -6.758e-05  1.852e-06  -36.48   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.645601)\n\n    Null deviance: 5034.6  on 573  degrees of freedom\nResidual deviance: 1513.3  on 572  degrees of freedom\nAIC: 2191.4\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nanova(glm_model1)\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: psavert\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev    F    Pr(&gt;F)    \nNULL                   573     5034.6                   \npop   1   3521.3       572     1513.3 1331 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplot(glm_model1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 데이터 선택\neconomics &lt;- economics[100:180, ]  # 좁은 범위 선택\n\n# Loess 모델 생성\nloessMod10 &lt;- loess(uempmed ~ index, data=economics, span=0.10)  # 10% smoothing span\nloessMod25 &lt;- loess(uempmed ~ index, data=economics, span=0.25)\nloessMod50 &lt;- loess(uempmed ~ index, data=economics, span=0.50)\n\n# 예측값 계산\nsmoothed10 &lt;- predict(loessMod10)\nsmoothed25 &lt;- predict(loessMod25)\nsmoothed50 &lt;- predict(loessMod50)\n\n# 그래프 그리기\nplot(economics$date, economics$uempmed, type=\"l\", main=\"Loess Smoothing and Prediction\", xlab=\"Date\", ylab=\"Unemployment Median\")\n\n# 예측된 smoothed 라인 추가\nlines(economics$date, smoothed10, col=\"red\")\nlines(economics$date, smoothed25, col=\"green\")\nlines(economics$date, smoothed50, col=\"blue\")\n\n위 코드가 제 개발환경에서 실행시 오류가 발생하므로 추후 수정해서 올려 드리겠습니다.\n\neconomics &lt;- economics[1:58,]\nlibrary(ggplot2)\nggplot(data=economics, aes(x=index, y=uempmed))+\ngeom_point()+\ngeom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOne-way ANOVA\n\nsuppressMessages(library(psych))\nPlantGrowth ## 내장 dataset\n\n   weight group\n1    4.17  ctrl\n2    5.58  ctrl\n3    5.18  ctrl\n4    6.11  ctrl\n5    4.50  ctrl\n6    4.61  ctrl\n7    5.17  ctrl\n8    4.53  ctrl\n9    5.33  ctrl\n10   5.14  ctrl\n11   4.81  trt1\n12   4.17  trt1\n13   4.41  trt1\n14   3.59  trt1\n15   5.87  trt1\n16   3.83  trt1\n17   6.03  trt1\n18   4.89  trt1\n19   4.32  trt1\n20   4.69  trt1\n21   6.31  trt2\n22   5.12  trt2\n23   5.54  trt2\n24   5.50  trt2\n25   5.37  trt2\n26   5.29  trt2\n27   4.92  trt2\n28   6.15  trt2\n29   5.80  trt2\n30   5.26  trt2\n\n\n\nplot(weight~group, data = PlantGrowth) ## Boxplot으로 자동으로 그려준다.\n\n\n\n\n\n\n\n\n\nwith(PlantGrowth, describeBy(weight,group))\n\n\n Descriptive statistics by group \ngroup: ctrl\n   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 10 5.03 0.58   5.15       5 0.72 4.17 6.11  1.94 0.23    -1.12 0.18\n------------------------------------------------------------ \ngroup: trt1\n   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 10 4.66 0.79   4.55    4.62 0.53 3.59 6.03  2.44 0.47     -1.1 0.25\n------------------------------------------------------------ \ngroup: trt2\n   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 10 5.53 0.44   5.44     5.5 0.36 4.92 6.31  1.39 0.48    -1.16 0.14\n\n\n\nbartlett.test(PlantGrowth$weight ~ PlantGrowth$group) ## 등분산 가정을 체크함\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  PlantGrowth$weight by PlantGrowth$group\nBartlett's K-squared = 2.8786, df = 2, p-value = 0.2371\n\n\n\naov_model &lt;- aov(weight~group, data = PlantGrowth)\nsummary(aov_model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCorrelation tests\nPearson correlation formula\n\nSpearman correlation formula : non-parametric\n\nwhere 𝑥′ = 𝑟𝑎𝑛𝑘(𝑥) and 𝑦′ = 𝑟𝑎𝑛𝑘(𝑦)\nKendall correlation formula : non-parametric\n\nwhere 𝑛𝑐 : number of concordant pairs, 𝑛𝑑 : number of concordant pairs, 𝑛 : size of 𝑥 + 𝑦\n\nres &lt;- cor.test(economics$index, economics$uempmed, method = \"pearson\")\nres\n\n\n    Pearson's product-moment correlation\n\ndata:  economics$index and economics$uempmed\nt = 10.152, df = 56, p-value = 2.639e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6901407 0.8802298\nsample estimates:\n      cor \n0.8049464 \n\n\n\nres$p.value ## res는 리스트형태로 나오는 결과물이다. 여기에서 필요한 값만 골라냄\n\n[1] 2.639201e-14\n\n\n\nres$estimate\n\n      cor \n0.8049464 \n\n\n\nres2 &lt;- cor.test(economics$index, economics$uempmed, method = \"spearman\")\n\nWarning in cor.test.default(economics$index, economics$uempmed, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\nres2\n\n\n    Spearman's rank correlation rho\n\ndata:  economics$index and economics$uempmed\nS = 9033.3, p-value = 1.578e-10\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7221299 \n\n\n\nres3 &lt;- cor.test(economics$index, economics$uempmed, method = \"kendall\")\nres3\n\n\n    Kendall's rank correlation tau\n\ndata:  economics$index and economics$uempmed\nz = 6.0101, p-value = 1.854e-09\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5541495 \n\n\n\n\nSurvival analysis\n\nKaplan Meier Analysis - Basic survival model survival::Surv\n\n\nkm &lt;- Surv(rotterdam$dtime, event = rotterdam$death) ## default type : \"right\"\nplot(km) ## km - Surv class (time, status) 가지고 있는 리스트\n\n\n\n\n\n\n\n\n\nmedian(km); mean(km) ## Surv 객체에 대한 method 함수들이 있다. plot.Surv포함\n\n$quantile\n  50 \n4033 \n\n$lower\n  50 \n3888 \n\n$upper\n  50 \n4309 \n\n\n[1] 1302.883\n\n\n\nKaplan Meier Analysis - survfit model km_\n\n\nkm_fit &lt;- survfit(km~rotterdam$meno)\nsummary(km_fit, c(365*1:19)) ### 정해진 time에 맞는 생존테이블표를 만든다.\n\nCall: survfit(formula = km ~ rotterdam$meno)\n\n                rotterdam$meno=0 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365   1298      13    0.990 0.00274        0.985        0.995\n  730   1236      56    0.947 0.00617        0.935        0.959\n 1095   1140      90    0.878 0.00905        0.861        0.896\n 1460   1071      59    0.833 0.01035        0.813        0.853\n 1825    973      59    0.786 0.01141        0.764        0.809\n 2190    865      50    0.745 0.01222        0.721        0.769\n 2555    754      43    0.706 0.01292        0.681        0.732\n 2920    611      31    0.675 0.01354        0.649        0.702\n 3285    480      15    0.656 0.01397        0.629        0.684\n 3650    345      21    0.623 0.01505        0.594        0.653\n 4015    217      13    0.595 0.01631        0.563        0.627\n 4380    138       6    0.575 0.01760        0.542        0.611\n 4745     88       4    0.554 0.02000        0.516        0.594\n 5110     54       3    0.530 0.02334        0.487        0.578\n 5475     29       2    0.506 0.02783        0.455        0.564\n 5840     14       1    0.481 0.03617        0.415        0.558\n 6205      5       2    0.391 0.06658        0.280        0.546\n 6570      3       0    0.391 0.06658        0.280        0.546\n 6935      1       0    0.391 0.06658        0.280        0.546\n\n                rotterdam$meno=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365   1616      46    0.972 0.00402       0.9645        0.980\n  730   1508     103    0.910 0.00701       0.8966        0.924\n 1095   1366     129    0.832 0.00919       0.8143        0.850\n 1460   1245     111    0.764 0.01046       0.7440        0.785\n 1825   1111      87    0.710 0.01122       0.6883        0.732\n 2190    944      82    0.655 0.01186       0.6326        0.679\n 2555    819      58    0.614 0.01231       0.5901        0.638\n 2920    642      45    0.577 0.01272       0.5530        0.603\n 3285    474      42    0.536 0.01334       0.5104        0.563\n 3650    342      31    0.496 0.01418       0.4685        0.524\n 4015    188      35    0.430 0.01614       0.3998        0.463\n 4380    113      17    0.386 0.01772       0.3531        0.423\n 4745     62       6    0.358 0.01989       0.3210        0.399\n 5110     28       7    0.309 0.02431       0.2652        0.361\n 5475     14       1    0.293 0.02796       0.2431        0.353\n 5840      8       3    0.217 0.04323       0.1469        0.321\n 6205      4       0    0.217 0.04323       0.1469        0.321\n 6570      1       1    0.163 0.05710       0.0819        0.324\n 6935      1       0    0.163 0.05710       0.0819        0.324\n\n\n\nsuppressMessages(library(survminer))\nplot(km_fit, col = rainbow(2), lty=1:2)\nlegend(\"topright\", legend = c(\"Menopause(-)\",\"Menopause(+)\"),\n       col= rainbow(2), lty=1:2)\nlibrary(survminer)\nggsurvplot(km_fit, data = rotterdam,\n           conf.int = T, xscale = 365.2425, ## xscale can be \"d_y\"\n           break.x.by = 5*365.2425,\n           pval = T, pval.size =4, surv.median.line = \"hv\",\n           risk.table = FALSE, ## if TRUE, risk table is displayed under graph\n           legend.title=\"Menopause\", legend.labs=c(\"No\",\"Yes\"),\n           palette = c(\"#E7B800\", \"#2E9FDF\"),)\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## ggplot + survminer package\n\nCox Proportional model\n\n\nargs(coxph)\n\nfunction (formula, data, weights, subset, na.action, init, control, \n    ties = c(\"efron\", \"breslow\", \"exact\"), singular.ok = TRUE, \n    robust, model = FALSE, x = FALSE, y = TRUE, tt, method = ties, \n    id, cluster, istate, statedata, nocenter = c(-1, 0, 1), ...) \nNULL\n\n\n\nlibrary(carData) ## Rossi data set 이용하기 위해서 사용\nsuppressMessages(library(car)) ## Anova function\ncolnames(Rossi) # emp1-52 : factor (yes or no)\n\n [1] \"week\"   \"arrest\" \"fin\"    \"age\"    \"race\"   \"wexp\"   \"mar\"    \"paro\"  \n [9] \"prio\"   \"educ\"   \"emp1\"   \"emp2\"   \"emp3\"   \"emp4\"   \"emp5\"   \"emp6\"  \n[17] \"emp7\"   \"emp8\"   \"emp9\"   \"emp10\"  \"emp11\"  \"emp12\"  \"emp13\"  \"emp14\" \n[25] \"emp15\"  \"emp16\"  \"emp17\"  \"emp18\"  \"emp19\"  \"emp20\"  \"emp21\"  \"emp22\" \n[33] \"emp23\"  \"emp24\"  \"emp25\"  \"emp26\"  \"emp27\"  \"emp28\"  \"emp29\"  \"emp30\" \n[41] \"emp31\"  \"emp32\"  \"emp33\"  \"emp34\"  \"emp35\"  \"emp36\"  \"emp37\"  \"emp38\" \n[49] \"emp39\"  \"emp40\"  \"emp41\"  \"emp42\"  \"emp43\"  \"emp44\"  \"emp45\"  \"emp46\" \n[57] \"emp47\"  \"emp48\"  \"emp49\"  \"emp50\"  \"emp51\"  \"emp52\" \n\n\n\ncox_model1 &lt;- coxph(Surv(week, arrest) ~\nfin + age + race + wexp + mar + paro + prio,\ndata = Rossi)\nsummary(cox_model1)\n\nCall:\ncoxph(formula = Surv(week, arrest) ~ fin + age + race + wexp + \n    mar + paro + prio, data = Rossi)\n\n  n= 432, number of events= 114 \n\n                   coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nfinyes         -0.37942   0.68426  0.19138 -1.983  0.04742 * \nage            -0.05744   0.94418  0.02200 -2.611  0.00903 **\nraceother      -0.31390   0.73059  0.30799 -1.019  0.30812   \nwexpyes        -0.14980   0.86088  0.21222 -0.706  0.48029   \nmarnot married  0.43370   1.54296  0.38187  1.136  0.25606   \nparoyes        -0.08487   0.91863  0.19576 -0.434  0.66461   \nprio            0.09150   1.09581  0.02865  3.194  0.00140 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nfinyes            0.6843     1.4614    0.4702    0.9957\nage               0.9442     1.0591    0.9043    0.9858\nraceother         0.7306     1.3688    0.3995    1.3361\nwexpyes           0.8609     1.1616    0.5679    1.3049\nmarnot married    1.5430     0.6481    0.7300    3.2614\nparoyes           0.9186     1.0886    0.6259    1.3482\nprio              1.0958     0.9126    1.0360    1.1591\n\nConcordance= 0.64  (se = 0.027 )\nLikelihood ratio test= 33.27  on 7 df,   p=2e-05\nWald test            = 32.11  on 7 df,   p=4e-05\nScore (logrank) test = 33.53  on 7 df,   p=2e-05\n\n\n\nAnova(cox_model1)\n\nAnalysis of Deviance Table (Type II tests)\n     LR Chisq Df Pr(&gt;Chisq)   \nfin    3.9862  1   0.045874 * \nage    7.9880  1   0.004709 **\nrace   1.1252  1   0.288812   \nwexp   0.5003  1   0.479352   \nmar    1.4312  1   0.231572   \nparo   0.1870  1   0.665450   \nprio   8.9766  1   0.002735 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(cox_model1)\n\nAnalysis of Deviance Table\n Cox model: response is Surv(week, arrest)\nTerms added sequentially (first to last)\n\n      loglik   Chisq Df Pr(&gt;|Chi|)    \nNULL -675.38                          \nfin  -673.46  3.8371  1  0.0501315 .  \nage  -666.24 14.4526  1  0.0001437 ***\nrace -665.84  0.7887  1  0.3745021    \nwexp -664.22  3.2472  1  0.0715467 .  \nmar  -663.58  1.2841  1  0.2571359    \nparo -663.24  0.6798  1  0.4096690    \nprio -658.75  8.9766  1  0.0027346 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n모델의 전체적인 생존곡선을 알고 싶으면 survfit 함수를 이용해서 생존곡선을 그릴 수 있다\n\nplot(survfit(cox_model1), ylim = c(0.6,1),xlab = \"weeks\",\nylab = \"Proportion not rearrested\")\n\n\n\n\n\n\n\n\n\nRossi.fin &lt;- with(Rossi, data.frame(fin=c(0, 1),\nage=rep(mean(age), 2), race=rep(mean(race == \"other\"), 2),\nwexp=rep(mean(wexp == \"yes\"), 2), mar=rep(mean(mar == \"not married\"), 2),\nparo=rep(mean(paro == \"yes\"), 2), prio=rep(mean(prio), 2)))\n## fin = 0,1 이것을 두그룹으로 나누고 나머지 변수들은 평균적인 값으로 고정해 버림\nplot(survfit(cox_model1, newdata = Rossi.fin), conf.int = T,\nlty = c(1,2), ylim = c(0.6,1),xlab = \"weeks\",\nylab = \"Proportion not rearrested\", col = c(\"blue\",\"red\")\n)\n\nWarning in model.frame.default(data = structure(list(fin = c(0, 1), age =\nc(24.5972222222222, : variable 'fin' is not a factor\n\n\nWarning in model.frame.default(data = structure(list(fin = c(0, 1), age =\nc(24.5972222222222, : variable 'race' is not a factor\n\n\nWarning in model.frame.default(data = structure(list(fin = c(0, 1), age =\nc(24.5972222222222, : variable 'wexp' is not a factor\n\n\nWarning in model.frame.default(data = structure(list(fin = c(0, 1), age =\nc(24.5972222222222, : variable 'mar' is not a factor\n\n\nWarning in model.frame.default(data = structure(list(fin = c(0, 1), age =\nc(24.5972222222222, : variable 'paro' is not a factor\n\nlegend(\"bottomleft\", legend=c(\"fin = no\", \"fin = yes\"), lty=c(1 ,2),\ncol=c(\"blue\",\"red\") , inset=0.02)",
    "crumbs": [
      "Statistics",
      "Basic statistics functions"
    ]
  },
  {
    "objectID": "posts/Chi-square-test/Chi_square_test.html",
    "href": "posts/Chi-square-test/Chi_square_test.html",
    "title": "Chi-Square Test",
    "section": "",
    "text": "The Chi-Square Test was developed by the British statistician Karl Pearson in 1900. It is a statistical method that analyzes the difference between observed frequencies and expected frequencies of categorical data to test for independence between two variables or to determine if the data fits a specific theoretical distribution.",
    "crumbs": [
      "Statistics",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "posts/Chi-square-test/Chi_square_test.html#history",
    "href": "posts/Chi-square-test/Chi_square_test.html#history",
    "title": "Chi-Square Test",
    "section": "",
    "text": "The Chi-Square Test was developed by the British statistician Karl Pearson in 1900. It is a statistical method that analyzes the difference between observed frequencies and expected frequencies of categorical data to test for independence between two variables or to determine if the data fits a specific theoretical distribution.",
    "crumbs": [
      "Statistics",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "posts/Chi-square-test/Chi_square_test.html#statistics",
    "href": "posts/Chi-square-test/Chi_square_test.html#statistics",
    "title": "Chi-Square Test",
    "section": "Statistics",
    "text": "Statistics\nPearson calculated the test statistic by taking the difference between the observed frequency and the expected value in each cell, squaring this difference to reflect absolute values, and then dividing it by the expected value to calculate the test statistic Equation 1.\n\\[\n\\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\tag{1}\\]",
    "crumbs": [
      "Statistics",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "posts/Chi-square-test/Chi_square_test.html#probability-density-function-and-distribution",
    "href": "posts/Chi-square-test/Chi_square_test.html#probability-density-function-and-distribution",
    "title": "Chi-Square Test",
    "section": "Probability Density Function and Distribution",
    "text": "Probability Density Function and Distribution\nThe derived probability density function Equation 2 and the graph Figure 1 exhibit various shapes depending on the degrees of freedom, and as the degrees of freedom increase, it approximates the normal distribution.\n\\[ f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{(k/2) - 1} e^{-x/2}, \\quad x &gt; 0  \\tag{2}\\]\n\n\\(k\\) : degrees of freedom\n\\(Γ(k/2)\\) : Gamma function\n\\(x\\) : the value of the Chi-Square statistic\n\\(e\\) : the base of the natural logarithm, approximately equal to 2.718.\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of Normal and Chi-Square Distributions with Different degrees of freedom",
    "crumbs": [
      "Statistics",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "posts/Chi-square-test/Chi_square_test.html#statistical-applications",
    "href": "posts/Chi-square-test/Chi_square_test.html#statistical-applications",
    "title": "Chi-Square Test",
    "section": "Statistical Applications",
    "text": "Statistical Applications\n\nAssumptions\nThe Chi-Square Test can be used when the expected frequency in each cell is greater than or equal to 5. If the expected frequency is low, the approximation of the Chi-Square Test may not be accurate.\n\n\nTest of Independence\nThe Chi-Square Test is widely used for testing independence. It is used to determine whether the relationship between two variables is independent or to ascertain if there is a statistically significant relationship.\n\n\nGoodness of Fit Test\nThe Chi-Square Test is also used for goodness of fit tests. This is used to determine if the observed data fits a theoretical distribution. For example, it can be used to check if the frequencies of each number when a die is rolled 60 times are uniform.",
    "crumbs": [
      "Statistics",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "posts/Chi-square-test/Chi_square_test.html#chi-square-test-in-r",
    "href": "posts/Chi-square-test/Chi_square_test.html#chi-square-test-in-r",
    "title": "Chi-Square Test",
    "section": "Chi-Square Test in R",
    "text": "Chi-Square Test in R\nPlease refer to the following link for more information.",
    "crumbs": [
      "Statistics",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "posts/DataGoKr/DataGoKr.html",
    "href": "posts/DataGoKr/DataGoKr.html",
    "title": "Open Goverment Data",
    "section": "",
    "text": "The Public Data Portal (https://www.data.go.kr/index.do) is a unified gateway that provides public data generated or acquired and managed by public institutions in one place. The portal provides public data in various ways, such as file data, open API, and visualization, so that citizens can easily and conveniently use public data, and anyone can quickly and accurately find the desired public data through easy and convenient search.",
    "crumbs": [
      "Resources",
      "Open Goverment Data"
    ]
  },
  {
    "objectID": "posts/DataGoKr/DataGoKr.html#what-is-open-government-data",
    "href": "posts/DataGoKr/DataGoKr.html#what-is-open-government-data",
    "title": "Open Goverment Data",
    "section": "",
    "text": "The Public Data Portal (https://www.data.go.kr/index.do) is a unified gateway that provides public data generated or acquired and managed by public institutions in one place. The portal provides public data in various ways, such as file data, open API, and visualization, so that citizens can easily and conveniently use public data, and anyone can quickly and accurately find the desired public data through easy and convenient search.",
    "crumbs": [
      "Resources",
      "Open Goverment Data"
    ]
  },
  {
    "objectID": "posts/DataGoKr/DataGoKr.html#sign-up",
    "href": "posts/DataGoKr/DataGoKr.html#sign-up",
    "title": "Open Goverment Data",
    "section": "Sign Up",
    "text": "Sign Up\nDuring the registration process, personal authentication such as a smartphone is required. The Python Research Group has not signed up as a corporate member because it does not have a business registration number. Therefore, in order to use public data, researchers must sign up themselves.",
    "crumbs": [
      "Resources",
      "Open Goverment Data"
    ]
  },
  {
    "objectID": "posts/DataGoKr/DataGoKr.html#finding-the-desired-api",
    "href": "posts/DataGoKr/DataGoKr.html#finding-the-desired-api",
    "title": "Open Goverment Data",
    "section": "Finding the Desired API",
    "text": "Finding the Desired API\nIf you search for “국립암센터 원격전이 암발생 지표” you can easily get a list like Figure 1. It seems that only Korean is supported for searching.\n\n\n\n\n\n\nFigure 1: API list example",
    "crumbs": [
      "Resources",
      "Open Goverment Data"
    ]
  },
  {
    "objectID": "posts/DataGoKr/DataGoKr.html#application-for-utilization",
    "href": "posts/DataGoKr/DataGoKr.html#application-for-utilization",
    "title": "Open Goverment Data",
    "section": "Application for Utilization",
    "text": "Application for Utilization\nState the purpose of using the API and apply.",
    "crumbs": [
      "Resources",
      "Open Goverment Data"
    ]
  },
  {
    "objectID": "posts/DataGoKr/DataGoKr.html#application-status",
    "href": "posts/DataGoKr/DataGoKr.html#application-status",
    "title": "Open Goverment Data",
    "section": "Application Status",
    "text": "Application Status\nYou can check the application status in the next step as shown in Figure 2.\n\n\n\n\n\n\nFigure 2: API status example",
    "crumbs": [
      "Resources",
      "Open Goverment Data"
    ]
  },
  {
    "objectID": "posts/Git-setup-in-RStudio/Git_setup_in_RStudio.html",
    "href": "posts/Git-setup-in-RStudio/Git_setup_in_RStudio.html",
    "title": "Git Setup in RStudio",
    "section": "",
    "text": "1단계: Git이 시스템에 설치되어 있고 RStudio에서 사용가능한지 확인하세요.\n\n\n\n\n\n\n1단계 예시\n\n\n\n\n\n\nRStudio Terminal pane에서 아래의 git 명령어를 실행하여 version을 확인합니다.\n\n\n\n\nRStdio Terminal pane\n\ngit --version\n\n\n\n오류가 있다면 Global Options… Git/SVM 메뉴에서 version control interface for RStudio가 check 되어있는지, git 실행파일의 경로가 제대로 인식되어 있는지 등을 확인하여 오류를 해결하고 진행 하시길 바랍니다.\n\n\n\n\n2단계: R 버전이 4.4.1일때 Git_Example이 프로젝트명에 포함되도록 R프로젝트를 만드세요.\n\n\n\n\n\n\n2단계 예시\n\n\n\n\n\n\nRStudio에서 New Project를 생성\nNew Directory에 생성\nQuarto Project를 생성\nC:/Projects 가 상위 디렉토리인지 확인 (=working directory)\nDirectory name으로 R-4.4.1-Git_Example 입력\ngit repository로 만들기 위해 Create a git repository 체크 유지\nCreate Project 클릭\n\n\n\n\nrecommeded project name\n\nR-4.4.1-Git_Example\n\n\n\n\n\n3단계: .gitignore 파일을 열어서 버전관리 예외로 설정된 파일/폴더 목록을 확인하세요.\n\n\n\n\n\n\n3단계 예시\n\n\n\n\n\n\nOutput pane Files 탭에서 .gitignore 파일을 열면 다음과 같습니다.\n\n\n\n\n.gitignore\n\n.Rproj.user\n.Rhistory\n.RData\n.Ruserdata\n\n\n\n\n\n4단계: git status 실행하여 branch 이름과 untracked file 목록을 확인하세요.\n\n\n\n\n\n\n4단계 예시\n\n\n\n\n\n\n\n\nRStudio Terminal pane\n\ngit status\n\n\n\n\n\ngit status output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .Rprofile\n        .gitignore\n        R-4.4.1-Git_Example.Rproj\n        R-4.4.1-Git_Example.qmd\n        _quarto.yml\n        renv.lock\n        renv/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n\n\nbranch name은 시스템에 Git 설치과정에서 main으로 설정했다면 main으로 출력됩니다. 참조: 기본브렌치 설정하기\n프로젝트 생성 시 프로젝트 폴더의 파일들은 git의 관리대상이 아닌 untracked file로 분류되어 보여집니다. 하지만, .gitignore에 설정된 파일들은 untracked file로도 보여지지 않음을 알 수 있습니다. 이로 이내 output pane의 Files탭에서 보이는 파일/폴더 목록과 untracked file들이 다름을 확인하시길 바랍니다.\nGit pane에서도 같은 결과를 GUI로 보여집니다. 파일/폴더는 Path 컬럼에 보여지고, Staged는 check되어 있지 않으며 Status는 모두 ?? 표시된 상태입니다.\n\n\n\n\n5단계: .gitignore 파일을 편집하여 .Rprofile, Rproj 확장자인 모든 파일, renv 폴더를 버전관리대상에서 제외하세요.\n\n\n\n\n\n\n5단계 예시\n\n\n\n\n\n\nRStudio에서 .gitignore 파일을 열어서 아래의 내용을 추가합니다.\n\n\n\n\n.gitignore\n\n.Rprofile\n*.Rproj\nrenv/\n\n\n\ngit status 명령을 실행하여 변경사항을 확인합니다\n\n\n\n\ngit status output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .gitignore\n        R-4.4.1-Git_Example.qmd\n        _quarto.yml\n        renv.lock\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n\n\nuntracked file 목록에서 .gitignore에 추가했던 파일들이 제외되었음을 알 수 있습니다.\nGit pane에서도 .gitignore에 추가했던 파일들이 제외되었음을 알 수 있습니다.\n\n\n\n\n6단계: git add . 명령으로 untracked file들을 staged 상태로 만드세요.\n\n\n\n\n\n\n6단계 예시\n\n\n\n\n\n\nRStudio Terminal pane에서 아래의 명령어를 실행합니다.\n\n\n\n\nRStudio Terminal pane\n\ngit add .\n\n\n\ngit status 명령을 실행하여 변경사항을 확인합니다\n\n\n\n\ngit status output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   .gitignore\n        new file:   R-4.4.1-Git_Example.qmd\n        new file:   _quarto.yml\n        new file:   renv.lock\n\n\n\ngit add . 명령어는 현재 디렉토리 및 모든 하위 폴더의 변경 사항을 staging 영역으로 추가하는 명령어입니다. 이는 새로 생성된 파일뿐만 아니라 수정된 파일도 staging 영역에 추가하게 됩니다.\n커밋할 준비가 된 (=staging 된) 파일/폴더들의 목록이 git status 명령어를 통해 확인할 수 있으며, 여기서는 .gitignore, R-4.4.1-Git_Example.qmd, _quarto.yml, renv.lock 파일들이 staging 영역에 추가된 상태입니다.\nstaging 영역에 추가한 파일을 다시 제거하고 싶다면 git rm –cached  명령을 사용해야 합니다. 이 명령은 staging에서만 해당 파일을 제거하며, 파일 자체는 그대로 유지됩니다.\nEnvironment/git pane에서 refresh 버튼을 클릭하여 목록을 갱신하면, Staged에 체크된 파일들이 나타납니다. 또한, Status 열에 A(added)를 나타내는 표시가 있어 해당 파일들이 staging 영역에 추가되었음을 알 수 있습니다.\n\n\n\n\n7단계: git commit -m \"initial commit\"를 실행하여 committed 상태로 만드세요.\n\n\n\n\n\n\n7단계 예시\n\n\n\n\n\n\nRStudio Terminal pane에서 아래의 명령어를 실행하고 결과를 확인합니다.\n\n\n\n\nRStudio Terminal pane\n\ngit commit -m \"initial commit\"\n\n\n\n\n\ngit status output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git commit -m \"initial commit\"\n[main (root-commit) bc95de0] initial commit\n 4 files changed, 357 insertions(+)\n create mode 100644 .gitignore\n create mode 100644 R-4.4.1-Git_Example.qmd\n create mode 100644 _quarto.yml\n create mode 100644 renv.lock\n\n\n\ngit commit -m \"initial commit\" 명령어는 Git 커밋 명령으로, 현재 staging 영역에 있는 변경 사항을 로컬 저장소에 저장(커밋)하고, “initial commit”이라는 메시지를 커밋에 붙입니다.\nroot-commit: 이 부분은 프로젝트의 첫 번째 커밋임을 나타냅니다. root-commit은 해당 브랜치에서 이전 커밋이 없음을 뜻합니다.\nbc95de0: 커밋의 고유한 SHA-1 해시 값입니다. Git은 각 커밋을 식별할 수 있도록 고유 해시 값을 할당합니다.\n4 files changed: 총 4개의 파일이 변경되었음을 의미합니다. 여기서는 처음 커밋이기 때문에 파일들이 추가되었다고 간주합니다.\n357 insertions(+): 총 357개의 라인이 추가되었음을 의미합니다. 이는 새로운 파일들이 추가되었기 때문에 발생한 추가된 라인 수입니다.\ncreate mode 100644: 파일이 추가됨을 나타냅니다. 100644는 파일의 권한을 의미하며, 읽기/쓰기 가능한 일반 파일이라는 의미입니다.\ngit status 명령으로 변경사항을 확인합니다.\n\n\n\n\nRStudio Terminal pane\n\ngit status\n\n\n\n\n\ngit status output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git status\nOn branch main\nnothing to commit, working tree clean\n\n\n\n커밋 후 변경사항이 없는 상태로 “nothing to commit, working tree clean”이 출력됩니다.\nEnvironment/git pane에서 refresh 버튼을 클릭하여 목록을 갱신하면, Staged에 체크된 파일들이 사라지고, Status 열에 M(modified)를 나타내는 표시가 없어진 것을 확인할 수 있습니다.\ngit log 명령어를 통해 커밋 이력을 확인할 수 있습니다.\n\n\n\n\nRStudio Terminal pane\n\ngit log\n\n\n\n\n\ngit log output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git log\ncommit bc95de0f7aeba9d13ace79cf8ac41b1da3870397 (HEAD -&gt; main)\nAuthor: BenKorea &lt;kimbi.kirams@gmail.com&gt;\nDate:   Sun Oct 6 23:10:28 2024 +0900\n\n    initial commit\n\n\n\ngit log 명령어를 통해 커밋 이력을 확인할 수 있습니다. 여기서는 커밋 메시지와 커밋한 시간, 커밋한 사람의 정보가 출력됩니다.\n\n\n\n\n8단계: Github에 계정을 만들고 원격저장소의 이름을 Git_Example로 만들어 보세요.\n\n\n\n\n\n\n8단계 예시\n\n\n\n\n\n\nGithub 공식사이트(https://github.com/)로 접속합니다.\n자신의 이메일과 사용자명을 이용해서 등록합니다 (연구회의 사용자명과 이메일이 아닙니다). 이왕이면 시스템에 git를 설치할 때 global로 설정했던 user.name과 email을 사용하는 것을 추천합니다.\nGit_Example 이름의 원격저장소를 만듭니다.\nQuick setup에서 HTTPS 주소를 복사합니다.\n\n\n\n\n9단계: 로컬과 원격저장소를 연결하세요.\n\n\n\n\n\n\n9단계 예시\n\n\n\n\n\n\nTerminal pane에서 아래의 git 명령어를 실행하여 원격저장소를 연결합니다.\n\n\n\n\nRStudio Terminal pane\n\ngit remote add origin https://github.com/RPythonStudy/Git_Example.git\n\n\n\ngit remote add origin 명령어는 로컬 저장소와 원격 저장소를 연결하는 명령어입니다. origin은 원격 저장소의 이름을 나타냅니다. 이후에 push나 pull 명령을 실행할 때 origin을 사용하여 원격 저장소를 지정할 수 있습니다.\n원격저장소의 주소는 앞단계에서 복사해 두었던 주소를 사용합니다.\n아래의 명령으로 원격 저장소의 연결 상태를 확인합니다.\n\n\n\n\nRStudio Terminal pane\n\ngit remote -v\n\n\n\n\n\ngit log output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git remote -v\norigin  https://github.com/RPythonStudy/Git_Example.git (fetch)\norigin  https://github.com/RPythonStudy/Git_Example.git (push)\n\n\n\norigin은 원격 저장소의 통칭 또는 별칭입니다.\nfetch 원격 저장소에서 최신변경사항을 가져오는 명령어이며 위에서는 명기된 원격저장소에서 가져옴을 보여줍니다.\npush 원격 저장소로 변경사항을 보내는 명령어이며 위에서는 명기된 원격저장소로 보냄을 보여줍니다.\n\n\n\n\n10단계: 로컬에 커밋된 사항들을 원격저장소로 push해 보세요.\n\n\n\n\n\n\n10단계 예시\n\n\n\n\n\n\nTerminal pane에서 아래의 git 명령어를 실행하여 원격저장소를 연결합니다.\n\n\n\n\nRStudio Terminal pane\n\ngit push origin main\n\n\n\ngit push 명령어는 로컬 저장소의 변경사항을 원격 저장소로 업로드합니다.\norigin은 원격저장소의 별칭입니다.\nmain은 로컬의 원격저장소 이름입니다. 그리고 위 명령으로 로컬의 main 브랜치에서 원격저장소 main 브랜치로 push합니다. `\n\n\n\n\ngit log output\n\nC:\\Projects\\R-4.4.1-Git_Example&gt;git push origin main\nEnumerating objects: 6, done.\nCounting objects: 100% (6/6), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 2.05 KiB | 525.00 KiB/s, done.\nTotal 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nTo https://github.com/RPythonStudy/Git_Example.git\n * [new branch]      main -&gt; main\n\n\n\nEnumerating objects: 객체수 계산에서 특이한 사항은 커밋된 파일은 4개 였는데 여기서는 객체수는 2개가 더 큽니다. 이는 디렉토리구조 파일과 커밋정보파일이 추가되기 때문입니다.\nCounting objects: Git이 객체를 전송할 준비가 되었음을 나타냅니다.\nDdelta compression…: Git이 객체를 압축하는 과정입니다. 이 과정에서 4개의 CPU 스레드를 사용해 병렬로 압축을 진행합니다. Delta 압축은 이미 존재하는 데이터와 비교해 변경된 부분만 압축해 전송하는 방식입니다. Compressing objects: Git이 전송할 모든 객체를 압축하여 준비를 완료한 상태입니다.\nWriting objects: 압축된 데이터를 원격 저장소에 전송하는 단계입니다. 6개의 객체가 2.05 KiB의 데이터로 전송되었으며, 전송 속도는 525 KiB/s로 전송이 완료되었습니다.는 압축된 객체를 디스크에 쓰는 과정입니다.\nTotal 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0): 전송된 객체의 총 개수와 압축된 객체의 크기를 보여줍니다. 여기서는 새로운 브랜치가 생성되었으므로 delta 값이 0으로 나타납니다.\n전송된 데이터가 원격 저장소인 https://github.com/RPythonStudy/Git_Example.git로 전송되었음을 나타냅니다.\n[new branch] main -&gt; main: 새로운 브랜치가 생성되었음을 나타냅니다. main 브랜치가 원격 저장소 main branch로 전송되었음을 보여줍니다.",
    "crumbs": [
      "Tools",
      "Git Setup in RStudio"
    ]
  },
  {
    "objectID": "posts/IT/introduction_to_IT.html",
    "href": "posts/IT/introduction_to_IT.html",
    "title": "Introduction to IT",
    "section": "",
    "text": "Introducing IT knowledges that are helpful for research groups."
  },
  {
    "objectID": "posts/outlier-detection/outlier_detection.html",
    "href": "posts/outlier-detection/outlier_detection.html",
    "title": "Outliers detection in continuous variables",
    "section": "",
    "text": "If the data follows a normal distribution, we can consider data points that are more than 3 standard deviations away from the mean as outliers.\nBelow Figure 1 is an example of visualization I created. I plotted a histogram with the mean and 3 standard deviations marked, and highlighted the outliers with red circles (the source code is in the source folder of the project repository, under the folder source/my_histogram_for_outlier_detection.R).\n\n\n\n\n\n\n\n\nFigure 1: Histogram of AGE\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histogram of CA19-9\n\n\n\n\n\nFigure 1 is a histogram of age data without any outliers. Figure 2 is a histogram of the tumor marker CA 19-9, which shows three outliers marked with red circles.\n\n\n\nBox-Cox Transformation is a method to transform non-normally distributed data to a distribution that is closer to normal using the following formula.\n\nTo apply the Box-Cox transformation, the data must be positive, and typically, this is achieved by adding a shift term to make the minimum value of the original data positive (https://blog.naver.com/pmw9440/221713858254). The process of finding the optimal normalization formula for a specific dataset involves finding the optimal λ. Notable values of λ are 0 and 1. When λ = 0, it means log(x) transformation, and when λ = 1, g(x) = x-1, which is equivalent to the identity transformation.\nAs shown in the figure above, the CA 19-9 measurement values do not follow a normal distribution. After transforming them to a normal distribution, the histogram looks like Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Histogram of CA19-9 after log transformation\n\n\n\n\n\nAfter taking the logarithm of the x-axis, the distribution looks more normal, and the number of outliers has decreased to one (Figure 3).",
    "crumbs": [
      "R",
      "Outliers detection in continuous variables"
    ]
  },
  {
    "objectID": "posts/outlier-detection/outlier_detection.html#outliers-detection-in-continuous-variables",
    "href": "posts/outlier-detection/outlier_detection.html#outliers-detection-in-continuous-variables",
    "title": "Outliers detection in continuous variables",
    "section": "",
    "text": "If the data follows a normal distribution, we can consider data points that are more than 3 standard deviations away from the mean as outliers.\nBelow Figure 1 is an example of visualization I created. I plotted a histogram with the mean and 3 standard deviations marked, and highlighted the outliers with red circles (the source code is in the source folder of the project repository, under the folder source/my_histogram_for_outlier_detection.R).\n\n\n\n\n\n\n\n\nFigure 1: Histogram of AGE\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histogram of CA19-9\n\n\n\n\n\nFigure 1 is a histogram of age data without any outliers. Figure 2 is a histogram of the tumor marker CA 19-9, which shows three outliers marked with red circles.\n\n\n\nBox-Cox Transformation is a method to transform non-normally distributed data to a distribution that is closer to normal using the following formula.\n\nTo apply the Box-Cox transformation, the data must be positive, and typically, this is achieved by adding a shift term to make the minimum value of the original data positive (https://blog.naver.com/pmw9440/221713858254). The process of finding the optimal normalization formula for a specific dataset involves finding the optimal λ. Notable values of λ are 0 and 1. When λ = 0, it means log(x) transformation, and when λ = 1, g(x) = x-1, which is equivalent to the identity transformation.\nAs shown in the figure above, the CA 19-9 measurement values do not follow a normal distribution. After transforming them to a normal distribution, the histogram looks like Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Histogram of CA19-9 after log transformation\n\n\n\n\n\nAfter taking the logarithm of the x-axis, the distribution looks more normal, and the number of outliers has decreased to one (Figure 3).",
    "crumbs": [
      "R",
      "Outliers detection in continuous variables"
    ]
  },
  {
    "objectID": "posts/Python-setup/Python_setup.html",
    "href": "posts/Python-setup/Python_setup.html",
    "title": "Python setup",
    "section": "",
    "text": "You may refer to the official installation documentation provided on the Python website (https://docs.python.org/3/using/index.html), or you can follow the simplified instructions provided below.",
    "crumbs": [
      "Python",
      "Python setup"
    ]
  },
  {
    "objectID": "posts/Python-setup/Python_setup.html#installation-guide",
    "href": "posts/Python-setup/Python_setup.html#installation-guide",
    "title": "Python setup",
    "section": "",
    "text": "You may refer to the official installation documentation provided on the Python website (https://docs.python.org/3/using/index.html), or you can follow the simplified instructions provided below.",
    "crumbs": [
      "Python",
      "Python setup"
    ]
  },
  {
    "objectID": "posts/Python-setup/Python_setup.html#installation-file",
    "href": "posts/Python-setup/Python_setup.html#installation-file",
    "title": "Python setup",
    "section": "Installation File",
    "text": "Installation File\nDownload and install Python interpreter from the official Python website (https://www.python.org/) by selecting the appropriate installation file for your operating system. As of September 6, 2024, it is recommended to download and install the python-3.12.6-amd64.exe file for Windows operating systems.",
    "crumbs": [
      "Python",
      "Python setup"
    ]
  },
  {
    "objectID": "posts/Python-setup/Python_setup.html#installation-options",
    "href": "posts/Python-setup/Python_setup.html#installation-options",
    "title": "Python setup",
    "section": "Installation Options",
    "text": "Installation Options\n\nChoose between Install Now and Customize installation\nFor first-time installations on Windows, it is recommended to choose Customize installation instead of Install Now (Figure 1). This option allows you to set the installation path to C:\\Python\\Python-x.y.z as desired. (You may abbreviate Python to P and x.y.z to xyz according to your preference.) (Assuming that you will enter the path in the command line interface, it is better to keep it short and intuitive).\n\n\nAdd python.exe to PATH\nRegistering the path of the Python executable file during installation is useful (Figure 1). This setting allows Python interpreter to be automatically recognized in integrated development environments (IDEs) like Visual Studio Code.\nMoreover, this setting enables you to run Python directly by typing the ‘python’ command in the command prompt or terminal from any location other than the folder where the Python executable is located. This provides developers with the flexibility to run and test Python code from any location.\n\n\n\n\n\n\nFigure 1: Selecting Customize installation and adding to PATH\n\n\n\n\n\nOptional Features\nIt is recommended to select all the optional features during the Optional Feature selection step (Figure 2).\n\n\n\n\n\n\nFigure 2: Checking all the options on Optional Features",
    "crumbs": [
      "Python",
      "Python setup"
    ]
  },
  {
    "objectID": "posts/Python-setup/Python_setup.html#installation-verification",
    "href": "posts/Python-setup/Python_setup.html#installation-verification",
    "title": "Python setup",
    "section": "Installation Verification",
    "text": "Installation Verification\n\n\n\n\n\n\nTo verify that Python has been installed correctly,\n\n\n\n\n\nopen the Run dialog by pressing Win + R, then type cmd to launch the command prompt.\n\n\n\nCommand Prompt\n\ncmd\n\n\nIn the command prompt, enter the following command to check that the installed Python version matches the expected version:\n\n\n\nCommand Prompt\n\npython --version\n\n\nor\n\n\n\nCommand Prompt\n\npython --V",
    "crumbs": [
      "Python",
      "Python setup"
    ]
  },
  {
    "objectID": "posts/Python-setup/Python_setup.html#folder-structure",
    "href": "posts/Python-setup/Python_setup.html#folder-structure",
    "title": "Python setup",
    "section": "Folder Structure",
    "text": "Folder Structure\nThe general folder structure is as shown in the figure below Figure 3, and it may vary based on the installation options chosen.\n\n\n\nC:\\Python\\Python-x.y.z\\                # Top-level Python installation directory\n           ├─ DLLs\\                   # Dynamic Link Libraries folder\n           ├─ Doc\\                    # Documentation folder (optional installation)\n           ├─ include\\                # C/C++ header files folder\n           ├─ Lib\\                    # Standard library and third-party packages folder\n           │   └─ site-packages\\      # Folder for packages installed via pip\n           ├─ libs\\                   # Additional libraries folder\n           ├─ Scripts\\                # Scripts folder, contains executable files like pip\n           ├─ tcl\\                    # Tcl/Tk libraries folder\n           ├─ python.exe              # Python executable file\n           └─ pythonw.exe             # Python executable for GUI applications without a console window\n\n\nFigure 3: Python Installation Directory Structure with Full Option",
    "crumbs": [
      "Python",
      "Python setup"
    ]
  },
  {
    "objectID": "posts/Python-setup/Python_setup.html#installing-new-versions-of-python-executables",
    "href": "posts/Python-setup/Python_setup.html#installing-new-versions-of-python-executables",
    "title": "Python setup",
    "section": "Installing New Versions of Python Executables",
    "text": "Installing New Versions of Python Executables\nWhen installing a new version of the executable file, it is recommended to choose Customize installation instead of Upgrade Now Figure 4.\n\n\n\n\n\n\nFigure 4: Selecting customize installation option during upgrading\n\n\n\nIt is recommended to install in a new path with the following naming convention. (You may abbreviate Python to P and x.y.z to xyz according to your preference.)\n\n\n\nInstall Window\n\nC:\\Python\\Python-x'.y'.z'\\  \n\n\n\n\n\n\n\n\nThe reason for installing in a new path is ……,\n\n\n\n\n\nTo maintain existing projects with the old executable file. This is because, although rare, the new executable file may cause errors in existing projects or may have limited compatibility or errors with existing packages. If you install in a new path, the folder structure will be as shown in Figure 5.\n\n\n\n\n\n\nC:\\Python\\Python-x.y.z\\     # existing Python installation directory\n       └─ Python-x'.y'.z'\\  # New Python installation directory\n\n\nFigure 5: Python customize Installation with new release and Directory Structure",
    "crumbs": [
      "Python",
      "Python setup"
    ]
  },
  {
    "objectID": "posts/R/introduction_to_R.html",
    "href": "posts/R/introduction_to_R.html",
    "title": "Introduciton to R",
    "section": "",
    "text": "History of R Development\nR was developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and was first released in 1993. The language was conceived as an open-source alternative to the S programming language, which was developed by John Chambers and others at Bell Laboratories. The design philosophy of R was centered around providing a user-friendly environment for data analysis, with an emphasis on extensibility and ease of use (Ihaka & Gentleman, 1996).\nOver the years, R has undergone significant enhancements, with contributions from a global community of developers and researchers. The Comprehensive R Archive Network (CRAN) was established to host R packages, which are collections of functions and data sets developed by the community. As of 2024, there are over 18,000 packages available on CRAN, covering a wide range of statistical methods, machine learning algorithms, and specialized tools for various disciplines, including biostatistics and epidemiology (CRAN, 2024).\n\n\nR & Medical Research\nThe adoption of R in medical research has been driven by its flexibility, robust statistical capabilities, and the availability of specialized packages. For instance, the survival package, developed by Terry Therneau, provides a comprehensive suite of tools for survival analysis, a critical method in clinical trials and epidemiological studies (Therneau, 2000). Additionally, the caret package simplifies the process of building and evaluating predictive models, which are increasingly used in precision medicine to tailor treatments to individual patients (Kuhn, 2008).\nOne of the key factors contributing to R’s importance in medical research is its ability to handle large, complex datasets. As medical records become increasingly digitized, researchers face the challenge of analyzing vast amounts of heterogeneous data. R’s data manipulation and visualization capabilities, particularly through packages like dplyr and ggplot2, allow researchers to explore and interpret data effectively, uncovering insights that might be missed with traditional methods (Wickham, 2016).\nGenomic Research: R has played a pivotal role in the analysis of genomic data, which is crucial for understanding the genetic basis of diseases and developing targeted therapies. The Bioconductor project, an open-source software project built on R, provides tools for the analysis of high-throughput genomic data. This has enabled researchers to conduct genome-wide association studies (GWAS) and identify genetic variants associated with diseases such as cancer and diabetes (Huber et al., 2015).\nClinical Trials: In clinical trials, accurate and reproducible statistical analysis is essential for assessing the efficacy and safety of new treatments. R’s robust statistical packages, combined with its reproducibility features, make it an ideal choice for clinical trial analysis. For example, the rms package facilitates regression modeling strategies, helping researchers to develop and validate predictive models in clinical studies (Harrell, 2015).\nThe increasing complexity of medical research, coupled with the rise of big data, has underscored the importance of R as a tool for medical researchers. The ability to integrate data from diverse sources, perform complex statistical analyses, and generate reproducible research has made R indispensable in modern medical research. Moreover, the open-source nature of R ensures that researchers can collaborate and share their methods, leading to more robust and transparent scientific practices.\nAs the field of medical research continues to evolve, the importance of R is likely to grow. Emerging areas such as precision medicine, where treatments are tailored to individual patients based on their genetic profiles, rely heavily on the advanced analytical capabilities provided by R. Additionally, the integration of machine learning methods into clinical research, facilitated by R, is expected to lead to new insights and innovations in patient care.\n\n\nReferences\n\nIhaka, R., & Gentleman, R. (1996). R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5(3), 299-314.\nTherneau, T. (2000). A package for survival analysis in S. Mayo Clinic.\nKuhn, M. (2008). Building predictive models in R using the caret package. Journal of Statistical Software, 28(5), 1-26.\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag.\nHuber, W., Carey, V. J., Gentleman, R., et al. (2015). Orchestrating high-throughput genomic analysis with Bioconductor. Nature Methods, 12(2), 115-121.\nHarrell, F. E. (2015). Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis. Springer.",
    "crumbs": [
      "R",
      "Introduciton to R"
    ]
  },
  {
    "objectID": "posts/R-syntax/R_syntax.html",
    "href": "posts/R-syntax/R_syntax.html",
    "title": "R syntax",
    "section": "",
    "text": "심도깊은 이해를 원하시면 R manual 중 R language definition과 번역본 ( https://translation.r-project.org/man/R-lang/R-lang-ko.html#Objects)을 참고하시길 바랍니다.\n아래는 위의 내용을 일부 발췌한 것과 이해원회장님의 강의자료를 바탕으로 합니다.",
    "crumbs": [
      "R",
      "R syntax"
    ]
  },
  {
    "objectID": "posts/R-syntax/R_syntax.html#기본유형basic-types",
    "href": "posts/R-syntax/R_syntax.html#기본유형basic-types",
    "title": "R syntax",
    "section": "기본유형(Basic types)",
    "text": "기본유형(Basic types)\n\nVectors\n벡터들은 데이터를 포함하고 있는 인접한 셀들과 같이 생각될 수 있습니다.셀들은 x[5]와 같이 인덱싱 조작(indexing operation)을 통하여 접근 되어집니다. 더 자세한 사항은 Indexing에 설명되어 있습니다.\nR은 여섯 가지의 기본 (‘atomic’, 아토믹) 벡터를 가지고 있습니다. 이들은 논리형(logical), 정수형(integer), 실수형(real), 복소수형(complex), 문자열 (string) 또는 문자 (character), 그리고 원형(raw)입니다. 다음의 표는 유형이 다른 벡터들의 모드와 저장모드들을 정리하였습니다.\n\n\n\n\ntypeof\nmode\nstorage.mode\n\n\n\n\nlogical\nlogical\nlogical\n\n\ninteger\nnumeric\ninteger\n\n\ndouble\nnumeric\ndouble\n\n\ncomplex\ncomplex\ncomplex\n\n\ncharacter\ncharacter\ncharacter\n\n\nraw\nraw\nraw\n\n\n\n\n\ndouble: 실수형 데이터\n\nR에서 숫자형 데이터는 기본적으로 실수형 데이터로 인식됩니다. 이는 프로그래밍의 융통성을 부여합니다. 아래의 예시처럼 소수점이 있든, 소수점이 없는 형태를 변수에 할당하든 기본적으로 실수형으로 인식됩니다.\n\nnumeric_value&lt;-pi; print(numeric_value); typeof(numeric_value); typeof(1+1)\n\n[1] 3.141593\n\n\n[1] \"double\"\n\n\n[1] \"double\"\n\n\n\ninteger: 정수형 데이터\n\n하지만 소수점이 없는 정수를 명시적으로 변수에 할당할 때(대문자L을 끝에 붙임)와 범위연산자(range operator = colon operator)가 명시적으로 정수를 할당할 때는 정수형이 만들어집니다.\n\ninteger_value &lt;-42L; typeof(integer_value); typeof(1:3); typeof(1L+1L)\n\n[1] \"integer\"\n\n\n[1] \"integer\"\n\n\n[1] \"integer\"\n\n\n\ncharacter: 문자형 데이터\n\n“abc”, “a”, “a123xz” 등 quotation mark로 된 문자열\n\nletters[5:10]; paste(\"ab\",\"cde\", sep = \"\")\n\n[1] \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\n\n[1] \"abcde\"\n\n\n\nas.character(345); as.numeric(\"23.5\")\n\n[1] \"345\"\n\n\n[1] 23.5\n\n\n\nsub(\"a\",\"x\", \"father and grandpa\"); gsub(\"a\",\"x\", \"father and grandpa\")\n\n[1] \"fxther and grandpa\"\n\n\n[1] \"fxther xnd grxndpx\"\n\n\n\n(ex2 &lt;- 'The \"R\" project for statistical computing')\n\n[1] \"The \\\"R\\\" project for statistical computing\"\n\n\n\ncomplex: 복소수형 데이터\n\n\ncomplex_value&lt;-(1+sqrt(2)*1i)*(1-sqrt(2)*1i); print(complex_value); typeof(complex_value) # complex 연산\n\n[1] 3+0i\n\n\n[1] \"complex\"\n\n\n\nraw: 주로 이진 데이터(binary data)를 표현하는 데 사용됩니다. raw 타입은 주로 파일 입출력 또는 네트워크 통신에서 원시 데이터를 다룰 때 사용됩니다. 이 데이터는 변환 없이 그대로 저장되고 전달됩니다.\n\n벡터의기본유형에서 typeof 함수와 mode 함수의 반환값들을 비교하면 아래의 그림과 같습니다.\n\n\n\n기본벡터에서 type와 mode의 관계\n\n\n아래서부터는 퀴즈를 풀어보시기 바랍니다\nR에서는 모든 변수가 벡터 (열) 로 되어 있다. 다음 연산결과를 예상해 보시오\n\n1:3 + 2:4 ; 1:10 + 1:2\n\n[1] 3 5 7\n\n\n [1]  2  4  4  6  6  8  8 10 10 12\n\n\n\npaste(LETTERS[1:10],1:3,sep = \"-\"); paste(LETTERS[1:3],1:10)\n\n [1] \"A-1\" \"B-2\" \"C-3\" \"D-1\" \"E-2\" \"F-3\" \"G-1\" \"H-2\" \"I-3\" \"J-1\"\n\n\n [1] \"A 1\"  \"B 2\"  \"C 3\"  \"A 4\"  \"B 5\"  \"C 6\"  \"A 7\"  \"B 8\"  \"C 9\"  \"A 10\"\n\n\nvector의 특징은 모든 요소가 단일한 것이라는 점이다. NA 값을 제외하고는 모든 요소가 같아야 하기 때문에 서로 다른 성질의 것을 입력하게 되면 에러가 생기거나 변형된다.\n\nc(1,2,3); c(1,2,3,\"a\")\n\n[1] 1 2 3\n\n\n[1] \"1\" \"2\" \"3\" \"a\"\n\n\n\narray : multidimensional vector\n\n\n(arr1 &lt;- array(data=1:90, dim = c(6,5,3))) # 3Dimensional array\n\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    7   13   19   25\n[2,]    2    8   14   20   26\n[3,]    3    9   15   21   27\n[4,]    4   10   16   22   28\n[5,]    5   11   17   23   29\n[6,]    6   12   18   24   30\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   31   37   43   49   55\n[2,]   32   38   44   50   56\n[3,]   33   39   45   51   57\n[4,]   34   40   46   52   58\n[5,]   35   41   47   53   59\n[6,]   36   42   48   54   60\n\n, , 3\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   61   67   73   79   85\n[2,]   62   68   74   80   86\n[3,]   63   69   75   81   87\n[4,]   64   70   76   82   88\n[5,]   65   71   77   83   89\n[6,]   66   72   78   84   90\n\n\n\narr1[6,4,2] # 3Dimensional indexing\n\n[1] 54\n\n\n\nwhich(arr1==54, arr.ind = TRUE )\n\n     dim1 dim2 dim3\n[1,]    6    4    2\n\n\n\nmatrix : 2dimensional vector\n\n\nmatrix(data = c(3,4,5,6,7,8),\n       nrow=2,\n       ncol=3, # nrow=2 하나만 지정해도 ncol=3은 내부적으로 결정됨\n       byrow = TRUE, # data assign 하는 방향\n       dimnames = list(c(\"pt_1\", \"pt_2\"), # row names\n                       c(\"var1\",\"var2\",\"var3\")) # col names\n              )\n\n     var1 var2 var3\npt_1    3    4    5\npt_2    6    7    8\n\n\n\nx &lt;- 2:9 ; names(x) &lt;- x # x의 이름을 부여\nx %o% x # = outer function : outer(x,x, FUN=\"*\")\n\n   2  3  4  5  6  7  8  9\n2  4  6  8 10 12 14 16 18\n3  6  9 12 15 18 21 24 27\n4  8 12 16 20 24 28 32 36\n5 10 15 20 25 30 35 40 45\n6 12 18 24 30 36 42 48 54\n7 14 21 28 35 42 49 56 63\n8 16 24 32 40 48 56 64 72\n9 18 27 36 45 54 63 72 81\n\n\n\ndata frame : vector를 구성요소로 한 list의 형태 (외형적으로 보면 2dimension으로 보인다) dataframe의 구성요소는 vector들 (각각의 vector는 동일한 데이터 타입이라야 함)\n\n\n## dataframe 만들기\ndf1 &lt;- data.frame( col1 = c(\"A\", \"B\", \"Anyone\", \"None\"),\ncol2 = c(160, 170, 180, 200),\ncol3 = c(TRUE, FALSE, FALSE, TRUE)\n)\ndf1\n\n    col1 col2  col3\n1      A  160  TRUE\n2      B  170 FALSE\n3 Anyone  180 FALSE\n4   None  200  TRUE\n\n\n\n데이터프레임 이름 &lt;- data.frame(컬럼이름= c(data_1, …. , data_n), ….. ) 이런 형식으로 데이터 프레임을 만들 수 있다. 데이터프레임이 R의 기본적인 데이터 양식이기 때문에 이를 다루는 방법이 다양하게 존재함\n\n\n## dataframe cell 찾기\ndf1[3,2] #3행 2열의 데이터\n\n[1] 180\n\n\n\n## column 이름으로 찾기\ndf1$col1 ; df1[, \"col1\"]; df1[\"col1\"] ### df1의 col1 열을 찾는 방법들\n\n[1] \"A\"      \"B\"      \"Anyone\" \"None\"  \n\n\n[1] \"A\"      \"B\"      \"Anyone\" \"None\"  \n\n\n    col1\n1      A\n2      B\n3 Anyone\n4   None\n\n\n\ndf1[,1]\n\n[1] \"A\"      \"B\"      \"Anyone\" \"None\"  \n\n\n\n\n리스트(Lists)\n리스트 (“generic vectors”)는 데이터 저장(data storage)의 또 다른 종류입니다. 리스트들의 구성요소는 어떠한 유형의 R 객체들이 될 수 있습니다. 즉, 리스트의 구성요소들은 같은 유형일 필요가 없습니다.리스트의 구성요소들은 세가지 다른 인덱싱 조작(indexing operation)에 의하여 접근되어 집니다. 자세한 사항은 Indexing에 설명되어 있습니다\n리스트도 벡터의 종류이긴 하지만, 리스트 유형을 제외한 기본 벡터의 종류들을 atomic vectors(벡터를 구성하는데 있어 더 이상 하위단계로 분류할 수 최소의 기본구성단위 요소만으로 된 벡터 – 아토믹 벡터)라고 합니다.\n\n필자주: 데이터구조적인 측면에서 R에서의 vecter와 list 개념을 비교하면 같은 데이터 타입으로 구성된 1차원 구조를 vector라 하고, 2차원 구조는 matrix, 3차원 구조는 array가 됩니다. 데이터타입이 다르면 리스트가 됩니다. 데이터프레임은 2차원이지만 컬럼별로 데이터 타입이 다른 경우로 대부분의 엑셀자료에 해당합니다.\n\n\n\n\nVector와 List의 비교\n\n\n\nlist : R에만 있는 독특한 데이터타입이다. 이것은 모든 데이터 타입을 담을 수 있는 형태이고 자료의 길이가 달라도 같이 담을 수가 있게 되어 있다. 또한 리스트 속에 리스트를 넣을 수 있기에 다단계로 nesting 되는 구조로 만들 수 있다.\n\n\nsample_list &lt;- list(data1=df1, data2 = arr1, data3 = x%o%x)\nstr(sample_list)\n\nList of 3\n $ data1:'data.frame':  4 obs. of  3 variables:\n  ..$ col1: chr [1:4] \"A\" \"B\" \"Anyone\" \"None\"\n  ..$ col2: num [1:4] 160 170 180 200\n  ..$ col3: logi [1:4] TRUE FALSE FALSE TRUE\n $ data2: int [1:6, 1:5, 1:3] 1 2 3 4 5 6 7 8 9 10 ...\n $ data3: num [1:8, 1:8] 4 6 8 10 12 14 16 18 6 9 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:8] \"2\" \"3\" \"4\" \"5\" ...\n  .. ..$ : chr [1:8] \"2\" \"3\" \"4\" \"5\" ...\n\n\n\nsample_list$data1\n\n    col1 col2  col3\n1      A  160  TRUE\n2      B  170 FALSE\n3 Anyone  180 FALSE\n4   None  200  TRUE\n\n\n\nsample_list$data1[,3]\n\n[1]  TRUE FALSE FALSE  TRUE",
    "crumbs": [
      "R",
      "R syntax"
    ]
  },
  {
    "objectID": "posts/R-syntax/R_syntax.html#속성attribute",
    "href": "posts/R-syntax/R_syntax.html#속성attribute",
    "title": "R syntax",
    "section": "속성(Attribute)",
    "text": "속성(Attribute)\nNULL을 제외한 모든 object들은 그들에게 부여된 하나 이상의 attribute들을 가질 수 있습니다. Attribute들은 모든 element들이 이름지어진 곳에 pairlist처럼 저장되지만, name=value pair들의 세트처럼 생각되어야만 합니다. attribute들의 목록은 attributes를 사용하여 얻을 수 있고 attributes&lt;-에 의해 설정됩니다. 각각의 구성요소들은 attr와 attr&lt;-를 사용하여 접근됩니다.\n몇몇의 attribute들은 (예를들어 factor들을 위한 levels&lt;-) 특별한 accessor function들을 가지고있고 이들은 사용가능할 때만 사용되어야만 합니다. 실행의 숨겨진 디테일에 추가적으로, 그들은 추가적인operation들을 실행할 지도 모릅니다. R은 특별한 attribute을 포함하고 일관적인 확인들을 강요하는 attr&lt;-와 attributes&lt;-로의 call들을 가로채려고 시도합니다.\n행렬들과 열들은 간단하게말해서 dim attribute를 가진 벡터들이고 옵션적으로 dimnames가 벡터에 부여되어있습니다.\nAttribute들은 R에 사용된 class structure을 이행하하기위해 사용되었습니다. 만약 object가 class attribute를 가지고 있다면 그 attribute는 평가 도중 검토될 것입니다. R의 class structure은 Object-oriented programming에 자세하게 설명되어있습니다.\n\nNames\nNames 속성은, 존재할 때, 벡터나 목록의 각각의 요소들에 label을 합니다. Object가 존재하는 names의 속성을 프린트하려고 할 때, 요소들을 label하기위하여 쓰여집니다. Names 속성은 예를들어quantile(x)[“25%”]와 같은 indexing 목적으로도 쓰여질 수 있습니다.\nnames와 names&lt;- 구성들을 사용하여 name들을 얻거나 설정할 수도 있습니다. 뒷쪽 것 (names&lt;-)은 names 속성이 적합한 타입과 길이를 가지고있는지를 확실히 하기 위해 필요한 일관적인 체크를 수행할 것입니다.\nPairlist들과 일차원 행들은 특별하게 대해집니다. Pairlist object들에는, 가상의 names 속성이 사용되어집니다; names 속성은 사실상 목록 구성요소들의 태그에서부터 구성됩니다. 일차원 행들에서 names 속성은 실제로dimnames[[1]]에 접근합니다.\n\n\nDimensions\ndim 속성은 행들을 이행하기위하여 쓰여집니다. 행의 내용은 열방향순서의 벡터안에 저장되고, 그 dim 속성은 각각의 행의 규모를 명시하는 정수의 벡터입니다. R은 벡터의 길이가dimension 길이의 산출물임을 확실하게 합니다. 하나 이상의 dimension의 길이가 0일 수도 있습니다.\n벡터는 dim 속성이 없는 반면, 일차원 행은 길이가 1인 dim 속성을 가지고 있기때문에, 일차원 행과 같지 않습니다.\n\n\nDimnames\n행은 문자 벡터의 목록인 dimnames 속성을 사용하여 각각의 dimension을 따로 이름지을 수도 있습니다. dimnames 목록이 자기 자신의 이름을 가지고 있을 수 있으며, 그러면 이는 행들을 프린트 할 때 extent heading들로 쓰여집니다.\n\n\nClasses\nR은class 속성을 통해 주로 컨트롤되는 정교한 class 시스테을 가지고 있습니다. 이 속성은object가 inherit하는 곳의 class들의 목록을 포함하고있는 문자 벡터입니다. 이 형태들은 R의 “일반적인 방법을” 기능의 기초를 형성합니다.\n이 속성은 사용자에의한 제한 없이도 가상적으로 접근되고 조작될 수 있습니다. 여기에는 object가 class 방법들이 예상하는 구성요소들을 확실히 포함하고 있는지를 확인하는 것이 없습니다. 그러므로 class 속성을 대신하는 것은 조심해서 해야만하며, 이들이 사용 가능할 때는 구체적인 창출과 강제 function들이 선호되어야만합니다.\n\n\nTime series attributes\ntsp 속성은 시계열 분석, 시작, 끝, 그리고 빈도의 매개변수를 붙잡고 있기위해 사용됩니다. 이 구성은 월간 혹은 분기별 데이터처럼 주기적인 하부구조를 가진 series들을 다루기위해 주로 사용됩니다.",
    "crumbs": [
      "R",
      "R syntax"
    ]
  },
  {
    "objectID": "posts/R-syntax/R_syntax.html#special-compound-objects",
    "href": "posts/R-syntax/R_syntax.html#special-compound-objects",
    "title": "R syntax",
    "section": "Special compound objects",
    "text": "Special compound objects\n\nFactors\nFactor들은 유한 값들을 가질 수 있는 아이템들 (성별, 사회계층 등)을 설명하기위하여 사용됩니다. Factor는 levels 속성과 “factor” class를 가집니다. 선택적으로, factor이 modeling function들에 사용되었을 때 사용된 parametrisation을 조정하기위한 contrasts 속성도 포함하고 있을 수 있습니다.\nFactor는 순수하게 명목상이거나 순차적인 카테고리일 수도 있습니다. 순차적인 카테고리일 경우, 그렇게 정의되어야만하고 class 벡터c(“ordered”,” factor”)를 가집니다.\n\n필자주: 순차적인 범주형변수에서 요인화를 할 때 아래와 같이 속성에서 ordered = TRUE로 설정해야 함을 의미하여 class로 확인하면 ordered도 같이 출력됨을 의미합니다.\n\n\nsize &lt;- factor(c(\"중간\", \"낮음\", \"높음\", \"중간\", \"낮음\"), \n              levels = c(\"낮음\", \"중간\", \"높음\"), \n              ordered = TRUE)\nprint(size)\n\n[1] 중간 낮음 높음 중간 낮음\nLevels: 낮음 &lt; 중간 &lt; 높음\n\n\n\nclass(size)\n\n[1] \"ordered\" \"factor\" \n\n\n\n\nData frame objects\n데이터 프레임은 SAS 혹은 SPSS 데이터 셋을 가장 비슷하게 흉내내는 R structure들 입니다..\n데이터 프레임은 모두가 같은 길이(행렬일 경우에는 열의 갯수)를 가지고있는 벡터들, factor들, 행렬들의 목록입니다. 추가적으로, 데이터 프레임은 일반적으로 값들을 label하는 names 속성과 행데이터를 label하는 row.names 속성을 가지고 있습니다.\n\n\nTime objects\nR에서 시간과 날짜를 다루기 위해 다양한 객체 유형들이 사용됩니다. 각 객체는 특정 용도에 맞춰 설계되었으며, 시계열 데이터 분석, 시간대 처리, 시간 간격 계산 등 다양한 작업에 활용됩니다. Date, POSIXct, POSIXlt 같은 기본 객체들 외에도, lubridate와 hms 패키지에서 제공하는 객체들이 더욱 세분화된 시간 데이터 처리를 가능하게 합니다.\n\nDate 객체\n\n설명: Date 객체는 날짜를 일 단위로 표현합니다. 시간 정보는 포함하지 않으며, 주로 연도, 월, 일로 구성된 데이터를 다룹니다.\n주요 용도: 날짜를 다루고, 날짜 간의 차이를 계산할 때 사용됩니다.\n예시:\n\n\ntoday &lt;- Sys.Date(); print(today); class(today); typeof(today)\n\n[1] \"2024-10-24\"\n\n\n[1] \"Date\"\n\n\n[1] \"double\"\n\n\n\n\nPOSIXct 객체\n\n설명: POSIXct 객체는 날짜와 시간을 초 단위로 저장합니다. 이는 유닉스 타임스탬프처럼 시간대(time zone)를 고려하여 시간 데이터를 다룹니다. 숫자 벡터로 저장되며, 시계열 데이터를 처리할 때 유용합니다.\n주요 용도: 시계열 데이터 분석, 시간 계산, 시간대 관리.\n예시\n\n\ncurrent_time &lt;- Sys.time()\nprint(current_time); class(current_time); typeof(current_time)\n\n[1] \"2024-10-24 18:36:16 KST\"\n\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\n[1] \"double\"\n\n\n\n\nPOSIXlt 객체\n\n설명: POSIXlt 객체는 리스트 형태로 날짜와 시간 데이터를 저장하며, 연도, 월, 일, 시, 분, 초 등 각각의 요소로 분리됩니다. 사람이 읽기 쉬운 형태로 데이터가 저장되며, 개별 요소를 쉽게 접근할 수 있습니다.\n주요 용도: 날짜와 시간의 세부 요소를 조작할 때 유용.\n\n예시\n\n(time1 &lt;- as.POSIXlt(\"1960-01-01\")); class(time1); typeof(time1)\n\n[1] \"1960-01-01 KST\"\n\n\n[1] \"POSIXlt\" \"POSIXt\" \n\n\n[1] \"list\"\n\n\n\n\ndifftime 객체\n\n설명: difftime 객체는 두 날짜 또는 시간 간의 차이를 나타냅니다. 차이는 일(day), 시간(hour), 분(minute), 초(second) 등의 단위로 표현됩니다.\n주요 용도: 시간 간격 또는 지속 시간 계산.\n예시\n\nfirst &lt;- \"2022-08-20 08:15:22\" ; second &lt;- \"2022-01-01 20:04:48\"\ndifftime(first, second); difftime(first, second, units = \"hours\")\n\nTime difference of 230.5073 days\n\n\nTime difference of 5532.176 hours\n\ndifftime1&lt;-difftime(first, second)\nclass(difftime1); typeof(difftime1)\n\n[1] \"difftime\"\n\n\n[1] \"double\"\n\n\n\n\nfirst2 &lt;- as.POSIXlt(first); second2 &lt;- as.POSIXlt(second)\nsecond2 - first2\n\nTime difference of -230.5073 days\n\n\n\n## difftime(first, second, units = \"months\")\n## match.arg(units)에서 다음과 같은 에러가 발생했습니다:\n## 'arg' should be one of “auto”, “secs”, “mins”, “hours”, “days”, “weeks”\n\n\n\nhms 객체 (hms 패키지)\n\n설명: hms 객체는 시, 분, 초를 저장하고 다루기 위한 객체입니다. 하루 시간을 다루는 데 유용하며, 타임스탬프 없이 시간을 표현할 수 있습니다.\n주요 용도: 시, 분, 초 단위의 시간 데이터 관리.\n예시:\n\nlibrary(hms)\ntime_of_day &lt;- hms::as_hms(\"12:34:56\")\nprint(time_of_day)\n\n12:34:56\n\n\n\n\n\nInterval, Period, Duration 객체 (lubridate 패키지)\n\n설명: Interval, Period, Duration 객체는 lubridate 패키지에서 제공되며, 각각 시간 간격을 다루는 데 특화되어 있습니다.\n\nInterval: 시작 시간과 종료 시간을 포함하는 시간 간격.\nPeriod: 달력상의 시간 단위 (예: 월, 일, 년)로 표현된 기간.\nDuration: 일정한 시간 간격을 초 단위로 표현.\n\n주요 용도: 시간 간격 계산, 특정 기간 동안의 시간 변화 분석.\n예시\n\nsuppressMessages(library(lubridate))\n\nstart_time &lt;- as.POSIXct(\"2024-08-08 08:00:00\")\nend_time &lt;- as.POSIXct(\"2024-08-08 12:00:00\")\ninterval &lt;- interval(start_time, end_time)\nprint(interval)\n\n[1] 2024-08-08 08:00:00 KST--2024-08-08 12:00:00 KST\n\nperiod &lt;- months(3) + days(10)\nprint(period)\n\n[1] \"3m 10d 0H 0M 0S\"\n\nduration &lt;- as.duration(period)\nprint(duration)\n\n[1] \"8753400s (~14.47 weeks)\"",
    "crumbs": [
      "R",
      "R syntax"
    ]
  },
  {
    "objectID": "posts/R-syntax/R_syntax.html#data-manipulation",
    "href": "posts/R-syntax/R_syntax.html#data-manipulation",
    "title": "R syntax",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nData reading\ndata file이 존재하는 디렉토리를 먼저 설정해주어야 합니다. 이를 위한 명령어는 setwd() = set working directory 라는 의미 setwd(“C:/R/projects/R-4.4.1-RPythonStudy_HANJM”) 와 같이 디렉토리를 설정해줄 수도 있지만, 만약 디렉토리를 찾기 어렵다면 setwd( choose.dir() ) 와 같은 명령으로 파일탐색기를 열어서 디렉토리를 선택할 수 있습니다. 현재 사용 할 xlsx 파일들이 다음 디렉토리에 있다고 가정합니다.\n\nsetwd(\"C:/R/projects/R-4.4.1-RPythonStudy_HANJM/raw_data\")\nlibrary(readxl)\ndir(pattern = \"*.xlsx\")\n\n[1] \"deidentified_han20230213.xlsx\"\n\nxlsxfiles &lt;- dir(pattern = \"*.xlsx\")\nptinfo &lt;- read_xlsx(xlsxfiles[1])\n\n\n\nBinding tables\n데이터프레임 결합 방법들\nrbind(), cbind(), merge()\n\n\n\n데이터프레임 결합방법\n\n\n** 당연한 이야기지만 rbind는 컬럼의 갯수가 같아야 하고, cbind는 행의 갯수가 같아야 합니다.\n\n\nJoin (Merge) tables\nmerge function\nmerge(x, y, by = intersect(names(x), names(y)), ## 공통된 컬럼하나를 결합용 키로 선택\nby.x = by, by.y = by, all = FALSE, all.x = all, all.y = all, ## x와 y의 결합용 키의 이름이 서로 다를 경우에는 독립적으로 지정\nsort = TRUE, suffixes = c(“.x”,“.y”), no.dups = TRUE,\nincomparables = NULL, …)\n\ndf1 &lt;- data.frame( ID = 1:10, Name = c(\"Lee\",\"Kim\",\"Park\", \"Kang\",\n\"Shin\", \"Lim\", \"Kwon\", \"Choi\", \"Nam\", \"Baek\" ),\nScore = as.integer(rnorm(10, 80,6 ))\n)\ndf2 &lt;- data.frame( ID = sample(1:10, 9, replace = F),\nDepartment = sample( c(\"IM\",\"GS\",\"GY\",\"PD\" ),9, replace = T),\nAge = as.integer(rnorm(9, 40,6 )) )\ndf1\n\n   ID Name Score\n1   1  Lee    84\n2   2  Kim    71\n3   3 Park    78\n4   4 Kang    83\n5   5 Shin    82\n6   6  Lim    83\n7   7 Kwon    91\n8   8 Choi    80\n9   9  Nam    79\n10 10 Baek    86\n\n\n\nmerged_df &lt;- merge(df1,df2, by=\"ID\", all = TRUE) # full join\nmerged_df\n\n   ID Name Score Department Age\n1   1  Lee    84       &lt;NA&gt;  NA\n2   2  Kim    71         PD  37\n3   3 Park    78         GY  37\n4   4 Kang    83         GY  34\n5   5 Shin    82         GS  48\n6   6  Lim    83         PD  46\n7   7 Kwon    91         GS  33\n8   8 Choi    80         GY  33\n9   9  Nam    79         GY  42\n10 10 Baek    86         IM  41\n\n\n\n\nTypes of Join merge\n함수를 실행하여 데이터를 결합할 때에는 데이터 join 방법이 다음과 같이 4가지가 있다. 두개의 df에서 모든 데이터가 완전하게 존재하지 않기 때문에 일치하지 않는 부분에 대한 처리규칙이 중요하다.\n\n\n\nTypes of Join\n\n\nmerge 함수의 옵션에서 all = TRUE 를 선택하면 full join, all.x 는 left join, all.y는 right join이 된다.\nall= FALSE 인 경우에는 당연히 inner join dplyr package에는 개별적인 join 함수가 있는데 그것을 사용해도 됨\ninner_join(df1, df2), left_join(df1, df2), right_join(df1, df2), full_join(df1, df2) left_join(df2, df1) : alternative right join\n\n\nReshape data",
    "crumbs": [
      "R",
      "R syntax"
    ]
  },
  {
    "objectID": "posts/R-syntax/R_syntax.html#pipeline-operator",
    "href": "posts/R-syntax/R_syntax.html#pipeline-operator",
    "title": "R syntax",
    "section": "Pipeline operator",
    "text": "Pipeline operator\nlibrary magrittr 를 사용하면 pipeline 연산자를 쓸 수 있게 된다. %&gt;% 형식이다.\n만약 c(“A”,“B”,“C”) 라는 데이터를 “ABC” 로 paste 한 다음에 다시 tolower 함수를 적용하여 “abc”로 변환하는 작업을 한다고 하자. 그런 경우에는 다음과 같이 코딩을 해야 한다. 하지만 pipeline operator를 사용하면 함수 중첩을 줄이고 코드를 이해하기 쉽게 사용할 수 있다.\n\nlibrary(magrittr)\ntolower(paste(c(\"A\",\"B\",\"C\"), collapse = \"\"))\n\n[1] \"abc\"\n\n\n\nc(\"A\",\"B\",\"C\") %&gt;% paste(., collapse = \"\") %&gt;% tolower\n\n[1] \"abc\"\n\n\n\n## 첫번째 인자로 들어가기 위해서 . 을 사용함",
    "crumbs": [
      "R",
      "R syntax"
    ]
  },
  {
    "objectID": "posts/R-syntax/R_syntax.html#basic-latex-code",
    "href": "posts/R-syntax/R_syntax.html#basic-latex-code",
    "title": "R syntax",
    "section": "Basic LaTeX code",
    "text": "Basic LaTeX code",
    "crumbs": [
      "R",
      "R syntax"
    ]
  },
  {
    "objectID": "posts/RStudio-setup/RStudio_setup.html",
    "href": "posts/RStudio-setup/RStudio_setup.html",
    "title": "RStudio setup",
    "section": "",
    "text": "RStudio and VS Code are both integrated development environments (IDEs) for R programming, each with its own features. RStudio is a dedicated environment optimized for R language, providing various features related to R programming, data visualization, and reporting. On the other hand, VS Code supports multiple programming languages and offers flexibility for developers to add the required features through extensions available in the Marketplace. VS Code provides lightweight performance and integrated terminal functionality, making it suitable for various development environments, but it may lack specialized features for R programming. The study group recommends using RStudio for R programming.\n\n\nDownload the latest version of RStudio for your operating system from the official download site ((https://posit.co/downloads/) (as of September 27, 2024, the latest installation file for Windows Desktop is RStudio-2024.09.0-375.exe).\nInstall it in the default folder.\nAfter installing RStudio, run it and select the path to the latest version of R you installed in the Global Options menu under Tools &gt; Global Options… &gt; R General &gt; Basic tab as shown in the example below. (If you want to run a project created by an older version of R in that version, specify the path to the executable file of the older version of R in this menu and open the project to use it.)\n\n\n\nR version: example\n\n[64-bit] C:\\R\\R-4.4.1\n\n\nWe recommend specifying the parent folder for managing projects as the Default working directory (when not in a project) as shown in the example below.\n\n\n\nDefault working directory: example\n\nC:\\Projects\n\n\nRefer to the manual created by the developer posit at https://docs.posit.co/ide/user/.\nDevelopers are likely to use RStudio rather than R as a command-line interface. In that respect, the usage of R and RStudio is closely related, and there are tutorials embedded in RStudio created by posit. If you follow the link, you will find Beginners, Intermediates, and Experts courses, so it is recommended to choose the appropriate course to start.\n(In RStudio, there is an option to set whether to use a version control interface like git, as shown in Figure 1. You should check this option so that you can choose whether to use git when creating a new project.)\n\n\n\n\n\n\nFigure 1: RStudio에서 git 사용여부 설정",
    "crumbs": [
      "R",
      "RStudio setup"
    ]
  },
  {
    "objectID": "posts/RStudio-setup/RStudio_setup.html#choosing-an-integrated-development-environment-for-r",
    "href": "posts/RStudio-setup/RStudio_setup.html#choosing-an-integrated-development-environment-for-r",
    "title": "RStudio setup",
    "section": "",
    "text": "RStudio and VS Code are both integrated development environments (IDEs) for R programming, each with its own features. RStudio is a dedicated environment optimized for R language, providing various features related to R programming, data visualization, and reporting. On the other hand, VS Code supports multiple programming languages and offers flexibility for developers to add the required features through extensions available in the Marketplace. VS Code provides lightweight performance and integrated terminal functionality, making it suitable for various development environments, but it may lack specialized features for R programming. The study group recommends using RStudio for R programming.\n\n\nDownload the latest version of RStudio for your operating system from the official download site ((https://posit.co/downloads/) (as of September 27, 2024, the latest installation file for Windows Desktop is RStudio-2024.09.0-375.exe).\nInstall it in the default folder.\nAfter installing RStudio, run it and select the path to the latest version of R you installed in the Global Options menu under Tools &gt; Global Options… &gt; R General &gt; Basic tab as shown in the example below. (If you want to run a project created by an older version of R in that version, specify the path to the executable file of the older version of R in this menu and open the project to use it.)\n\n\n\nR version: example\n\n[64-bit] C:\\R\\R-4.4.1\n\n\nWe recommend specifying the parent folder for managing projects as the Default working directory (when not in a project) as shown in the example below.\n\n\n\nDefault working directory: example\n\nC:\\Projects\n\n\nRefer to the manual created by the developer posit at https://docs.posit.co/ide/user/.\nDevelopers are likely to use RStudio rather than R as a command-line interface. In that respect, the usage of R and RStudio is closely related, and there are tutorials embedded in RStudio created by posit. If you follow the link, you will find Beginners, Intermediates, and Experts courses, so it is recommended to choose the appropriate course to start.\n(In RStudio, there is an option to set whether to use a version control interface like git, as shown in Figure 1. You should check this option so that you can choose whether to use git when creating a new project.)\n\n\n\n\n\n\nFigure 1: RStudio에서 git 사용여부 설정",
    "crumbs": [
      "R",
      "RStudio setup"
    ]
  },
  {
    "objectID": "posts/RStudio-setup/RStudio_setup.html#renv-for-package-dependency-management",
    "href": "posts/RStudio-setup/RStudio_setup.html#renv-for-package-dependency-management",
    "title": "RStudio setup",
    "section": "renv for Package Dependency Management",
    "text": "renv for Package Dependency Management\nIn R, packages are usually installed in the library folder under the installation folder of the respective R version. renv is a tool designed to manage package dependencies in R projects. When you install and activate renv, a renv folder is created under the project folder, and packages are installed under that folder. It also manages the information of the installed packages in the renv.lock file. This way, R can manage packages on a project-by-project basis, so the author recommends using it. When creating a new project in RStudio, the option to use renv is automatically set when checked.\nFor usage instructions, refer to https://rstudio.github.io/renv/articles/renv.html.",
    "crumbs": [
      "R",
      "RStudio setup"
    ]
  },
  {
    "objectID": "posts/RStudio-setup/RStudio_setup.html#r-rstudio-folder-management-recommendations",
    "href": "posts/RStudio-setup/RStudio_setup.html#r-rstudio-folder-management-recommendations",
    "title": "RStudio setup",
    "section": "R & RStudio Folder Management Recommendations",
    "text": "R & RStudio Folder Management Recommendations\n\nProject Folder Management\nIt is recommended to manage project folders under C:\\Projects with the folder name as the R version and project name Figure 2.\n\n\n\nC:\\Projects\\\n      └─ R-x.y.z-Project_Name\n\n\nFigure 2: Recommended nomenclautue for Project directory name",
    "crumbs": [
      "R",
      "RStudio setup"
    ]
  },
  {
    "objectID": "posts/statistics/introduction_to_statistics.html",
    "href": "posts/statistics/introduction_to_statistics.html",
    "title": "Introduction to statistics",
    "section": "",
    "text": "Introduction to statistics used in R.",
    "crumbs": [
      "Statistics",
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "posts/tools/introduction_to_tools.html",
    "href": "posts/tools/introduction_to_tools.html",
    "title": "Introduction to Utilities",
    "section": "",
    "text": "Introducing open source programs that are helpful for research groups.",
    "crumbs": [
      "Tools",
      "Introduction to Utilities"
    ]
  }
]